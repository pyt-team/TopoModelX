{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial 2-complex convolutional neural network (SCConv)\n",
    "\n",
    "\n",
    "In this notebook, we will create and train a Simplicial 2-complex convolutional neural in the simplicial complex domain, as proposed in the paper by [Bunch et. al : Simplicial 2-Complex Convolutional Neural Networks (2020)](https://openreview.net/pdf?id=Sc8glB-k6e9).\n",
    "\n",
    "\n",
    "We train the model to perform\n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{y\\rightarrow x}^{(0\\rightarrow 0)} = ({\\tilde{A}_{\\uparrow,0}})_{xy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0\\rightarrow0)}$\n",
    "\n",
    "游린 $\\quad m^{(1\\rightarrow0)}_{y\\rightarrow x}  = (B_1)_{xy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(1\\rightarrow 0)}$\n",
    "\n",
    "游린 $\\quad m^{(0 \\rightarrow 1)}_{y \\rightarrow x}  = (\\tilde B_1)_{xy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0 \\rightarrow1)}$\n",
    "\n",
    "游린 $\\quad m^{(1\\rightarrow1)}_{y\\rightarrow x} = ({\\tilde{A}_{\\downarrow,1}} + {\\tilde{A}_{\\uparrow,1}})_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1\\rightarrow1)}$\n",
    "\n",
    "游린 $\\quad m^{(2\\rightarrow1)}_{y \\rightarrow x}  = (B_2)_{xy} \\cdot h_y^{t,(2)} \\cdot \\Theta^{t,(2 \\rightarrow1)}$\n",
    "\n",
    "游린 $\\quad m^{(1 \\rightarrow 2)}_{y \\rightarrow x}  = (\\tilde B_2)_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 2)}$\n",
    "\n",
    "游린 $\\quad m^{(2 \\rightarrow 2)}_{y \\rightarrow x}  = ({\\tilde{A}_{\\downarrow,2}})\\_{xy} \\cdot h_y^{t,(2)} \\cdot \\Theta^{t,(2 \\rightarrow 2)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(0 \\rightarrow 0)}  = \\sum_{y \\in \\mathcal{L}_\\uparrow(x)} m_{y \\rightarrow x}^{(0 \\rightarrow 0)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 0)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 0)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(0 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(0 \\rightarrow 1)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 1)}  = \\sum_{y \\in (\\mathcal{L}_\\uparrow(x) + \\mathcal{L}_\\downarrow(x))} m_{y \\rightarrow x}^{(1 \\rightarrow 1)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(2 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(2 \\rightarrow 1)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 2)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 2)}$\n",
    "\n",
    "游릲 $\\quad m_x^{(2 \\rightarrow 2)}  = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)} m_{y \\rightarrow x}^{(2 \\rightarrow 2)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}  = m_x^{(1\\rightarrow0)}+ m_x^{(0\\rightarrow0)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}  = m_x^{(2\\rightarrow1)}+ m_x^{(1\\rightarrow1)}$\n",
    "\n",
    "游릱 $\\quad h^{t+1, (0)}_x  = \\sigma(m_x^{(0)})$\n",
    "\n",
    "游릱 $\\quad h^{t+1, (1)}_x  = \\sigma(m_x^{(1)})$\n",
    "\n",
    "游릱 $\\quad h^{t+1, (2)}_x  = \\sigma(m_x^{(2)})$\n",
    "\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:56:40.324878004Z",
     "start_time": "2023-07-13T23:56:40.279753898Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import toponetx.datasets as datasets\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import diags\n",
    "\n",
    "from topomodelx.base.aggregation import Aggregation\n",
    "\n",
    "from topomodelx.nn.simplicial.scconv import SCConv\n",
    "from topomodelx.utils.sparse import from_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:24:30.399752783Z",
     "start_time": "2023-07-13T23:24:30.357813405Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T04:25:01.902814606Z",
     "start_time": "2023-06-26T04:25:00.067431655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:25:44.545809975Z",
     "start_time": "2023-07-13T23:25:44.537748017Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# l = np.unique(ys, return_counts=True)\n",
    "# print(l)\n",
    "print(len(np.unique(ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T04:25:05.161535039Z",
     "start_time": "2023-06-26T04:25:05.142961595Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th simplicial complex has 252 nodes with features of dimension 6.\n",
      "The 0th simplicial complex has 750 edges with features of dimension 10.\n",
      "The 0th simplicial complex has 500 faces with features of dimension 7.\n"
     ]
    }
   ],
   "source": [
    "i_complex = 0\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_0s[i_complex].shape[0]} nodes with features of dimension {x_0s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_1s[i_complex].shape[0]} edges with features of dimension {x_1s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_2s[i_complex].shape[0]} faces with features of dimension {x_2s[i_complex].shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_higher_order_adj(A_opt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A_opt is an opt that maps a j-cochain to a k-cochain.\n",
    "        shape [num_of_k_simplices num_of_j_simplices]\n",
    "\n",
    "    return:\n",
    "         D^{-0.5}* (A_opt)* D^{-0.5}.\n",
    "    \"\"\"\n",
    "    rowsum = np.array(np.abs(A_opt).sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.0\n",
    "    r_mat_inv_sqrt = diags(r_inv_sqrt)\n",
    "    A_opt_to = A_opt.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "    return coo_matrix(A_opt_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T14:30:08.309300865Z",
     "start_time": "2023-07-09T14:30:05.378291670Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T04:54:56.698762101Z",
     "start_time": "2023-06-26T04:54:56.690271544Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# incidence_1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T09:06:50.412427908Z",
     "start_time": "2023-07-09T09:06:50.361595391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 750)\n",
      "(750, 750)\n"
     ]
    }
   ],
   "source": [
    "adjacency_1 = simplexes[13].adjacency_matrix(rank=1, signed=False)\n",
    "incidence_1 = simplexes[13].incidence_matrix(rank=1, signed=False)\n",
    "\n",
    "# k = normalize_higher_order_adj(adjacency_1)\n",
    "# print(k)\n",
    "\n",
    "print(adjacency_1.todense().shape)\n",
    "k = normalize_higher_order_adj(adjacency_1)\n",
    "print(k.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define Neighbourhood Structures\n",
    "\n",
    "We create the neigborood structures expected by SSConv. The SSConv layer expects the following neighbourhood structures:\n",
    "* incidence_1 $B_1$\n",
    "* incidence_1_norm $\\tilde{B}_1$\n",
    "* incidence_2 $B_2$\n",
    "* incidence_2_norm $\\tilde{B}_1$\n",
    "* adjacency_up_0_norm $\\tilde{A}_{\\uparrow,0}$\n",
    "* adjacency_up_1_norm $\\tilde{A}_{\\uparrow,1}$\n",
    "* adjacency_down_1_norm $\\tilde{A}_{\\downarrow,1}$\n",
    "* adjacency_down_2_norm $\\tilde{A}_{\\downarrow,2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:33:28.375213622Z",
     "start_time": "2023-07-13T23:33:28.328016738Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_neighborhoods(simplexes):\n",
    "    incidence_1_list = []\n",
    "    incidence_1_norm_list = []\n",
    "    incidence_2_list = []\n",
    "    incidence_2_norm_list = []\n",
    "    adjacency_up_0_norm_list = []\n",
    "    adjacency_up_1_norm_list = []\n",
    "    adjacency_down_1_norm_list = []\n",
    "    adjacency_down_2_norm_list = []\n",
    "\n",
    "    # incidence_1_list = []\n",
    "    # incidence_2_list = []\n",
    "    # up_laplacian_1_list = []\n",
    "    # up_laplacian_2_list = []\n",
    "    # down_laplacian_1_list = []\n",
    "    # down_laplacian_2_list = []\n",
    "    # for simplex in simplexes:\n",
    "    #     B1 = simplex.incidence_matrix(rank=1, signed=False)\n",
    "    #     B2 = simplex.incidence_matrix(rank=2, signed=False)\n",
    "    #\n",
    "    #     up_laplacian_1 = simplex.up_laplacian_matrix(rank=0)  #1\n",
    "    #     up_laplacian_2 = simplex.up_laplacian_matrix(rank=1)  #2\n",
    "    #\n",
    "    #     down_laplacian_1 = simplex.down_laplacian_matrix(rank=1)  #1\n",
    "    #     down_laplacian_2 = simplex.down_laplacian_matrix(rank=2)  #2\n",
    "    #\n",
    "    #     incidence_1 = from_sparse(B1)\n",
    "    #     incidence_2 = from_sparse(B2)\n",
    "    #\n",
    "    #     up_laplacian_1 = from_sparse(up_laplacian_1)\n",
    "    #     up_laplacian_2 = from_sparse(up_laplacian_2)\n",
    "    #\n",
    "    #     down_laplacian_1 = from_sparse(down_laplacian_1)\n",
    "    #     down_laplacian_2 = from_sparse(down_laplacian_2)\n",
    "    #\n",
    "    #     incidence_1_list.append(incidence_1)\n",
    "    #     incidence_2_list.append(incidence_2)\n",
    "    #     up_laplacian_1_list.append(up_laplacian_1)\n",
    "    #     up_laplacian_2_list.append(up_laplacian_2)\n",
    "    #     down_laplacian_1_list.append(down_laplacian_1)\n",
    "    #     down_laplacian_2_list.append(down_laplacian_2)\n",
    "\n",
    "    return (\n",
    "        incidence_1_list,\n",
    "        incidence_1_norm_list,\n",
    "        incidence_2_list,\n",
    "        incidence_2_norm_list,\n",
    "        adjacency_up_0_norm_list,\n",
    "        adjacency_up_1_norm_list,\n",
    "        adjacency_down_1_norm_list,\n",
    "        adjacency_down_2_norm_list,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    incidence_1_list,\n",
    "    incidence_1_norm_list,\n",
    "    incidence_2_list,\n",
    "    incidence_2_norm_list,\n",
    "    adjacency_up_0_norm_list,\n",
    "    adjacency_up_1_norm_list,\n",
    "    adjacency_down_1_norm_list,\n",
    "    adjacency_down_2_norm_list,\n",
    ") = get_neighborhoods(simplexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## prepare training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:54:20.825930864Z",
     "start_time": "2023-07-13T23:54:20.783159857Z"
    }
   },
   "outputs": [],
   "source": [
    "# ToDo: apply train/test splitting\n",
    "\n",
    "x_0_train = x_0s\n",
    "x_1_train = x_1s\n",
    "x_2_train = x_2s\n",
    "\n",
    "(\n",
    "    incidence_1_list,\n",
    "    incidence_1_norm_list,\n",
    "    incidence_2_list,\n",
    "    incidence_2_norm_list,\n",
    "    adjacency_up_0_norm_list,\n",
    "    adjacency_up_1_norm_list,\n",
    "    adjacency_down_1_norm_list,\n",
    "    adjacency_down_2_norm_list,\n",
    ") = get_neighborhoods(simplexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T23:57:13.419769194Z",
     "start_time": "2023-07-13T23:57:12.242273643Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kha053/anaconda3/envs/topomodelx/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "model = SCConv(\n",
    "    node_channels=x_0s[0].shape[1],\n",
    "    edge_channels=x_1s[0].shape[1],\n",
    "    face_channels=x_1s[0].shape[1],\n",
    "    n_classes=len(np.unique(ys)),\n",
    "    n_layers=2,\n",
    ")\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T00:07:17.739647566Z",
     "start_time": "2023-07-14T00:07:17.696911071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: nan\n",
      "Epoch: 2 loss: nan\n",
      "Epoch: 3 loss: nan\n",
      "Epoch: 4 loss: nan\n",
      "Epoch: 5 loss: nan\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for (\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        incid1,\n",
    "        incid1_norm,\n",
    "        incid2,\n",
    "        incid2_norm,\n",
    "        adj0_up_norm,\n",
    "        adj1_up_norm,\n",
    "        adj1_down_norm,\n",
    "        adj2_down_norm,\n",
    "        y,\n",
    "    ) in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        x_2_train,\n",
    "        incidence_1_list,\n",
    "        incidence_1_norm_list,\n",
    "        incidence_2_list,\n",
    "        incidence_2_norm_list,\n",
    "        adjacency_up_0_norm_list,\n",
    "        adjacency_up_1_norm_list,\n",
    "        adjacency_down_1_norm_list,\n",
    "        adjacency_down_2_norm_list,\n",
    "        ys,\n",
    "    ):\n",
    "        (\n",
    "            x_0,\n",
    "            x_1,\n",
    "            x_2,\n",
    "            y,\n",
    "            incid1,\n",
    "            incid1_norm,\n",
    "            incid2,\n",
    "            incid2_norm,\n",
    "            adj0_up_norm,\n",
    "            adj1_up_norm,\n",
    "            adj1_down_norm,\n",
    "            adj2_down_norm,\n",
    "        ) = (\n",
    "            torch.tensor(x_0).float().to(device),\n",
    "            torch.tensor(x_1).float().to(device),\n",
    "            torch.tensor(x_2).float().to(device),\n",
    "            torch.tensor(y).float().to(device),\n",
    "            incid1.float().to(device),\n",
    "            incid1_norm.float().to(device),\n",
    "            incid2.float().to(device),\n",
    "            incid2_norm.float().to(device),\n",
    "            adj0_up_norm.float().to(device),\n",
    "            adj1_up_norm.float().to(device),\n",
    "            adj1_down_norm.float().to(device),\n",
    "            adj2_down_norm.float().to(device),\n",
    "        )\n",
    "\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(\n",
    "            x_0,\n",
    "            x_1,\n",
    "            x_2,\n",
    "            y,\n",
    "            incid1,\n",
    "            incid1_norm,\n",
    "            incid2,\n",
    "            incid2_norm,\n",
    "            adj0_up_norm,\n",
    "            adj1_up_norm,\n",
    "            adj1_down_norm,\n",
    "            adj2_down_norm,\n",
    "        )\n",
    "        loss = loss_fn(y_hat.flatten(), y)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss)}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        correct_count = 0\n",
    "        with torch.no_grad():\n",
    "            for (\n",
    "                x_0t,\n",
    "                x_1t,\n",
    "                x_2t,\n",
    "                incid1t,\n",
    "                incid1_normt,\n",
    "                incid2t,\n",
    "                incid2_normt,\n",
    "                adj0_up_normt,\n",
    "                adj1_up_normt,\n",
    "                adj1_down_normt,\n",
    "                adj2_down_normt,\n",
    "                yt,\n",
    "            ) in zip(\n",
    "                x_0_train,\n",
    "                x_1_train,\n",
    "                x_2_train,\n",
    "                incidence_1_list,\n",
    "                incidence_1_norm_list,\n",
    "                incidence_2_list,\n",
    "                incidence_2_norm_list,\n",
    "                adjacency_up_0_norm_list,\n",
    "                adjacency_up_1_norm_list,\n",
    "                adjacency_down_1_norm_list,\n",
    "                adjacency_down_2_norm_list,\n",
    "                ys,\n",
    "            ):\n",
    "                (\n",
    "                    x_0t,\n",
    "                    x_1t,\n",
    "                    x_2t,\n",
    "                    yt,\n",
    "                    incid1t,\n",
    "                    incid1_normt,\n",
    "                    incid2t,\n",
    "                    incid2_normt,\n",
    "                    adj0_up_normt,\n",
    "                    adj1_up_normt,\n",
    "                    adj1_down_norm,\n",
    "                    adj2_down_norm,\n",
    "                ) = (\n",
    "                    torch.tensor(x_0t).float().to(device),\n",
    "                    torch.tensor(x_1t).float().to(device),\n",
    "                    torch.tensor(x_2t).float().to(device),\n",
    "                    torch.tensor(yt).float().to(device),\n",
    "                    incid1t.float().to(device),\n",
    "                    incid1_normt.float().to(device),\n",
    "                    incid2t.float().to(device),\n",
    "                    incid2_normt.float().to(device),\n",
    "                    adj0_up_normt.float().to(device),\n",
    "                    adj1_up_normt.float().to(device),\n",
    "                    adj1_down_normt.float().to(device),\n",
    "                    adj2_down_normt.float().to(device),\n",
    "                )\n",
    "\n",
    "                y_hat = model(\n",
    "                    x_0t,\n",
    "                    x_1t,\n",
    "                    x_2t,\n",
    "                    yt,\n",
    "                    incid1t,\n",
    "                    incid1_normt,\n",
    "                    incid2t,\n",
    "                    incid2_normt,\n",
    "                    adj0_up_normt,\n",
    "                    adj1_up_normt,\n",
    "                    adj1_down_normt,\n",
    "                    adj2_down_normt,\n",
    "                )\n",
    "                test_loss = loss_fn(y_hat, yt)\n",
    "\n",
    "                if round(y_hat.item()) == round(yt.item()):\n",
    "                    correct_count += 1\n",
    "\n",
    "                print(f\"Test_loss: {test_loss}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
