{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Convolutional Neural Network (SCNN)\n",
    "\n",
    "In this notebook, we will create and train a convolutional neural network in the simplicial complex domain, as proposed in the paper by [Yang et. al : SIMPLICIAL CONVOLUTIONAL NEURAL NETWORKS (2022)](https://arxiv.org/pdf/2110.02585.pdf). \n",
    "\n",
    "### We train the model to perform:\n",
    "    1.  Complex classification using the shrec16 benchmark dataset.\n",
    "    2.  Node classification using the karate dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simplicial Convolutional Neural Networks <a href=\"https://arxiv.org/pdf/2110.02585.pdf\">[SCNN]</a>\n",
    "\n",
    "At layer $t$, given the input simplicial (edge) feature matrix $\\mathbf{H}_t$, the SCNN layer is defined as \n",
    "$$\n",
    "    \\mathbf{H}_{t+1} = \\sigma \\Bigg[ \\mathbf{H}_t\\mathbf{\\Theta}_t + \\sum_{p_d=1}^{P_d}(\\mathbf{L}_{\\downarrow,1})^{p_d}\\mathbf{H}_t\\mathbf{\\Theta}_{t,p_d} + \\sum_{p_u=1}^{P_u}(\\mathbf{L}_{\\uparrow,1})^{p_u}\\mathbf{H}_{t}\\mathbf{\\Theta}_{t,p_u} \\Bigg]\n",
    "$$\n",
    "where $p_d$ and $p_u$ are the lower and upper convolution orders, respectively, and $\\mathbf{\\Theta}_{t,p_d}$ and $\\mathbf{\\Theta}_{t,p_u}$ are the learnable weights.\n",
    "One can use $(\\mathbf{L}_{\\uparrow,1})^{p_u}$ and $(\\mathbf{L}_{\\uparrow,1})^{p_d}$ to perform higher-order upper and lower convolutions.\n",
    "\n",
    "\n",
    "To align with the notations in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031), we can use the following to denote the above layer definition. \n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u(1 \\rightarrow 2 \\rightarrow 1)}  = ((L_{\\uparrow,1})^{p_u})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_u} $  -------- Aggregate from $p_u$-hop upper neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d(1 \\rightarrow 0 \\rightarrow 1)} = ((L_{\\downarrow,1})^{p_d})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_d} $ -------- Aggregate from $p_d$-hop lower neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m^{(1 \\rightarrow 1)}_{x \\rightarrow x} = \\theta^t \\cdot h_x^{t, (1)}$ --------  Aggregate from $x$ itself\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{L}_\\uparrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}$  -------- Collect the aggregated information from the upper neighborhood\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect the aggregated information from the lower neighborhood\n",
    "\n",
    "游릲 $\\quad m^{(1 \\rightarrow 1)}_{x} = m^{(1 \\rightarrow 1)}_{x \\rightarrow x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}  = m_x^{(1 \\rightarrow 1)} + \\sum_{p_u=1}^{P_u} m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)} + \\sum_{p_d=1}^{P_d} m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect all the aggregated information \n",
    "\n",
    "游릱 $\\quad h_x^{t+1, (1)} = \\sigma(m_x^{(1)})$ -------- Pass through the nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complex Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import toponetx.datasets as datasets\n",
    "\n",
    "from topomodelx.nn.simplicial.scnn import SCNN\n",
    "from topomodelx.utils.sparse import from_sparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import shrec dataset ##\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider using edge features for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_0 = x_0s[-1].shape[1]\n",
    "in_channels_1 = x_1s[-1].shape[1]\n",
    "in_channels_2 = x_2s[-1].shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices and Hodge Laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2  # the order of the SC is two\n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "\n",
    "laplacian_0_list = []\n",
    "laplacian_down_1_list = []\n",
    "laplacian_up_1_list = []\n",
    "laplacian_2_list = []\n",
    "\n",
    "for simplex in simplexes:\n",
    "    incidence_1 = simplex.incidence_matrix(rank=1)\n",
    "    incidence_2 = simplex.incidence_matrix(rank=2)\n",
    "    laplacian_0 = simplex.hodge_laplacian_matrix(rank=0)\n",
    "    laplacian_down_1 = simplex.down_laplacian_matrix(rank=1)\n",
    "    laplacian_up_1 = simplex.up_laplacian_matrix(rank=1)\n",
    "    laplacian_2 = simplex.hodge_laplacian_matrix(rank=2)\n",
    "\n",
    "    incidence_1 = from_sparse(incidence_1)\n",
    "    incidence_2 = from_sparse(incidence_2)\n",
    "    laplacian_0 = from_sparse(laplacian_0)\n",
    "    laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "    laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "    laplacian_2 = from_sparse(laplacian_2)\n",
    "\n",
    "    incidence_1_list.append(incidence_1)\n",
    "    incidence_2_list.append(incidence_2)\n",
    "    laplacian_0_list.append(laplacian_0)\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_up_1_list.append(laplacian_up_1)\n",
    "    laplacian_2_list.append(laplacian_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1  # simplex level\n",
    "conv_order_down = 2\n",
    "conv_order_up = 2\n",
    "intermediate_channels = 4\n",
    "out_channels = intermediate_channels\n",
    "num_layers = 2\n",
    "\n",
    "# select the simplex level\n",
    "if rank == 0:\n",
    "    laplacian_down = None\n",
    "    laplacian_up = laplacian_0_list  # the graph laplacian\n",
    "    conv_order_down = 0\n",
    "    x = x_0s\n",
    "    in_channels = in_channels_0\n",
    "elif rank == 1:\n",
    "    laplacian_down = laplacian_down_1_list\n",
    "    laplacian_up = laplacian_up_1_list\n",
    "    x = x_1s\n",
    "    in_channels = in_channels_1\n",
    "elif rank == 2:\n",
    "    laplacian_down = laplacian_2_list\n",
    "    laplacian_up = None\n",
    "    x = x_2s\n",
    "    in_channels = in_channels_2\n",
    "else:\n",
    "    raise ValueError(f\"Rank must be not larger than 2 on this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCNN(\n",
    "    in_channels=in_channels,\n",
    "    intermediate_channels=intermediate_channels,\n",
    "    out_channels=out_channels,\n",
    "    conv_order_down=conv_order_down,\n",
    "    conv_order_up=conv_order_up,\n",
    "    n_layers=num_layers,\n",
    "    aggr=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_train, x_test = train_test_split(x, test_size=test_size, shuffle=False)\n",
    "\n",
    "laplacian_down_train, laplacian_down_test = train_test_split(\n",
    "    laplacian_down, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_up_train, laplacian_up_test = train_test_split(\n",
    "    laplacian_up, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninamiolane/opt/anaconda3/envs/tmx/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 1094.7131\n",
      "Test_loss: 198.5218\n",
      "Epoch: 2 loss: 103.6860\n",
      "Test_loss: 177.6109\n",
      "Epoch: 3 loss: 88.7219\n",
      "Test_loss: 121.8411\n",
      "Epoch: 4 loss: 85.7747\n",
      "Test_loss: 90.2382\n",
      "Epoch: 5 loss: 84.1926\n",
      "Test_loss: 79.0308\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "\n",
    "# select which feature to use for labeling\n",
    "simplex_order_select = 1\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x, laplacian_down, laplacian_up, y in zip(\n",
    "        x_train, laplacian_down_train, laplacian_up_train, y_train\n",
    "    ):\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "        # print(y_hat.shape)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x, laplacian_down, laplacian_up, y in zip(\n",
    "                x_test, laplacian_down_test, laplacian_up_test, y_test\n",
    "            ):\n",
    "                x = torch.tensor(x, dtype=torch.float)\n",
    "                y = torch.tensor(y, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "                loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toponetx.datasets.graph as graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Import Karate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n",
      "maximal simple order: 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)\n",
    "\n",
    "# Maximal simplex order\n",
    "max_rank = dataset.dim\n",
    "print(\"maximal simple order:\", max_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices and Hodge Laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "incidence_1 = from_sparse(incidence_1)\n",
    "incidence_2 = dataset.incidence_matrix(rank=2)\n",
    "incidence_2 = from_sparse(incidence_2)\n",
    "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "print(f\"The incidence matrix B2 has shape: {incidence_2.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Hodge Laplacians\n",
    "In the original paper, the weighted versions of the Hodge Laplacians are used. However, the current TOPONETX package does not provide this weighting feature yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_0 = dataset.hodge_laplacian_matrix(rank=0)\n",
    "laplacian_down_1 = dataset.down_laplacian_matrix(rank=1)\n",
    "laplacian_up_1 = dataset.up_laplacian_matrix(rank=1)\n",
    "laplacian_down_2 = dataset.down_laplacian_matrix(rank=2)\n",
    "laplacian_up_2 = dataset.up_laplacian_matrix(rank=2)\n",
    "\n",
    "laplacian_0 = from_sparse(laplacian_0)\n",
    "laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "laplacian_down_2 = from_sparse(laplacian_down_2)\n",
    "laplacian_up_2 = from_sparse(laplacian_up_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import signals\n",
    "#### Depending on the task, we can perform learning on any order of the simplices. Thus, the corresponding order of the input can be selected. \n",
    "\n",
    "For example, performing learning on the edges, we use the input on edges $\\mathbf{x}_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = np.stack(x_1)\n",
    "chennel_edges = x_1.shape[-1]\n",
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = np.stack(x_2)\n",
    "channel_faces = x_2.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to select the features on certain order of simplices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function to obtain features based on the input: rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_simplicial_features(dataset, rank):\n",
    "    if rank == 0:\n",
    "        which_feat = \"node_feat\"\n",
    "    elif rank == 1:\n",
    "        which_feat = \"edge_feat\"\n",
    "    elif rank == 2:\n",
    "        which_feat = \"face_feat\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"input dimension must be 0, 1 or 2, because features are supported on nodes, edges and faces\"\n",
    "        )\n",
    "\n",
    "    x = []\n",
    "    for _, v in dataset.get_simplex_attributes(which_feat).items():\n",
    "        x.append(v)\n",
    "\n",
    "    x = torch.tensor(np.stack(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define binary labels and Prepare the training-testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[-4:]\n",
    "y_train = y_true[:30]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SCNN for node classification\n",
    "Use the SCNNLayer classm we create a neural network with stacked layers, without aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCNN(\n",
    "    in_channels=in_channels,\n",
    "    intermediate_channels=intermediate_channels,\n",
    "    out_channels=out_channels,\n",
    "    conv_order_down=conv_order_down,\n",
    "    conv_order_up=conv_order_up,\n",
    "    n_layers=num_layers,\n",
    "    aggr=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a final computation that produces an output with shape $n_{\\rm{nodes}}\\times 2$, so we can compare with the binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCNN(\n",
      "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x SCNNLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Select the simplex order, i.e., on which level of simplices the learning will be performed \n",
    "\"\"\"\n",
    "rank = 1  # simplex level\n",
    "conv_order_down = 2\n",
    "conv_order_up = 2\n",
    "x = get_simplicial_features(dataset, rank)\n",
    "channels_x = x.shape[-1]\n",
    "if rank == 0:\n",
    "    laplacian_down = None\n",
    "    laplacian_up = laplacian_0  # the graph laplacian\n",
    "    conv_order_down = 0\n",
    "elif rank == 1:\n",
    "    laplacian_down = laplacian_down_1\n",
    "    laplacian_up = laplacian_up_1\n",
    "elif rank == 2:\n",
    "    laplacian_down = laplacian_down_2\n",
    "    laplacian_up = laplacian_up_2\n",
    "else:\n",
    "    raise ValueError(f\"Rank must be not larger than 2 on this dataset\")\n",
    "\n",
    "intermediate_channels = 16\n",
    "out_channels = intermediate_channels\n",
    "\n",
    "num_layers = 2\n",
    "model = SCNN(\n",
    "    in_channels=channels_nodes,\n",
    "    intermediate_channels=intermediate_channels,\n",
    "    out_channels=out_channels,\n",
    "    conv_order_down=conv_order_down,\n",
    "    conv_order_up=conv_order_up,\n",
    "    n_layers=num_layers,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 1])\n",
      "torch.Size([30, 2])\n",
      "Epoch: 1 loss: 0.8799 Train_acc: 0.0000\n",
      "torch.Size([34, 1])\n",
      "torch.Size([30, 2])\n",
      "Epoch: 2 loss: 0.8799 Train_acc: 0.0000\n",
      "Test_acc: 0.0000\n",
      "torch.Size([34, 1])\n",
      "torch.Size([30, 2])\n",
      "Epoch: 3 loss: 0.8799 Train_acc: 0.0000\n",
      "torch.Size([34, 1])\n",
      "torch.Size([30, 2])\n",
      "Epoch: 4 loss: 0.8799 Train_acc: 0.0000\n",
      "Test_acc: 0.0000\n",
      "torch.Size([34, 1])\n",
      "torch.Size([30, 2])\n",
      "Epoch: 5 loss: 0.8799 Train_acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(x, laplacian_down, laplacian_up)\n",
    "    y_hat = torch.softmax(\n",
    "        incidence_1 @ y_hat, dim=-1\n",
    "    )  # Transform features on edges to features on nodes\n",
    "    print(y_hat.shape)\n",
    "    print(y_train.shape)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float().squeeze(), torch.argmax(y_train, dim=1).float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x, laplacian_down, laplacian_up)\n",
    "            y_hat_test = torch.softmax(\n",
    "                incidence_1 @ y_hat_test, dim=-1\n",
    "            )  # Transform features on edges to features on nodes\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
