{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Convolutional Neural Network (SCNN)\n",
    "\n",
    "In this notebook, we will create and train a convolutional neural network in the simplicial complex domain, as proposed in the paper by [Yang et. al : SIMPLICIAL CONVOLUTIONAL NEURAL NETWORKS (2022)](https://arxiv.org/pdf/2110.02585.pdf). \n",
    "\n",
    "### We train the model to perform:\n",
    "    1.  Complex classification using the shrec16 benchmark dataset.\n",
    "    2.  Node classification using the karate dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simplicial Convolutional Neural Networks <a href=\"https://arxiv.org/pdf/2110.02585.pdf\">[SCNN]</a>\n",
    "\n",
    "At layer $t$, given the input simplicial (edge) feature matrix $\\mathbf{H}_t$, the SCNN layer is defined as \n",
    "$$\n",
    "    \\mathbf{H}_{t+1} = \\sigma \\Bigg[ \\mathbf{H}_t\\mathbf{\\Theta}_t + \\sum_{p_d=1}^{P_d}(\\mathbf{L}_{\\downarrow,1})^{p_d}\\mathbf{H}_t\\mathbf{\\Theta}_{t,p_d} + \\sum_{p_u=1}^{P_u}(\\mathbf{L}_{\\uparrow,1})^{p_u}\\mathbf{H}_{t}\\mathbf{\\Theta}_{t,p_u} \\Bigg]\n",
    "$$\n",
    "where $p_d$ and $p_u$ are the lower and upper convolution orders, respectively, and $\\mathbf{\\Theta}_{t,p_d}$ and $\\mathbf{\\Theta}_{t,p_u}$ are the learnable weights.\n",
    "One can use $(\\mathbf{L}_{\\uparrow,1})^{p_u}$ and $(\\mathbf{L}_{\\uparrow,1})^{p_d}$ to perform higher-order upper and lower convolutions.\n",
    "\n",
    "\n",
    "To align with the notations in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031), we can use the following to denote the above layer definition. \n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u(1 \\rightarrow 2 \\rightarrow 1)}  = ((L_{\\uparrow,1})^{p_u})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_u} $  -------- Aggregate from $p_u$-hop upper neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d(1 \\rightarrow 0 \\rightarrow 1)} = ((L_{\\downarrow,1})^{p_d})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_d} $ -------- Aggregate from $p_d$-hop lower neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m^{(1 \\rightarrow 1)}_{x \\rightarrow x} = \\theta^t \\cdot h_x^{t, (1)}$ --------  Aggregate from $x$ itself\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{L}_\\uparrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}$  -------- Collect the aggregated information from the upper neighborhood\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect the aggregated information from the lower neighborhood\n",
    "\n",
    "游릲 $\\quad m^{(1 \\rightarrow 1)}_{x} = m^{(1 \\rightarrow 1)}_{x \\rightarrow x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}  = m_x^{(1 \\rightarrow 1)} + \\sum_{p_u=1}^{P_u} m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)} + \\sum_{p_d=1}^{P_d} m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect all the aggregated information \n",
    "\n",
    "游릱 $\\quad h_x^{t+1, (1)} = \\sigma(m_x^{(1)})$ -------- Pass through the nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complex Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import toponetx.datasets as datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from topomodelx.nn.simplicial.scnn import SCNN\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import shrec dataset ##\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider using edge features for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_0 = x_0s[-1].shape[1]\n",
    "in_channels_1 = x_1s[-1].shape[1]\n",
    "in_channels_2 = x_2s[-1].shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices and Hodge Laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2  # the order of the SC is two\n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "\n",
    "laplacian_0_list = []\n",
    "laplacian_down_1_list = []\n",
    "laplacian_up_1_list = []\n",
    "laplacian_2_list = []\n",
    "\n",
    "for simplex in simplexes:\n",
    "    incidence_1 = simplex.incidence_matrix(rank=1)\n",
    "    incidence_2 = simplex.incidence_matrix(rank=2)\n",
    "    laplacian_0 = simplex.hodge_laplacian_matrix(rank=0)\n",
    "    laplacian_down_1 = simplex.down_laplacian_matrix(rank=1)\n",
    "    laplacian_up_1 = simplex.up_laplacian_matrix(rank=1)\n",
    "    laplacian_2 = simplex.hodge_laplacian_matrix(rank=2)\n",
    "\n",
    "    incidence_1 = from_sparse(incidence_1)\n",
    "    incidence_2 = from_sparse(incidence_2)\n",
    "    laplacian_0 = from_sparse(laplacian_0)\n",
    "    laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "    laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "    laplacian_2 = from_sparse(laplacian_2)\n",
    "\n",
    "    incidence_1_list.append(incidence_1)\n",
    "    incidence_2_list.append(incidence_2)\n",
    "    laplacian_0_list.append(laplacian_0)\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_up_1_list.append(laplacian_up_1)\n",
    "    laplacian_2_list.append(laplacian_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1  # simplex level\n",
    "conv_order_down = 2\n",
    "conv_order_up = 2\n",
    "hidden_channels = 4\n",
    "out_channels = 1  # num classes\n",
    "num_layers = 2\n",
    "\n",
    "# select the simplex level\n",
    "if rank == 0:\n",
    "    laplacian_down = None\n",
    "    laplacian_up = laplacian_0_list  # the graph laplacian\n",
    "    conv_order_down = 0\n",
    "    x = x_0s\n",
    "    in_channels = in_channels_0\n",
    "elif rank == 1:\n",
    "    laplacian_down = laplacian_down_1_list\n",
    "    laplacian_up = laplacian_up_1_list\n",
    "    x = x_1s\n",
    "    in_channels = in_channels_1\n",
    "elif rank == 2:\n",
    "    laplacian_down = laplacian_2_list\n",
    "    laplacian_up = None\n",
    "    x = x_2s\n",
    "    in_channels = in_channels_2\n",
    "else:\n",
    "    raise ValueError(\"Rank must be not larger than 2 on this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        conv_order_down,\n",
    "        conv_order_up,\n",
    "        n_layers=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_model = SCNN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            conv_order_down=conv_order_down,\n",
    "            conv_order_up=conv_order_up,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, laplacian_down, laplacian_up):\n",
    "        x = self.base_model(x, laplacian_down, laplacian_up)\n",
    "        x = self.linear(x)\n",
    "        one_dimensional_cells_mean = torch.nanmean(x, dim=0)\n",
    "        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] = 0\n",
    "        return one_dimensional_cells_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    conv_order_down=conv_order_down,\n",
    "    conv_order_up=conv_order_up,\n",
    "    n_layers=num_layers,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_train, x_test = train_test_split(x, test_size=test_size, shuffle=False)\n",
    "\n",
    "laplacian_down_train, laplacian_down_test = train_test_split(\n",
    "    laplacian_down, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_up_train, laplacian_up_test = train_test_split(\n",
    "    laplacian_up, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoModelX/venv_modelx/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 888.6446\n",
      "Epoch: 2 loss: 149.8124\n",
      "Test_loss: 16.6203\n",
      "Epoch: 3 loss: 158.0982\n",
      "Epoch: 4 loss: 178.9690\n",
      "Test_loss: 64.3029\n",
      "Epoch: 5 loss: 172.9724\n",
      "Epoch: 6 loss: 87.4266\n",
      "Test_loss: 110.1082\n",
      "Epoch: 7 loss: 91.9618\n",
      "Epoch: 8 loss: 89.1734\n",
      "Test_loss: 117.3142\n",
      "Epoch: 9 loss: 87.6673\n",
      "Epoch: 10 loss: 86.8797\n",
      "Test_loss: 116.6862\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# select which feature to use for labeling\n",
    "simplex_order_select = 1\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x, laplacian_down, laplacian_up, y in zip(\n",
    "        x_train, laplacian_down_train, laplacian_up_train, y_train, strict=False\n",
    "    ):\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "        # print(y_hat.shape)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x, laplacian_down, laplacian_up, y in zip(\n",
    "                x_test, laplacian_down_test, laplacian_up_test, y_test, strict=False\n",
    "            ):\n",
    "                x = torch.tensor(x, dtype=torch.float)\n",
    "                y = torch.tensor(y, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "                loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toponetx.datasets.graph as graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Import Karate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n",
      "maximal simple order: 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)\n",
    "\n",
    "# Maximal simplex order\n",
    "max_rank = dataset.dim\n",
    "print(\"maximal simple order:\", max_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices and Hodge Laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "incidence_1 = from_sparse(incidence_1)\n",
    "incidence_2 = dataset.incidence_matrix(rank=2)\n",
    "incidence_2 = from_sparse(incidence_2)\n",
    "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "print(f\"The incidence matrix B2 has shape: {incidence_2.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Hodge Laplacians\n",
    "In the original paper, the weighted versions of the Hodge Laplacians are used. However, the current TOPONETX package does not provide this weighting feature yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_0 = dataset.hodge_laplacian_matrix(rank=0)\n",
    "laplacian_down_1 = dataset.down_laplacian_matrix(rank=1)\n",
    "laplacian_up_1 = dataset.up_laplacian_matrix(rank=1)\n",
    "laplacian_down_2 = dataset.down_laplacian_matrix(rank=2)\n",
    "laplacian_up_2 = dataset.up_laplacian_matrix(rank=2)\n",
    "\n",
    "laplacian_0 = from_sparse(laplacian_0)\n",
    "laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "laplacian_down_2 = from_sparse(laplacian_down_2)\n",
    "laplacian_up_2 = from_sparse(laplacian_up_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import signals\n",
    "#### Depending on the task, we can perform learning on any order of the simplices. Thus, the corresponding order of the input can be selected. \n",
    "\n",
    "For example, performing learning on the edges, we use the input on edges $\\mathbf{x}_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = list(dataset.get_simplex_attributes(\"node_feat\").values())\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "x_1 = list(dataset.get_simplex_attributes(\"edge_feat\").values())\n",
    "x_1 = np.stack(x_1)\n",
    "chennel_edges = x_1.shape[-1]\n",
    "x_2 = list(dataset.get_simplex_attributes(\"face_feat\").values())\n",
    "x_2 = np.stack(x_2)\n",
    "channel_faces = x_2.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to select the features on certain order of simplices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function to obtain features based on the input: rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_simplicial_features(dataset, rank):\n",
    "    if rank == 0:\n",
    "        which_feat = \"node_feat\"\n",
    "    elif rank == 1:\n",
    "        which_feat = \"edge_feat\"\n",
    "    elif rank == 2:\n",
    "        which_feat = \"face_feat\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"input dimension must be 0, 1 or 2, because features are supported on nodes, edges and faces\"\n",
    "        )\n",
    "\n",
    "    x = list(dataset.get_simplex_attributes(which_feat).values())\n",
    "    return torch.tensor(np.stack(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define binary labels and Prepare the training-testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_train = y_true[:30]\n",
    "y_test = y_true[-4:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SCNN for node classification\n",
    "Use the SCNNLayer classm we create a neural network with stacked layers, without aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        conv_order_down,\n",
    "        conv_order_up,\n",
    "        n_layers=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_model = SCNN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            conv_order_down=conv_order_down,\n",
    "            conv_order_up=conv_order_up,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, laplacian_down, laplacian_up):\n",
    "        x = self.base_model(x, laplacian_down, laplacian_up)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (base_model): SCNN(\n",
      "    (layers): ModuleList(\n",
      "      (0): SCNNLayer()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Select the simplex order, i.e., on which level of simplices the learning will be performed\n",
    "\"\"\"\n",
    "rank = 1  # simplex level\n",
    "conv_order_down = 2\n",
    "conv_order_up = 2\n",
    "x = get_simplicial_features(dataset, rank)\n",
    "channels_x = x.shape[-1]\n",
    "if rank == 0:\n",
    "    laplacian_down = None\n",
    "    laplacian_up = laplacian_0  # the graph laplacian\n",
    "    conv_order_down = 0\n",
    "elif rank == 1:\n",
    "    laplacian_down = laplacian_down_1\n",
    "    laplacian_up = laplacian_up_1\n",
    "elif rank == 2:\n",
    "    laplacian_down = laplacian_down_2\n",
    "    laplacian_up = laplacian_up_2\n",
    "else:\n",
    "    raise ValueError(\"Rank must be not larger than 2 on this dataset\")\n",
    "\n",
    "hidden_channels = 16\n",
    "out_channels = 2  # num classes\n",
    "num_layers = 1\n",
    "\n",
    "model = Network(\n",
    "    in_channels=channels_x,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    conv_order_down=conv_order_down,\n",
    "    conv_order_up=conv_order_up,\n",
    "    n_layers=num_layers,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a final linear layer that produces an output with shape $n_{\\rm{nodes}}\\times 2$, so we can compare with the binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SCNN \n",
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7327 Train_acc: 0.3000\n",
      "Epoch: 2 loss: 0.7171 Train_acc: 0.7333\n",
      "Epoch: 3 loss: 0.6984 Train_acc: 0.7333\n",
      "Epoch: 4 loss: 0.6773 Train_acc: 0.7333\n",
      "Epoch: 5 loss: 0.6590 Train_acc: 0.7333\n",
      "Epoch: 6 loss: 0.6431 Train_acc: 0.7667\n",
      "Epoch: 7 loss: 0.6288 Train_acc: 0.7667\n",
      "Epoch: 8 loss: 0.6184 Train_acc: 0.8000\n",
      "Epoch: 9 loss: 0.6100 Train_acc: 0.8000\n",
      "Epoch: 10 loss: 0.6023 Train_acc: 0.8333\n",
      "Test_acc: 0.5000\n",
      "Epoch: 11 loss: 0.5951 Train_acc: 0.8333\n",
      "Epoch: 12 loss: 0.5880 Train_acc: 0.8333\n",
      "Epoch: 13 loss: 0.5796 Train_acc: 0.8333\n",
      "Epoch: 14 loss: 0.5721 Train_acc: 0.8667\n",
      "Epoch: 15 loss: 0.5693 Train_acc: 0.9000\n",
      "Epoch: 16 loss: 0.5686 Train_acc: 0.9000\n",
      "Epoch: 17 loss: 0.5679 Train_acc: 0.9000\n",
      "Epoch: 18 loss: 0.5669 Train_acc: 0.9000\n",
      "Epoch: 19 loss: 0.5655 Train_acc: 0.8667\n",
      "Epoch: 20 loss: 0.5639 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 21 loss: 0.5622 Train_acc: 0.9000\n",
      "Epoch: 22 loss: 0.5605 Train_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 loss: 0.5592 Train_acc: 0.9000\n",
      "Epoch: 24 loss: 0.5590 Train_acc: 0.9000\n",
      "Epoch: 25 loss: 0.5603 Train_acc: 0.9000\n",
      "Epoch: 26 loss: 0.5607 Train_acc: 0.9000\n",
      "Epoch: 27 loss: 0.5594 Train_acc: 0.9000\n",
      "Epoch: 28 loss: 0.5582 Train_acc: 0.9000\n",
      "Epoch: 29 loss: 0.5579 Train_acc: 0.9000\n",
      "Epoch: 30 loss: 0.5582 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 31 loss: 0.5584 Train_acc: 0.9000\n",
      "Epoch: 32 loss: 0.5584 Train_acc: 0.9000\n",
      "Epoch: 33 loss: 0.5581 Train_acc: 0.9000\n",
      "Epoch: 34 loss: 0.5576 Train_acc: 0.9000\n",
      "Epoch: 35 loss: 0.5570 Train_acc: 0.9000\n",
      "Epoch: 36 loss: 0.5565 Train_acc: 0.9000\n",
      "Epoch: 37 loss: 0.5563 Train_acc: 0.9000\n",
      "Epoch: 38 loss: 0.5564 Train_acc: 0.9000\n",
      "Epoch: 39 loss: 0.5564 Train_acc: 0.9000\n",
      "Epoch: 40 loss: 0.5562 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 41 loss: 0.5558 Train_acc: 0.9000\n",
      "Epoch: 42 loss: 0.5555 Train_acc: 0.9000\n",
      "Epoch: 43 loss: 0.5554 Train_acc: 0.9000\n",
      "Epoch: 44 loss: 0.5553 Train_acc: 0.9000\n",
      "Epoch: 45 loss: 0.5553 Train_acc: 0.9000\n",
      "Epoch: 46 loss: 0.5553 Train_acc: 0.9000\n",
      "Epoch: 47 loss: 0.5552 Train_acc: 0.9000\n",
      "Epoch: 48 loss: 0.5551 Train_acc: 0.9000\n",
      "Epoch: 49 loss: 0.5549 Train_acc: 0.9000\n",
      "Epoch: 50 loss: 0.5548 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 51 loss: 0.5547 Train_acc: 0.9000\n",
      "Epoch: 52 loss: 0.5547 Train_acc: 0.9000\n",
      "Epoch: 53 loss: 0.5546 Train_acc: 0.9000\n",
      "Epoch: 54 loss: 0.5546 Train_acc: 0.9000\n",
      "Epoch: 55 loss: 0.5546 Train_acc: 0.9000\n",
      "Epoch: 56 loss: 0.5545 Train_acc: 0.9000\n",
      "Epoch: 57 loss: 0.5545 Train_acc: 0.9000\n",
      "Epoch: 58 loss: 0.5544 Train_acc: 0.9000\n",
      "Epoch: 59 loss: 0.5544 Train_acc: 0.9000\n",
      "Epoch: 60 loss: 0.5544 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 61 loss: 0.5543 Train_acc: 0.9000\n",
      "Epoch: 62 loss: 0.5543 Train_acc: 0.9000\n",
      "Epoch: 63 loss: 0.5543 Train_acc: 0.9000\n",
      "Epoch: 64 loss: 0.5543 Train_acc: 0.9000\n",
      "Epoch: 65 loss: 0.5543 Train_acc: 0.9000\n",
      "Epoch: 66 loss: 0.5542 Train_acc: 0.9000\n",
      "Epoch: 67 loss: 0.5542 Train_acc: 0.9000\n",
      "Epoch: 68 loss: 0.5542 Train_acc: 0.9000\n",
      "Epoch: 69 loss: 0.5542 Train_acc: 0.9000\n",
      "Epoch: 70 loss: 0.5541 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 71 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 72 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 73 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 74 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 75 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 76 loss: 0.5541 Train_acc: 0.9000\n",
      "Epoch: 77 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 78 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 79 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 80 loss: 0.5540 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 81 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 82 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 83 loss: 0.5540 Train_acc: 0.9000\n",
      "Epoch: 84 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 85 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 86 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 87 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 88 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 89 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 90 loss: 0.5539 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 91 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 92 loss: 0.5539 Train_acc: 0.9000\n",
      "Epoch: 93 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 94 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 95 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 96 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 97 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 98 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 99 loss: 0.5538 Train_acc: 0.9000\n",
      "Epoch: 100 loss: 0.5538 Train_acc: 0.9000\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat_edge = model(x, laplacian_down, laplacian_up)\n",
    "    # We project the edge-level output of the model to the node-level\n",
    "    # and apply softmax fn to get the final node-level classification output\n",
    "    y_hat = torch.softmax(torch.sparse.mm(incidence_1, y_hat_edge), dim=1)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_edge_test = model(x, laplacian_down, laplacian_up)\n",
    "            # Projection to node-level\n",
    "            y_hat_test = torch.softmax(\n",
    "                torch.sparse.mm(incidence_1, y_hat_edge_test), dim=1\n",
    "            )\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
