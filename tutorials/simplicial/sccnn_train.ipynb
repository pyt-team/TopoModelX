{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SCCNN\n",
    "\n",
    "In this notebook, we will create and train a convolutional neural network in the simplicial complex domain, as proposed in the paper by [Yang et. al : Convolutional Learning on Simplicial Complexes (2023)](https://arxiv.org/abs/2301.11163). \n",
    "\n",
    "### We train the model to perform:\n",
    "    1.  Complex classification using the shrec16 benchmark dataset.\n",
    "    2.  Node classification using the karate dataset \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplicial Complex Convolutional Neural Networks <a href=\"https://arxiv.org/pdf/2301.11163.pdf\">[SCCNN]</a>\n",
    "\n",
    "SCCNN extends the SCNN to the complex domain by accounting for inter-simplicial connections, i.e., contributions from simplices of adjacent orders. \n",
    "\n",
    "For example, we consider SCCNN layers in an SC of order two. At layer $t$, given the inputs on nodes, edges and faces, $\\mathbf{h}_{t}^0,\\mathbf{h}_{t}^1$ and $\\mathbf{h}_{t}^2$, the SCCNN layer contains the following\n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^1 = \\sigma \\bigg[ \\mathbf{F}_{t,\\downarrow} \\mathbf{B}_{1}^\\top \\mathbf{h}_{t}^{0} + \\mathbf{F}_{t} \\mathbf{h}_t^1 + \\mathbf{F}_{t,\\uparrow} \\mathbf{B}_{2}  \\mathbf{h}_t^{2} \\bigg] \n",
    "$$\n",
    "where $\\mathbf{F}_t$ is the simplicial convolutional filter defined in the edge space, and $\\mathbf{F}_{t,\\downarrow}$ and $\\mathbf{F}_{t,\\uparrow}$ are the convolutional filters based on, respectively, only the lower and upper Laplacians. They are given by \n",
    "$$\n",
    "    \\mathbf{F}_{t} = {\\theta}_t + \\sum_{p_d=1}^{P_d} {\\theta}_{t,p_d} (\\mathbf{L}_{\\downarrow,1})^{p_d}  + \\sum_{p_u=1}^{P_u} {\\theta}_{t,p_u}  (\\mathbf{L}_{\\uparrow,1})^{p_u} \n",
    "$$\n",
    "$$\n",
    "    \\mathbf{F}_{t,\\downarrow} = {\\theta}_t + \\sum_{p_d=1}^{P_d} {\\theta}_{t,p_d} (\\mathbf{L}_{\\downarrow,1})^{p_d}  \n",
    "    \\text{ and } \n",
    "    \\mathbf{F}_{t,\\uparrow} = {\\theta}_t + \\sum_{p_u=1}^{P_u} {\\theta}_{t,p_u}  (\\mathbf{L}_{\\uparrow,1})^{p_u} \n",
    "$$\n",
    "\n",
    "Likewise, for the node output, we have \n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^0 = \\sigma \\bigg[ \\mathbf{F}_{t} \\mathbf{h}_t^0 + \\mathbf{F}_{t,\\uparrow} \\mathbf{B}_{1}  \\mathbf{h}_t^{1} \\bigg]\n",
    "$$\n",
    "where $\\mathbf{F}_t$ and $\\mathbf{F}_{t,\\uparrow}$ are two graph filters essentially. \n",
    "\n",
    "For the face output, we have \n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^2 = \\sigma \\bigg[ \\mathbf{F}_{t} \\mathbf{h}_{t}^{2}  + \\mathbf{F}_{t,\\downarrow} \\mathbf{B}_{2}^\\top  \\mathbf{h}_t^{1} \\bigg]\n",
    "$$\n",
    "where $\\mathbf{F}_t$ and $\\mathbf{F}_{t,\\downarrow}$ are two simplicial filters defined in the triangle (face) space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complex Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import toponetx.datasets as datasets\n",
    "from topomodelx.nn.simplicial.sccnn import SCCNN, SCCNNComplex\n",
    "from topomodelx.utils.sparse import from_sparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "### Import shrec dataset ##\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10, 7)\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0s[-1].shape[1]\n",
    "in_channels_1 = x_1s[-1].shape[1]\n",
    "in_channels_2 = x_2s[-1].shape[1]\n",
    "\n",
    "in_channels_all = (in_channels_0, in_channels_1, in_channels_2)\n",
    "print(in_channels_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices $\\mathbf{B}_1,\\mathbf{B}_2$ and Hodge Laplacians $\\mathbf{L}_0, \\mathbf{L}_1$ and $\\mathbf{L}_2$.\n",
    "\n",
    "Note that the original paper considered the weighted versions of these operators. However, the current TOPONETX package does not provide such feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2  # the order of the SC is two\n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "\n",
    "laplacian_0_list = []\n",
    "laplacian_down_1_list = []\n",
    "laplacian_up_1_list = []\n",
    "laplacian_2_list = []\n",
    "\n",
    "for simplex in simplexes:\n",
    "    incidence_1 = simplex.incidence_matrix(rank=1)\n",
    "    incidence_2 = simplex.incidence_matrix(rank=2)\n",
    "    laplacian_0 = simplex.hodge_laplacian_matrix(rank=0)\n",
    "    laplacian_down_1 = simplex.down_laplacian_matrix(rank=1)\n",
    "    laplacian_up_1 = simplex.up_laplacian_matrix(rank=1)\n",
    "    laplacian_2 = simplex.hodge_laplacian_matrix(rank=2)\n",
    "\n",
    "    incidence_1 = from_sparse(incidence_1)\n",
    "    incidence_2 = from_sparse(incidence_2)\n",
    "    laplacian_0 = from_sparse(laplacian_0)\n",
    "    laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "    laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "    laplacian_2 = from_sparse(laplacian_2)\n",
    "\n",
    "    incidence_1_list.append(incidence_1)\n",
    "    incidence_2_list.append(incidence_2)\n",
    "    laplacian_0_list.append(laplacian_0)\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_up_1_list.append(laplacian_up_1)\n",
    "    laplacian_2_list.append(laplacian_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCCNN(\n",
      "  (in_linear_0): Linear(in_features=6, out_features=16, bias=True)\n",
      "  (in_linear_1): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (in_linear_2): Linear(in_features=7, out_features=16, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x SCCNNLayer()\n",
      "  )\n",
      "  (out_linear_0): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (out_linear_1): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (out_linear_2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoModelX/venv_modelx/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "conv_order = 2\n",
    "intermediate_channels_all = (16, 16, 16)\n",
    "out_channels_all = intermediate_channels_all\n",
    "num_layers = 2\n",
    "\n",
    "model = SCCNN(\n",
    "    in_channels_all=in_channels_all,\n",
    "    intermediate_channels_all=intermediate_channels_all,\n",
    "    out_channels_all=out_channels_all,\n",
    "    conv_order=conv_order,\n",
    "    sc_order=max_rank,\n",
    "    num_classes=1,\n",
    "    n_layers=num_layers,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss(size_average=True, reduction=\"mean\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, shuffle=False)\n",
    "x_2_train, x_2_test = train_test_split(x_2s, test_size=test_size, shuffle=False)\n",
    "\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "incidence_2_train, incidence_2_test = train_test_split(\n",
    "    incidence_2_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_0_train, laplacian_0_test = train_test_split(\n",
    "    laplacian_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_down_1_train, laplacian_down_1_test = train_test_split(\n",
    "    laplacian_down_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_up_1_train, laplacian_up_1_test = train_test_split(\n",
    "    laplacian_up_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_2_train, laplacian_2_test = train_test_split(\n",
    "    laplacian_2_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the SCCNN using low amount of epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoModelX/venv_modelx/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 122747.1568\n",
      "Test_loss: 240.8677\n",
      "Epoch: 2 loss: 935.8619\n",
      "Test_loss: 537.9067\n",
      "Epoch: 3 loss: 266.6894\n",
      "Test_loss: 300.4679\n",
      "Epoch: 4 loss: 189.6432\n",
      "Test_loss: 233.4675\n",
      "Epoch: 5 loss: 157.9087\n",
      "Test_loss: 212.7377\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for (\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        incidence_1,\n",
    "        incidence_2,\n",
    "        laplacian_0,\n",
    "        laplacian_down_1,\n",
    "        laplacian_up_1,\n",
    "        laplacian_2,\n",
    "        y,\n",
    "    ) in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        x_2_train,\n",
    "        incidence_1_train,\n",
    "        incidence_2_train,\n",
    "        laplacian_0_train,\n",
    "        laplacian_down_1_train,\n",
    "        laplacian_up_1_train,\n",
    "        laplacian_2_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0 = torch.tensor(x_0)\n",
    "        x_1 = torch.tensor(x_1)\n",
    "        x_2 = torch.tensor(x_2)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        x_all = (x_0.float(), x_1.float(), x_2.float())\n",
    "        laplacian_all = (laplacian_0, laplacian_down_1, laplacian_up_1, laplacian_2)\n",
    "        incidence_all = (incidence_1, incidence_2)\n",
    "\n",
    "        y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "\n",
    "        # print(y_hat)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for (\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                incidence_1,\n",
    "                incidence_2,\n",
    "                laplacian_0,\n",
    "                laplacian_down_1,\n",
    "                laplacian_up_1,\n",
    "                laplacian_2,\n",
    "                y,\n",
    "            ) in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                x_2_test,\n",
    "                incidence_1_test,\n",
    "                incidence_2_test,\n",
    "                laplacian_0_test,\n",
    "                laplacian_down_1_test,\n",
    "                laplacian_up_1_test,\n",
    "                laplacian_2_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = torch.tensor(x_0)\n",
    "                x_1 = torch.tensor(x_1)\n",
    "                x_2 = torch.tensor(x_2)\n",
    "                y = torch.tensor(y, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                x_all = (x_0.float(), x_1.float(), x_2.float())\n",
    "                laplacian_all = (\n",
    "                    laplacian_0,\n",
    "                    laplacian_down_1,\n",
    "                    laplacian_up_1,\n",
    "                    laplacian_2,\n",
    "                )\n",
    "                incidence_all = (incidence_1, incidence_2)\n",
    "\n",
    "                y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "\n",
    "                loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccnn import SCCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)\n",
    "max_rank = dataset.dim\n",
    "print(max_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices $\\mathbf{B}_1,\\mathbf{B}_2$ and Hodge Laplacians $\\mathbf{L}_0, \\mathbf{L}_1$ and $\\mathbf{L}_2$.\n",
    "\n",
    "Note that the original paper considered the weighted versions of these operators. However, the current TOPONETX package does not provide such feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B1 has shape: (34, 78).\n",
      "The incidence matrix B2 has shape: (78, 45).\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "incidence_2 = dataset.incidence_matrix(rank=2)\n",
    "\n",
    "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "print(f\"The incidence matrix B2 has shape: {incidence_2.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_0 = dataset.hodge_laplacian_matrix(rank=0)\n",
    "laplacian_down_1 = dataset.down_laplacian_matrix(rank=1)\n",
    "laplacian_up_1 = dataset.up_laplacian_matrix(rank=1)\n",
    "laplacian_down_2 = dataset.down_laplacian_matrix(rank=2)\n",
    "laplacian_up_2 = dataset.up_laplacian_matrix(rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_0 = from_sparse(laplacian_0)\n",
    "laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "laplacian_up_1 = from_sparse(laplacian_up_1)\n",
    "laplacian_down_2 = from_sparse(laplacian_down_2)\n",
    "laplacian_up_2 = from_sparse(laplacian_up_2)\n",
    "\n",
    "incidence_1 = from_sparse(incidence_1)\n",
    "incidence_2 = from_sparse(incidence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. Here, we have in_channels = channels_nodes $ = 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function to obtain features based on the input: rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_simplicial_features(dataset, rank):\n",
    "    if rank == 0:\n",
    "        which_feat = \"node_feat\"\n",
    "    elif rank == 1:\n",
    "        which_feat = \"edge_feat\"\n",
    "    elif rank == 2:\n",
    "        which_feat = \"face_feat\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"input dimension must be 0, 1 or 2, because features are supported on nodes, edges and faces\"\n",
    "        )\n",
    "\n",
    "    x = []\n",
    "    for _, v in dataset.get_simplex_attributes(which_feat).items():\n",
    "        x.append(v)\n",
    "\n",
    "    x = torch.tensor(np.stack(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = get_simplicial_features(dataset, rank=0)\n",
    "x_1 = get_simplicial_features(dataset, rank=1)\n",
    "x_2 = get_simplicial_features(dataset, rank=2)\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the first four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[-4:]\n",
    "y_train = y_true[:30]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Obtain the initial features on all simplices\"\"\"\n",
    "x_all = (x_0, x_1, x_2)\n",
    "\n",
    "conv_order = 2\n",
    "in_channels_all = (x_0.shape[-1], x_1.shape[-1], x_2.shape[-1])\n",
    "intermediate_channels_all = (16, 16, 16)\n",
    "out_channels_all = intermediate_channels_all\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "\n",
    "laplacian_all = (\n",
    "    laplacian_0,\n",
    "    laplacian_down_1,\n",
    "    laplacian_up_1,\n",
    "    laplacian_down_2,\n",
    "    laplacian_up_2,\n",
    ")\n",
    "\n",
    "incidence_all = (incidence_1, incidence_2)\n",
    "\n",
    "model = SCCNNComplex(\n",
    "    in_channels_all=in_channels_all,\n",
    "    intermediate_channels_all=intermediate_channels_all,\n",
    "    out_channels_all=out_channels_all,\n",
    "    conv_order=conv_order,\n",
    "    sc_order=max_rank,\n",
    "    n_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 14.1251 Train_acc: 0.6333\n",
      "Epoch: 2 loss: 78.4961 Train_acc: 0.3667\n",
      "Epoch: 3 loss: 79.1704 Train_acc: 0.3667\n",
      "Epoch: 4 loss: 79.3702 Train_acc: 0.3667\n",
      "Epoch: 5 loss: 79.4000 Train_acc: 0.3667\n",
      "Epoch: 6 loss: 79.3739 Train_acc: 0.3667\n",
      "Epoch: 7 loss: 79.3400 Train_acc: 0.3667\n",
      "Epoch: 8 loss: 79.3188 Train_acc: 0.3667\n",
      "Epoch: 9 loss: 79.3181 Train_acc: 0.3667\n",
      "Epoch: 10 loss: 79.3396 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 11 loss: 79.3820 Train_acc: 0.3667\n",
      "Epoch: 12 loss: 79.4425 Train_acc: 0.3667\n",
      "Epoch: 13 loss: 79.5180 Train_acc: 0.3667\n",
      "Epoch: 14 loss: 79.6053 Train_acc: 0.3667\n",
      "Epoch: 15 loss: 79.7014 Train_acc: 0.3667\n",
      "Epoch: 16 loss: 79.7811 Train_acc: 0.3667\n",
      "Epoch: 17 loss: 79.8250 Train_acc: 0.3667\n",
      "Epoch: 18 loss: 79.8697 Train_acc: 0.3667\n",
      "Epoch: 19 loss: 79.9146 Train_acc: 0.3667\n",
      "Epoch: 20 loss: 79.9591 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 21 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 22 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 23 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 24 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 25 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 26 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 27 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 28 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 29 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 30 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 31 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 32 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 33 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 34 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 35 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 36 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 37 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 38 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 39 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 40 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 41 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 42 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 43 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 44 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 45 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 46 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 47 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 48 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 49 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 50 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 51 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 52 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 53 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 54 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 55 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 56 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 57 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 58 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 59 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 60 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 61 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 62 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 63 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 64 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 65 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 66 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 67 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 68 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 69 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 70 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 71 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 72 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 73 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 74 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 75 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 76 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 77 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 78 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 79 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 80 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 81 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 82 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 83 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 84 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 85 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 86 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 87 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 88 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 89 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 90 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 91 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 92 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 93 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 94 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 95 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 96 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 97 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 98 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 99 loss: 80.0000 Train_acc: 0.3667\n",
      "Epoch: 100 loss: 80.0000 Train_acc: 0.3667\n",
      "Test_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "    loss = torch.nn.functional.binary_cross_entropy(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x_all, laplacian_all, incidence_all)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
