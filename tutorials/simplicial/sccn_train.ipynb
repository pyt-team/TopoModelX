{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Convolutional Network (SCCN)\n",
    "\n",
    "We create a SCCN model a la [Yang et al : Efficient Representation Learning for Higher-Order Data with\n",
    "Simplicial Complexes (LoG 2022)](https://proceedings.mlr.press/v198/yang22a/yang22a.pdf)\n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The model operates on cells of all ranks up to some max rank $r_\\mathrm{max}$.\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r \\rightarrow r)} = (H_{r})_{xy} \\cdot h^{t,(r)}_y \\cdot \\Theta^{t,(r\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r-1 \\rightarrow r)} = (B_{r}^T)_{xy} \\cdot h^{t,(r-1)}_y \\cdot \\Theta^{t,(r-1\\to r)}$,    (for $1\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r+1 \\rightarrow r)} = (B_{r+1})_{xy} \\cdot h^{t,(r+1)}_y \\cdot \\Theta^{t,(r+1\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}-1$)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r \\rightarrow r)}  = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)\\bigcup \\mathcal{L}_\\uparrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r-1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r-1 \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r+1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(r)}  = m_x^{(r \\rightarrow r)} + m_x^{(r-1 \\rightarrow r)} + m_x^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(r)}  = \\sigma(m_x^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccn_layer import SCCNLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain.\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. The feature dimension is `feat_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=8)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 3  # There are features up to tetrahedron order in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix H0 has shape: torch.Size([34, 34]).\n",
      "The adjacency matrix H1 has shape: torch.Size([78, 78]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix H2 has shape: torch.Size([45, 45]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The adjacency matrix H3 has shape: torch.Size([11, 11]).\n",
      "The incidence matrix B3 has shape: torch.Size([45, 11]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georg/Programs/anaconda3/envs/recent/lib/python3.10/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def sparse_to_torch(X):\n",
    "    return torch.from_numpy(X.todense()).to_sparse()\n",
    "\n",
    "incidences = {\n",
    "    f'rank_{r}': sparse_to_torch(\n",
    "        dataset.incidence_matrix(rank=r)\n",
    "    )\n",
    "    for r in range(1, max_rank + 1)\n",
    "}\n",
    "\n",
    "adjacencies = {}\n",
    "adjacencies['rank_0'] = (sparse_to_torch(dataset.adjacency_matrix(rank=0)) \n",
    "                  + torch.eye(dataset.shape[0]).to_sparse())\n",
    "for r in range(1, max_rank):\n",
    "    adjacencies[f'rank_{r}'] = (\n",
    "        sparse_to_torch(\n",
    "            dataset.adjacency_matrix(rank=r)\n",
    "            + dataset.coadjacency_matrix(rank=r)\n",
    "        )\n",
    "        + 2 * torch.eye(dataset.shape[r]).to_sparse()\n",
    "    )\n",
    "adjacencies[f'rank_{max_rank}'] = (sparse_to_torch(dataset.coadjacency_matrix(rank=max_rank)) \n",
    "                         + torch.eye(dataset.shape[max_rank]).to_sparse())\n",
    "\n",
    "for r in range(max_rank + 1):\n",
    "    print(f\"The adjacency matrix H{r} has shape: {adjacencies[f'rank_{r}'].shape}.\")\n",
    "    if r > 0:\n",
    "        print(f\"The incidence matrix B{r} has shape: {incidences[f'rank_{r}'].shape}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "We import the features at each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = torch.tensor(np.stack(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"tetrahedron_feat\").items():\n",
    "    x_3.append(v)\n",
    "x_3 = torch.tensor(np.stack(x_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tetrahedrons with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_3.shape[0]} tetrahedrons with features of dimension {x_3.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'rank_0': x_0, 'rank_1': x_1, 'rank_2': x_2, 'rank_3': x_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train = torch.from_numpy(y[:30])\n",
    "y_test = torch.from_numpy(y[30:])\n",
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCCN(torch.nn.Module):\n",
    "    \"\"\"Simplicial Complex Convolutional Network Implementation for binary node classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels : int\n",
    "        Dimension of features\n",
    "    n_layers : int\n",
    "        Number of message passing layers.\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    update_func : str\n",
    "        Activation function used in aggregation layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, max_rank, n_layers=2, n_classes=2, update_func=\"sigmoid\"):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                SCCNLayer(\n",
    "                    channels=channels,\n",
    "                    max_rank=max_rank,\n",
    "                    update_func=update_func,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        assert n_classes >= 2, \"n_classes must be >= 2\"\n",
    "\n",
    "        if n_classes == 2: \n",
    "            n_classes = 1\n",
    "\n",
    "        self.linear = torch.nn.Linear(channels, n_classes)\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, features, incidences, adjacencies):\n",
    "        \"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        features: Dict[int, torch.Tensor],\n",
    "                length=max_rank+1,\n",
    "                shape=[n_rank_r_cells, channels]\n",
    "            Input features on the cells of the simplicial complex.\n",
    "        incidences : Dict[int, torch.sparse],\n",
    "                length=max_rank,\n",
    "                shape=[n_rank_r_minus_1_cells, n_rank_r_cells]\n",
    "            Incidence matrices :math:`B_r` mapping r-cells to (r-1)-cells.\n",
    "        adjacencies : Dict[int, torch.sparse],\n",
    "                length=max_rank,\n",
    "                shape=[n_rank_r_cells, n_rank_r_cells]\n",
    "            Adjacency matrices :math:`H_r` mapping cells to cells\n",
    "                via lower and upper cells.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            If n_classes > 2:\n",
    "                shape = [n_nodes, n_classes]\n",
    "                Logits assigned to each node.\n",
    "            If n_classes == 2:\n",
    "                shape = [n_nodes,]\n",
    "                Binary logits assigned to each node.\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            features = layer(features, incidences, adjacencies)\n",
    "        logits = self.linear(features['rank_0']).squeeze()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCCN(\n",
    "    channels=channels_nodes,\n",
    "    max_rank=max_rank,\n",
    "    n_layers=5,\n",
    "    update_func=\"sigmoid\",\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs. Typically achieves 100% train accuracy. Test accuracy is more arbitrary between runs, likely due to the small dataset set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.6779 Train_acc: 0.5667\n",
      "Epoch: 2 loss: 0.6273 Train_acc: 0.7667\n",
      "Epoch: 3 loss: 0.6076 Train_acc: 0.6667\n",
      "Epoch: 4 loss: 0.5821 Train_acc: 0.6667\n",
      "Epoch: 5 loss: 0.5566 Train_acc: 0.7000\n",
      "Epoch: 6 loss: 0.5347 Train_acc: 0.7667\n",
      "Epoch: 7 loss: 0.5144 Train_acc: 0.7333\n",
      "Epoch: 8 loss: 0.4904 Train_acc: 0.7333\n",
      "Epoch: 9 loss: 0.4728 Train_acc: 0.7667\n",
      "Epoch: 10 loss: 0.4449 Train_acc: 0.8000\n",
      "Epoch: 11 loss: 0.3994 Train_acc: 0.8333\n",
      "Epoch: 12 loss: 0.4043 Train_acc: 0.8000\n",
      "Epoch: 13 loss: 0.3834 Train_acc: 0.8333\n",
      "Epoch: 14 loss: 0.3222 Train_acc: 0.8333\n",
      "Epoch: 15 loss: 0.3318 Train_acc: 0.9000\n",
      "Epoch: 16 loss: 0.2878 Train_acc: 0.9000\n",
      "Epoch: 17 loss: 0.2678 Train_acc: 0.9000\n",
      "Epoch: 18 loss: 0.2527 Train_acc: 0.9000\n",
      "Epoch: 19 loss: 0.2272 Train_acc: 0.9000\n",
      "Epoch: 20 loss: 0.2147 Train_acc: 0.9000\n",
      "Epoch: 21 loss: 0.1983 Train_acc: 0.9000\n",
      "Epoch: 22 loss: 0.1781 Train_acc: 0.9000\n",
      "Epoch: 23 loss: 0.1628 Train_acc: 0.9000\n",
      "Epoch: 24 loss: 0.1588 Train_acc: 0.9000\n",
      "Epoch: 25 loss: 0.1638 Train_acc: 0.9000\n",
      "Epoch: 26 loss: 0.1293 Train_acc: 0.9667\n",
      "Epoch: 27 loss: 0.1449 Train_acc: 0.9333\n",
      "Epoch: 28 loss: 0.1150 Train_acc: 1.0000\n",
      "Epoch: 29 loss: 0.1067 Train_acc: 1.0000\n",
      "Epoch: 30 loss: 0.0912 Train_acc: 1.0000\n",
      "Epoch: 31 loss: 0.0805 Train_acc: 1.0000\n",
      "Epoch: 32 loss: 0.0722 Train_acc: 1.0000\n",
      "Epoch: 33 loss: 0.0601 Train_acc: 1.0000\n",
      "Epoch: 34 loss: 0.0431 Train_acc: 1.0000\n",
      "Epoch: 35 loss: 0.0412 Train_acc: 1.0000\n",
      "Epoch: 36 loss: 0.0382 Train_acc: 1.0000\n",
      "Epoch: 37 loss: 0.0336 Train_acc: 1.0000\n",
      "Epoch: 38 loss: 0.0248 Train_acc: 1.0000\n",
      "Epoch: 39 loss: 0.0182 Train_acc: 1.0000\n",
      "Epoch: 40 loss: 0.0147 Train_acc: 1.0000\n",
      "Epoch: 41 loss: 0.0132 Train_acc: 1.0000\n",
      "Epoch: 42 loss: 0.0121 Train_acc: 1.0000\n",
      "Epoch: 43 loss: 0.0114 Train_acc: 1.0000\n",
      "Epoch: 44 loss: 0.0106 Train_acc: 1.0000\n",
      "Epoch: 45 loss: 0.0093 Train_acc: 1.0000\n",
      "Epoch: 46 loss: 0.0080 Train_acc: 1.0000\n",
      "Epoch: 47 loss: 0.0070 Train_acc: 1.0000\n",
      "Epoch: 48 loss: 0.0062 Train_acc: 1.0000\n",
      "Epoch: 49 loss: 0.0057 Train_acc: 1.0000\n",
      "Epoch: 50 loss: 0.0056 Train_acc: 1.0000\n",
      "Test_acc: 0.2500\n",
      "Epoch: 51 loss: 0.0053 Train_acc: 1.0000\n",
      "Epoch: 52 loss: 0.0047 Train_acc: 1.0000\n",
      "Epoch: 53 loss: 0.0040 Train_acc: 1.0000\n",
      "Epoch: 54 loss: 0.0033 Train_acc: 1.0000\n",
      "Epoch: 55 loss: 0.0029 Train_acc: 1.0000\n",
      "Epoch: 56 loss: 0.0027 Train_acc: 1.0000\n",
      "Epoch: 57 loss: 0.0026 Train_acc: 1.0000\n",
      "Epoch: 58 loss: 0.0025 Train_acc: 1.0000\n",
      "Epoch: 59 loss: 0.0023 Train_acc: 1.0000\n",
      "Epoch: 60 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 61 loss: 0.0021 Train_acc: 1.0000\n",
      "Epoch: 62 loss: 0.0020 Train_acc: 1.0000\n",
      "Epoch: 63 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 64 loss: 0.0018 Train_acc: 1.0000\n",
      "Epoch: 65 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 66 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 67 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 68 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 69 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 70 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 71 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 72 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 73 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 74 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 75 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 76 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 77 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 78 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 79 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 80 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 81 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 82 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 83 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 84 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 85 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 86 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 87 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 88 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 89 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 90 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 91 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 92 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 93 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 94 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 95 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 96 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 97 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 98 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 99 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 100 loss: 0.0007 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 101 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 102 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 103 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 104 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 105 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 106 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 107 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 108 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 109 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 110 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 111 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 112 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 113 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 114 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 115 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 116 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 117 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 118 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 119 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 120 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 121 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 122 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 123 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 124 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 125 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 126 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 127 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 128 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 129 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 130 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 131 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 132 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 133 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 134 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 135 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 136 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 137 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 138 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 139 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 140 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 141 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 142 loss: 0.0006 Train_acc: 1.0000\n",
      "Epoch: 143 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 144 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 145 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 146 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 147 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 148 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 149 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 150 loss: 0.0005 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 151 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 152 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 153 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 154 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 155 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 156 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 157 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 158 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 159 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 160 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 161 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 162 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 163 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 164 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 165 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 166 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 167 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 168 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 169 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 170 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 171 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 172 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 173 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 174 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 175 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 176 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 177 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 178 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 179 loss: 0.0005 Train_acc: 1.0000\n",
      "Epoch: 180 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 181 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 182 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 183 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 184 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 185 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 186 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 187 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 188 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 189 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 190 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 191 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 192 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 193 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 194 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 195 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 196 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 197 loss: 0.0004 Train_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 199 loss: 0.0004 Train_acc: 1.0000\n",
      "Epoch: 200 loss: 0.0004 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 50\n",
    "num_epochs = 200\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(features, incidences, adjacencies)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    y_pred = (y_hat > 0).long()\n",
    "    accuracy = (y_pred[:len(y_train)] == y_train).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(features, incidences, adjacencies)\n",
    "            y_pred_test =(y_hat_test > 0).long()\n",
    "            test_accuracy = (y_pred_test[-len(y_test):] == y_test).float().mean().item()\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
