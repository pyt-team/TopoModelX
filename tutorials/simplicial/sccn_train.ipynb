{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Convolutional Network (SCCN)\n",
    "\n",
    "We create a SCCN model a la [Yang et al : Efficient Representation Learning for Higher-Order Data with\n",
    "Simplicial Complexes (LoG 2022)](https://proceedings.mlr.press/v198/yang22a/yang22a.pdf)\n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The model operates on cells of all ranks up to some max rank $r_\\mathrm{max}$.\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r \\rightarrow r)} = (H_{r})_{xy} \\cdot h^{t,(r)}_y \\cdot \\Theta^{t,(r\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r-1 \\rightarrow r)} = (B_{r}^T)_{xy} \\cdot h^{t,(r-1)}_y \\cdot \\Theta^{t,(r-1\\to r)}$,    (for $1\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r+1 \\rightarrow r)} = (B_{r+1})_{xy} \\cdot h^{t,(r+1)}_y \\cdot \\Theta^{t,(r+1\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}-1$)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r \\rightarrow r)}  = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)\\bigcup \\mathcal{L}_\\uparrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r-1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r-1 \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r+1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(r)}  = m_x^{(r \\rightarrow r)} + m_x^{(r-1 \\rightarrow r)} + m_x^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(r)}  = \\sigma(m_x^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import toponetx.datasets.graph as graph\n",
    "import torch\n",
    "\n",
    "from topomodelx.nn.simplicial.sccn import SCCN\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain.\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. The feature dimension is `feat_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=2)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Our implementation allows for features on cells up to an arbitrary maximum rank. In this dataset, we can use at most `max_rank = 3`, which is what we choose.\n",
    "\n",
    "We define incidence and adjacency matrices up to the max rank and put them in dictionaries indexed by the rank, as is expected by the `SCCNLayer`.\n",
    "The form of tha adjacency and incidence matrices could be chosen arbitrarily, here we follow the original formulation by Yang et al. quite closely and select the adjacencies as r-Hodge Laplacians $H_r$, summed with $2I$ (or just $I$ for $r\\in\\{0, r_\\mathrm{max}\\}$) to allow cells to pass messages to themselves. The incidence matrices are the usual boundary matrices $B_r$.\n",
    "One could additionally weight/normalize these matrices as suggested by Yang et al., but we refrain from doing this for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 3  # There are features up to tetrahedron order in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix H0 has shape: torch.Size([34, 34]).\n",
      "The adjacency matrix H1 has shape: torch.Size([78, 78]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix H2 has shape: torch.Size([45, 45]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The adjacency matrix H3 has shape: torch.Size([11, 11]).\n",
      "The incidence matrix B3 has shape: torch.Size([45, 11]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoModelX/venv_modelx/lib/python3.11/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def sparse_to_torch(X):\n",
    "    return from_sparse(X)\n",
    "\n",
    "\n",
    "incidences = {\n",
    "    f\"rank_{r}\": sparse_to_torch(dataset.incidence_matrix(rank=r))\n",
    "    for r in range(1, max_rank + 1)\n",
    "}\n",
    "\n",
    "adjacencies = {}\n",
    "adjacencies[\"rank_0\"] = (\n",
    "    sparse_to_torch(dataset.adjacency_matrix(rank=0))\n",
    "    + torch.eye(dataset.shape[0]).to_sparse()\n",
    ")\n",
    "for r in range(1, max_rank):\n",
    "    adjacencies[f\"rank_{r}\"] = (\n",
    "        sparse_to_torch(\n",
    "            dataset.adjacency_matrix(rank=r) + dataset.coadjacency_matrix(rank=r)\n",
    "        )\n",
    "        + 2 * torch.eye(dataset.shape[r]).to_sparse()\n",
    "    )\n",
    "adjacencies[f\"rank_{max_rank}\"] = (\n",
    "    sparse_to_torch(dataset.coadjacency_matrix(rank=max_rank))\n",
    "    + torch.eye(dataset.shape[max_rank]).to_sparse()\n",
    ")\n",
    "\n",
    "for r in range(max_rank + 1):\n",
    "    print(f\"The adjacency matrix H{r} has shape: {adjacencies[f'rank_{r}'].shape}.\")\n",
    "    if r > 0:\n",
    "        print(f\"The incidence matrix B{r} has shape: {incidences[f'rank_{r}'].shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "We import the features at each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = list(dataset.get_simplex_attributes(\"node_feat\").values())\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = list(dataset.get_simplex_attributes(\"edge_feat\").values())\n",
    "x_1 = torch.tensor(np.stack(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = list(dataset.get_simplex_attributes(\"face_feat\").values())\n",
    "x_2 = torch.tensor(np.stack(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = list(dataset.get_simplex_attributes(\"tetrahedron_feat\").values())\n",
    "x_3 = torch.tensor(np.stack(x_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tetrahedrons with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"There are {x_3.shape[0]} tetrahedrons with features of dimension {x_3.shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are organized in a dictionary keeping track of their rank, similar to the adjacencies/incidences earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\"rank_0\": x_0, \"rank_1\": x_1, \"rank_2\": x_2, \"rank_3\": x_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_train = y_true[:30]\n",
    "y_test = y_true[-4:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the SAN class, we create our neural network with stacked layers. Given the considered dataset and task (Karate Club, node classification), a linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, channels, out_channels, max_rank, n_layers=2, update_func=\"sigmoid\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_model = SCCN(\n",
    "            channels=channels,\n",
    "            max_rank=max_rank,\n",
    "            n_layers=n_layers,\n",
    "            update_func=update_func,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(channels, out_channels)\n",
    "\n",
    "    def forward(self, features, incidences, adjacencies):\n",
    "        features = self.base_model(features, incidences, adjacencies)\n",
    "        x = self.linear(features[\"rank_0\"])\n",
    "        return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "out_channels = 2\n",
    "\n",
    "model = Network(\n",
    "    channels=channels_nodes,\n",
    "    out_channels=out_channels,\n",
    "    max_rank=max_rank,\n",
    "    n_layers=n_layers,\n",
    "    update_func=\"sigmoid\",\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "The following cell performs the training, looping over the network for a low number of epochs. Test accuracy is more arbitrary between runs, likely due to the small dataset set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7267 Train_acc: 0.4333\n",
      "Epoch: 2 loss: 0.7176 Train_acc: 0.7000\n",
      "Epoch: 3 loss: 0.7089 Train_acc: 0.5000\n",
      "Epoch: 4 loss: 0.7013 Train_acc: 0.5667\n",
      "Epoch: 5 loss: 0.6953 Train_acc: 0.5667\n",
      "Epoch: 6 loss: 0.6906 Train_acc: 0.5667\n",
      "Epoch: 7 loss: 0.6867 Train_acc: 0.5667\n",
      "Epoch: 8 loss: 0.6829 Train_acc: 0.5667\n",
      "Epoch: 9 loss: 0.6789 Train_acc: 0.5667\n",
      "Epoch: 10 loss: 0.6745 Train_acc: 0.5667\n",
      "Test_acc: 0.0000\n",
      "Epoch: 11 loss: 0.6696 Train_acc: 0.5667\n",
      "Epoch: 12 loss: 0.6645 Train_acc: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 loss: 0.6592 Train_acc: 0.5667\n",
      "Epoch: 14 loss: 0.6538 Train_acc: 0.4667\n",
      "Epoch: 15 loss: 0.6486 Train_acc: 0.8000\n",
      "Epoch: 16 loss: 0.6437 Train_acc: 0.8000\n",
      "Epoch: 17 loss: 0.6392 Train_acc: 0.8000\n",
      "Epoch: 18 loss: 0.6351 Train_acc: 0.8000\n",
      "Epoch: 19 loss: 0.6316 Train_acc: 0.8000\n",
      "Epoch: 20 loss: 0.6286 Train_acc: 0.7667\n",
      "Test_acc: 0.7500\n",
      "Epoch: 21 loss: 0.6260 Train_acc: 0.7667\n",
      "Epoch: 22 loss: 0.6238 Train_acc: 0.7667\n",
      "Epoch: 23 loss: 0.6220 Train_acc: 0.7667\n",
      "Epoch: 24 loss: 0.6203 Train_acc: 0.7667\n",
      "Epoch: 25 loss: 0.6188 Train_acc: 0.7667\n",
      "Epoch: 26 loss: 0.6173 Train_acc: 0.7667\n",
      "Epoch: 27 loss: 0.6158 Train_acc: 0.7667\n",
      "Epoch: 28 loss: 0.6142 Train_acc: 0.7667\n",
      "Epoch: 29 loss: 0.6125 Train_acc: 0.7667\n",
      "Epoch: 30 loss: 0.6106 Train_acc: 0.8000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 31 loss: 0.6084 Train_acc: 0.8000\n",
      "Epoch: 32 loss: 0.6061 Train_acc: 0.8000\n",
      "Epoch: 33 loss: 0.6035 Train_acc: 0.8333\n",
      "Epoch: 34 loss: 0.6007 Train_acc: 0.8333\n",
      "Epoch: 35 loss: 0.5975 Train_acc: 0.8333\n",
      "Epoch: 36 loss: 0.5942 Train_acc: 0.8333\n",
      "Epoch: 37 loss: 0.5905 Train_acc: 0.8667\n",
      "Epoch: 38 loss: 0.5860 Train_acc: 0.8667\n",
      "Epoch: 39 loss: 0.5706 Train_acc: 0.8667\n",
      "Epoch: 40 loss: 0.5554 Train_acc: 0.9333\n",
      "Test_acc: 0.7500\n",
      "Epoch: 41 loss: 0.5529 Train_acc: 0.9333\n",
      "Epoch: 42 loss: 0.5500 Train_acc: 0.9333\n",
      "Epoch: 43 loss: 0.5448 Train_acc: 0.9667\n",
      "Epoch: 44 loss: 0.5443 Train_acc: 0.9667\n",
      "Epoch: 45 loss: 0.5422 Train_acc: 0.9667\n",
      "Epoch: 46 loss: 0.5390 Train_acc: 0.9667\n",
      "Epoch: 47 loss: 0.5368 Train_acc: 0.9667\n",
      "Epoch: 48 loss: 0.5358 Train_acc: 0.9667\n",
      "Epoch: 49 loss: 0.5351 Train_acc: 0.9667\n",
      "Epoch: 50 loss: 0.5342 Train_acc: 0.9667\n",
      "Test_acc: 0.7500\n",
      "Epoch: 51 loss: 0.5329 Train_acc: 0.9667\n",
      "Epoch: 52 loss: 0.5316 Train_acc: 0.9667\n",
      "Epoch: 53 loss: 0.5305 Train_acc: 0.9667\n",
      "Epoch: 54 loss: 0.5298 Train_acc: 0.9667\n",
      "Epoch: 55 loss: 0.5292 Train_acc: 0.9667\n",
      "Epoch: 56 loss: 0.5287 Train_acc: 0.9667\n",
      "Epoch: 57 loss: 0.5283 Train_acc: 0.9667\n",
      "Epoch: 58 loss: 0.5278 Train_acc: 0.9667\n",
      "Epoch: 59 loss: 0.5274 Train_acc: 0.9667\n",
      "Epoch: 60 loss: 0.5269 Train_acc: 0.9667\n",
      "Test_acc: 0.5000\n",
      "Epoch: 61 loss: 0.5264 Train_acc: 0.9667\n",
      "Epoch: 62 loss: 0.5260 Train_acc: 0.9667\n",
      "Epoch: 63 loss: 0.5256 Train_acc: 0.9667\n",
      "Epoch: 64 loss: 0.5252 Train_acc: 0.9667\n",
      "Epoch: 65 loss: 0.5249 Train_acc: 0.9667\n",
      "Epoch: 66 loss: 0.5246 Train_acc: 0.9667\n",
      "Epoch: 67 loss: 0.5244 Train_acc: 0.9667\n",
      "Epoch: 68 loss: 0.5242 Train_acc: 0.9667\n",
      "Epoch: 69 loss: 0.5240 Train_acc: 0.9667\n",
      "Epoch: 70 loss: 0.5238 Train_acc: 0.9667\n",
      "Test_acc: 0.7500\n",
      "Epoch: 71 loss: 0.5236 Train_acc: 0.9667\n",
      "Epoch: 72 loss: 0.5235 Train_acc: 0.9667\n",
      "Epoch: 73 loss: 0.5234 Train_acc: 0.9667\n",
      "Epoch: 74 loss: 0.5232 Train_acc: 0.9667\n",
      "Epoch: 75 loss: 0.5231 Train_acc: 0.9667\n",
      "Epoch: 76 loss: 0.5230 Train_acc: 0.9667\n",
      "Epoch: 77 loss: 0.5229 Train_acc: 0.9667\n",
      "Epoch: 78 loss: 0.5228 Train_acc: 0.9667\n",
      "Epoch: 79 loss: 0.5227 Train_acc: 0.9667\n",
      "Epoch: 80 loss: 0.5226 Train_acc: 0.9667\n",
      "Test_acc: 0.7500\n",
      "Epoch: 81 loss: 0.5225 Train_acc: 0.9667\n",
      "Epoch: 82 loss: 0.5225 Train_acc: 0.9667\n",
      "Epoch: 83 loss: 0.5224 Train_acc: 0.9667\n",
      "Epoch: 84 loss: 0.5223 Train_acc: 0.9667\n",
      "Epoch: 85 loss: 0.5222 Train_acc: 0.9667\n",
      "Epoch: 86 loss: 0.5222 Train_acc: 0.9667\n",
      "Epoch: 87 loss: 0.5221 Train_acc: 0.9667\n",
      "Epoch: 88 loss: 0.5221 Train_acc: 0.9667\n",
      "Epoch: 89 loss: 0.5220 Train_acc: 0.9667\n",
      "Epoch: 90 loss: 0.5220 Train_acc: 0.9667\n",
      "Test_acc: 0.7500\n",
      "Epoch: 91 loss: 0.5219 Train_acc: 0.9667\n",
      "Epoch: 92 loss: 0.5219 Train_acc: 0.9667\n",
      "Epoch: 93 loss: 0.5218 Train_acc: 0.9667\n",
      "Epoch: 94 loss: 0.5218 Train_acc: 0.9667\n",
      "Epoch: 95 loss: 0.5218 Train_acc: 0.9667\n",
      "Epoch: 96 loss: 0.5217 Train_acc: 0.9667\n",
      "Epoch: 97 loss: 0.5217 Train_acc: 0.9667\n",
      "Epoch: 98 loss: 0.5216 Train_acc: 0.9667\n",
      "Epoch: 99 loss: 0.5216 Train_acc: 0.9667\n",
      "Epoch: 100 loss: 0.5216 Train_acc: 0.9667\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(features, incidences, adjacencies)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(features, incidences, adjacencies)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            # _pred_test = torch.softmax(y_hat_test,dim=1).ge(0.5).float()\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
