{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BucHHYh0pEc"
   },
   "source": [
    "# Train a Simplicial Attention Network (SAN)\n",
    "\n",
    "We create and train a Simplicial Attention Neural Networks (SAN) originally proposed in [Giusti*, Battiloro* et. al : Simplicial Attention Neural Networks (2022)](https://arxiv.org/abs/2203.07485). The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.\n",
    "\n",
    "### Abstract\n",
    "The aim of this work is to introduce simplicial attention networks (SANs), i.e., novel neural architectures that operate on data defined on simplicial complexes leveraging masked self-attentional layers. Hinging on formal arguments from topological signal processing, we introduce a proper self-attention mechanism able to process data components at different layers (e.g., nodes, edges, triangles, and so on), while learning how to weight both upper and lower neighborhoods of the given topological domain in a totally task-oriented fashion. The proposed SANs generalize most of the current architectures available for processing data defined on simplicial complexes.\n",
    "\n",
    "<center><a href=\"https://ibb.co/jVggJzK\"><img src=\"https://i.ibb.co/PTwwDMp/SAN-architecture.jpg\" alt=\"SAN-architecture\" border=\"0\"></a></center>\n",
    "\n",
    "**Remark.** The notation we use is defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)and [Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)](https://arxiv.org/pdf/2206.00606.pdf). Custom symbols are introduced along the notebook, when necessary.\n",
    "\n",
    "### The Neural Network\n",
    "The SAN layer takes rank-$r$ signals as input  and gives rank-$r$ signals as output. The involved neighborhoods are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal N = \\{\\mathcal N_1, \\mathcal N_2,...,\\mathcal N_{2p+1}\\} =  \\{A_{\\uparrow, r}, A_{\\downarrow, r}, A_{\\uparrow, r}^2, A_{\\downarrow, r}^2,...,A_{\\uparrow, r}^p, A_{\\downarrow, r}^p, Q_r\\},\n",
    "\\end{equation}\n",
    "where $Q_r$ is a sparse projection operator (weighted matrix) over the kernel of the $r$-th Hodge Laplacian $L_r$, computed as in the original paper. $Q_r$ has the same topology of $L_r$.\n",
    "\n",
    "The equation of the SAN layer of this neural network is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{h}_x^{t+1} =  \\phi^l \\Bigg ( \\textbf{h}_x^{t}, \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}\\bigoplus_{y \\in \\mathcal{N}_k(x)}  \\widetilde{\\alpha}_k(h_x^t,hy^t)\\Bigg ),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\widetilde{\\alpha}_k$ being either an attention function $\\alpha_k$ if $\\mathcal{N}_k \\neq Q_r$ or a standard convolution term(affine transformation + weights) with weights given by the entries of $Q_r$ if $\\mathcal{N}_k = Q_r$.\n",
    "\n",
    "Therefore, the SAN layer is made by an attentional convolution from rank-$r$ cells to rank-$r$ cells using an adjacency message passing scheme up to $p$-hops neighborhoods:\n",
    "\n",
    "\\begin{align*}\n",
    "&ðŸŸ¥\\textrm{ Message.} &\\quad m_{(y \\rightarrow x),k} =&\n",
    "\\alpha_k(h_x^t,h_y^t) =\n",
    "a_k(h_x^{t}, h_y^{t}) \\cdot \\psi_k^t(h_x^{t})\\quad \\forall \\mathcal N_k \\in \\mathcal{N}\\\\\n",
    "\\\\\n",
    "&ðŸŸ§ \\textrm{ Within-Neighborhood Aggregation.} &\\quad m_{x,k}               =& \\bigoplus_{y \\in \\mathcal{N}_k(x)}  m_{(y \\rightarrow x),k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ© \\textrm{ Between-Neighborhood Aggregation.} &\\quad m_{x} =& \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}m_{x,k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ¦ \\textrm{ Update.}&\\quad h_x^{t+1}                =& \\phi^{t}(h_x^t, m_{x})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform a binary node classification task using KarateClub dataset. We use a [\"GAT-like\" attention function](https://arxiv.org/abs/1710.10903), in which two different sets of attention weights $a_\\uparrow$ and $a_\\downarrow$ are learned for the upper neighborhoods $A_{\\uparrow,1}^p$ and for the lower neighborhoods $A_{\\downarrow,1}^p$ ($p=1,...,P$), respectively,   i.e.:\n",
    "\n",
    "- If $\\mathcal{N}_k \\neq Q_r$  and suppose, as an example, $\\mathcal{N}_k = A_{\\downarrow,1}^g$, the $g$-hops lower neighborhood:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = (\\textrm{softmax}_j(\\textrm{LeakyReLU}(a_{\\downarrow}^T[\\underset{p=1}{\\overset{P}{||}}h_x^{t}W_{\\downarrow,p}|| \\underset{p=1}{\\overset{P}{||}}h_y^{t}W_{\\downarrow,p}]))^g\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W_{\\downarrow,g}.\n",
    "\\end{align}\n",
    "\n",
    "- If $\\mathcal{N}_k = Q_r$:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = Q_{x,y}\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W.\n",
    "\\end{align}\n",
    "\n",
    "$W$, $a_\\downarrow$, $a_\\uparrow$, \\{$W_{\\downarrow,p}\\}_{p=1}^P$ and $\\{W_{\\uparrow,p}\\}_{p=1}^P$ are learnable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZNrtWfL10pEe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import toponetx.datasets.graph as graph\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "from topomodelx.nn.simplicial.san import SAN\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z05cyYcw0pEh",
    "outputId": "0ba2482b-dc68-451b-95bd-d2ea97e04378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPJIacee0pEh"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BreEb4B00pEi",
    "outputId": "fed8fb52-16b7-418e-812c-7f29bf32de1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fdT4Zjsp0pEi",
    "outputId": "054e2a94-3503-421d-d10c-8a9ceb21cce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78, 45, 11, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALHwd2Q40pEi"
   },
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "We now retrieve the neighborhoods (i.e. their representative matrices) that we will use to send messages on the domain. In this case, we decide w.l.o.g. to work at the edge level (thus considering a simplicial complex of order 2). We therefore need the lower and upper laplacians of rank 1, $L_{\\downarrow,1}=B_1^TB_1$ and $L_{\\uparrow,1}=B_2B_2^T$, both with dimensions $n_\\text{edges} \\times n_\\text{edges}$, where $B_1$ and $B_2$ are the incidence matrices of rank 1 and 2. Please notice that the binary adjacencies $A_{\\downarrow,1}^p$ and  $A_{\\uparrow,1}^p$ encoding the $p$-hops neighborhoods are given by the support (the non-zeros pattern) of $L_{\\downarrow,1}^p$ and $L_{\\uparrow,1}^p$, respectively. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5-BgSXz0pEj"
   },
   "source": [
    "**Remark.** In the case of rank-0 simplices (nodes), there is no lower Laplacian; in this case, we just initialize the down laplacian as a 0-matrix, and SAN automatically becomes a GAT-like architecture.\n",
    "In the case of simplices of maxium rank (the order of the complex), there is no upper Laplacian. In this case we can also initialize it as a 0 matrix and SAN will only consider the lower adjacencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nukmcsOJ0pEj"
   },
   "outputs": [],
   "source": [
    "simplex_order_k = 1\n",
    "# Down laplacian\n",
    "try:\n",
    "    laplacian_down = from_sparse(dataset.down_laplacian_matrix(rank=simplex_order_k))\n",
    "except ValueError:\n",
    "    laplacian_down = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()\n",
    "# Up laplacian\n",
    "try:\n",
    "    laplacian_up = from_sparse(dataset.up_laplacian_matrix(rank=simplex_order_k))\n",
    "except ValueError:\n",
    "    laplacian_up = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHCGA1gY0pEj"
   },
   "source": [
    "## Import signal ##\n",
    "\n",
    "We define edge features to be the gradient of the nodes features, i.e. given the node feature matrix $X_0$, we compute the edge features matrix as $X_1 = B_1^TX_0$. We will finally obtain the estimated node labels from the updated edge features by multiplying them again with $B_1$, i.e. the final nodes features are computed as the divergence of the final edge features.\n",
    "\n",
    "**Remark.** Please notice that also this way of deriving edges/nodes features from nodes/edges features could be seen as a (non-learnable) message passing between rank-0/1 cells (nodes/edges) and rank-1/0 cells (nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EBf26K5N0pEj",
    "outputId": "22acb245-384f-4d62-9e59-b55bdea3353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = np.stack(x_2)\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omDMQ60D0pEk"
   },
   "source": [
    "We use the incidence matrix between nodes-edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MmP56FAH0pEk"
   },
   "outputs": [],
   "source": [
    "incidence_0_1 = from_sparse(dataset.incidence_matrix(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTdYu3WX0pEk"
   },
   "source": [
    "The final edge features are obtained summing the original features of those edges plus the projection of the node features onto edges (using the incidence matrix accordingly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K2zU2_jL0pEk"
   },
   "outputs": [],
   "source": [
    "x = x_1 + torch.sparse.mm(incidence_0_1.T, x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWQ_NRiU0pEl"
   },
   "source": [
    "Hence, the final input features are defined by this sum, and we also pre-define the number of hidden and output channels of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cS7o2U620pEl"
   },
   "outputs": [],
   "source": [
    "in_channels = x.shape[-1]\n",
    "hidden_channels = 16\n",
    "out_channels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDnM5N2G0pEl"
   },
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert one-hot encode the binary labels, and keep the first four nodes for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Ub-GWaMm0pEl"
   },
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true  # [:4]\n",
    "y_train = y_true[-30:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTHtR74f0pEl"
   },
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the SAN class, we create our neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2n-lFfkJ0pEm",
    "outputId": "3c633dee-f904-49ba-c169-f21499488cb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([78, 78]), torch.Size([78, 78]), torch.Size([78, 2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_up.shape, laplacian_down.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UEoWB2G0pEm"
   },
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNOcultR0pEm"
   },
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "sWopn2U60pEn",
    "outputId": "a2153bef-91eb-4f99-8aad-3166b4966c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7256 Train_acc: 0.3333\n",
      "Epoch: 2 loss: 0.7225 Train_acc: 0.8000\n",
      "Epoch: 3 loss: 0.7201 Train_acc: 0.7667\n",
      "Epoch: 4 loss: 0.7172 Train_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 loss: 0.7133 Train_acc: 0.7333\n",
      "Epoch: 6 loss: 0.7087 Train_acc: 0.7000\n",
      "Epoch: 7 loss: 0.7039 Train_acc: 0.7000\n",
      "Epoch: 8 loss: 0.6990 Train_acc: 0.7000\n",
      "Epoch: 9 loss: 0.6945 Train_acc: 0.7000\n",
      "Epoch: 10 loss: 0.6904 Train_acc: 0.7000\n",
      "Test_acc: 0.6471\n",
      "Epoch: 11 loss: 0.6867 Train_acc: 0.7000\n",
      "Epoch: 12 loss: 0.6835 Train_acc: 0.7000\n",
      "Epoch: 13 loss: 0.6807 Train_acc: 0.7000\n",
      "Epoch: 14 loss: 0.6784 Train_acc: 0.7000\n",
      "Epoch: 15 loss: 0.6764 Train_acc: 0.7000\n",
      "Epoch: 16 loss: 0.6748 Train_acc: 0.7000\n",
      "Epoch: 17 loss: 0.6734 Train_acc: 0.7000\n",
      "Epoch: 18 loss: 0.6722 Train_acc: 0.7000\n",
      "Epoch: 19 loss: 0.6713 Train_acc: 0.7000\n",
      "Epoch: 20 loss: 0.6705 Train_acc: 0.7000\n",
      "Test_acc: 0.6471\n",
      "Epoch: 21 loss: 0.6698 Train_acc: 0.7000\n",
      "Epoch: 22 loss: 0.6692 Train_acc: 0.7000\n",
      "Epoch: 23 loss: 0.6687 Train_acc: 0.7000\n",
      "Epoch: 24 loss: 0.6683 Train_acc: 0.7000\n",
      "Epoch: 25 loss: 0.6679 Train_acc: 0.7000\n",
      "Epoch: 26 loss: 0.6676 Train_acc: 0.7000\n",
      "Epoch: 27 loss: 0.6673 Train_acc: 0.7000\n",
      "Epoch: 28 loss: 0.6671 Train_acc: 0.7000\n",
      "Epoch: 29 loss: 0.6669 Train_acc: 0.7000\n",
      "Epoch: 30 loss: 0.6667 Train_acc: 0.7000\n",
      "Test_acc: 0.6471\n",
      "Epoch: 31 loss: 0.6665 Train_acc: 0.7000\n",
      "Epoch: 32 loss: 0.6664 Train_acc: 0.7000\n",
      "Epoch: 33 loss: 0.6662 Train_acc: 0.7000\n",
      "Epoch: 34 loss: 0.6661 Train_acc: 0.7000\n",
      "Epoch: 35 loss: 0.6660 Train_acc: 0.7000\n",
      "Epoch: 36 loss: 0.6659 Train_acc: 0.7000\n",
      "Epoch: 37 loss: 0.6658 Train_acc: 0.7000\n",
      "Epoch: 38 loss: 0.6657 Train_acc: 0.7000\n",
      "Epoch: 39 loss: 0.6656 Train_acc: 0.7000\n",
      "Epoch: 40 loss: 0.6655 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 41 loss: 0.6655 Train_acc: 0.7000\n",
      "Epoch: 42 loss: 0.6654 Train_acc: 0.7000\n",
      "Epoch: 43 loss: 0.6653 Train_acc: 0.7000\n",
      "Epoch: 44 loss: 0.6652 Train_acc: 0.7000\n",
      "Epoch: 45 loss: 0.6652 Train_acc: 0.7000\n",
      "Epoch: 46 loss: 0.6651 Train_acc: 0.7000\n",
      "Epoch: 47 loss: 0.6650 Train_acc: 0.7000\n",
      "Epoch: 48 loss: 0.6650 Train_acc: 0.7000\n",
      "Epoch: 49 loss: 0.6649 Train_acc: 0.7000\n",
      "Epoch: 50 loss: 0.6648 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 51 loss: 0.6648 Train_acc: 0.7000\n",
      "Epoch: 52 loss: 0.6647 Train_acc: 0.7000\n",
      "Epoch: 53 loss: 0.6646 Train_acc: 0.7000\n",
      "Epoch: 54 loss: 0.6645 Train_acc: 0.7000\n",
      "Epoch: 55 loss: 0.6645 Train_acc: 0.7000\n",
      "Epoch: 56 loss: 0.6644 Train_acc: 0.7000\n",
      "Epoch: 57 loss: 0.6643 Train_acc: 0.7000\n",
      "Epoch: 58 loss: 0.6643 Train_acc: 0.7000\n",
      "Epoch: 59 loss: 0.6642 Train_acc: 0.7000\n",
      "Epoch: 60 loss: 0.6641 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 61 loss: 0.6640 Train_acc: 0.7000\n",
      "Epoch: 62 loss: 0.6639 Train_acc: 0.7000\n",
      "Epoch: 63 loss: 0.6639 Train_acc: 0.7000\n",
      "Epoch: 64 loss: 0.6638 Train_acc: 0.7000\n",
      "Epoch: 65 loss: 0.6637 Train_acc: 0.7000\n",
      "Epoch: 66 loss: 0.6636 Train_acc: 0.7000\n",
      "Epoch: 67 loss: 0.6635 Train_acc: 0.7000\n",
      "Epoch: 68 loss: 0.6634 Train_acc: 0.7000\n",
      "Epoch: 69 loss: 0.6633 Train_acc: 0.7000\n",
      "Epoch: 70 loss: 0.6632 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 71 loss: 0.6631 Train_acc: 0.7000\n",
      "Epoch: 72 loss: 0.6630 Train_acc: 0.7000\n",
      "Epoch: 73 loss: 0.6629 Train_acc: 0.7000\n",
      "Epoch: 74 loss: 0.6628 Train_acc: 0.7000\n",
      "Epoch: 75 loss: 0.6627 Train_acc: 0.7000\n",
      "Epoch: 76 loss: 0.6626 Train_acc: 0.7000\n",
      "Epoch: 77 loss: 0.6625 Train_acc: 0.7000\n",
      "Epoch: 78 loss: 0.6624 Train_acc: 0.7000\n",
      "Epoch: 79 loss: 0.6622 Train_acc: 0.7000\n",
      "Epoch: 80 loss: 0.6621 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 81 loss: 0.6620 Train_acc: 0.7000\n",
      "Epoch: 82 loss: 0.6619 Train_acc: 0.7000\n",
      "Epoch: 83 loss: 0.6617 Train_acc: 0.7000\n",
      "Epoch: 84 loss: 0.6616 Train_acc: 0.7000\n",
      "Epoch: 85 loss: 0.6615 Train_acc: 0.7000\n",
      "Epoch: 86 loss: 0.6613 Train_acc: 0.7000\n",
      "Epoch: 87 loss: 0.6612 Train_acc: 0.7000\n",
      "Epoch: 88 loss: 0.6610 Train_acc: 0.7000\n",
      "Epoch: 89 loss: 0.6609 Train_acc: 0.7000\n",
      "Epoch: 90 loss: 0.6607 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 91 loss: 0.6606 Train_acc: 0.7000\n",
      "Epoch: 92 loss: 0.6604 Train_acc: 0.7000\n",
      "Epoch: 93 loss: 0.6603 Train_acc: 0.7000\n",
      "Epoch: 94 loss: 0.6601 Train_acc: 0.7000\n",
      "Epoch: 95 loss: 0.6600 Train_acc: 0.7000\n",
      "Epoch: 96 loss: 0.6598 Train_acc: 0.7000\n",
      "Epoch: 97 loss: 0.6596 Train_acc: 0.7000\n",
      "Epoch: 98 loss: 0.6595 Train_acc: 0.7000\n",
      "Epoch: 99 loss: 0.6593 Train_acc: 0.7000\n",
      "Epoch: 100 loss: 0.6592 Train_acc: 0.7000\n",
      "Test_acc: 0.6176\n",
      "Epoch: 101 loss: 0.6590 Train_acc: 0.7000\n",
      "Epoch: 102 loss: 0.6588 Train_acc: 0.7000\n",
      "Epoch: 103 loss: 0.6587 Train_acc: 0.7000\n",
      "Epoch: 104 loss: 0.6585 Train_acc: 0.7000\n",
      "Epoch: 105 loss: 0.6584 Train_acc: 0.7333\n",
      "Epoch: 106 loss: 0.6582 Train_acc: 0.7333\n",
      "Epoch: 107 loss: 0.6581 Train_acc: 0.7333\n",
      "Epoch: 108 loss: 0.6579 Train_acc: 0.7333\n",
      "Epoch: 109 loss: 0.6578 Train_acc: 0.7333\n",
      "Epoch: 110 loss: 0.6576 Train_acc: 0.7333\n",
      "Test_acc: 0.6471\n",
      "Epoch: 111 loss: 0.6575 Train_acc: 0.7333\n",
      "Epoch: 112 loss: 0.6573 Train_acc: 0.7333\n",
      "Epoch: 113 loss: 0.6572 Train_acc: 0.7333\n",
      "Epoch: 114 loss: 0.6571 Train_acc: 0.7333\n",
      "Epoch: 115 loss: 0.6569 Train_acc: 0.7333\n",
      "Epoch: 116 loss: 0.6568 Train_acc: 0.7333\n",
      "Epoch: 117 loss: 0.6567 Train_acc: 0.7333\n",
      "Epoch: 118 loss: 0.6566 Train_acc: 0.7333\n",
      "Epoch: 119 loss: 0.6565 Train_acc: 0.7667\n",
      "Epoch: 120 loss: 0.6564 Train_acc: 0.7667\n",
      "Test_acc: 0.6765\n",
      "Epoch: 121 loss: 0.6562 Train_acc: 0.7667\n",
      "Epoch: 122 loss: 0.6561 Train_acc: 0.7667\n",
      "Epoch: 123 loss: 0.6560 Train_acc: 0.7667\n",
      "Epoch: 124 loss: 0.6559 Train_acc: 0.7667\n",
      "Epoch: 125 loss: 0.6558 Train_acc: 0.7667\n",
      "Epoch: 126 loss: 0.6557 Train_acc: 0.7667\n",
      "Epoch: 127 loss: 0.6556 Train_acc: 0.7667\n",
      "Epoch: 128 loss: 0.6556 Train_acc: 0.7667\n",
      "Epoch: 129 loss: 0.6555 Train_acc: 0.7667\n",
      "Epoch: 130 loss: 0.6554 Train_acc: 0.7667\n",
      "Test_acc: 0.6765\n",
      "Epoch: 131 loss: 0.6553 Train_acc: 0.7667\n",
      "Epoch: 132 loss: 0.6552 Train_acc: 0.7667\n",
      "Epoch: 133 loss: 0.6551 Train_acc: 0.7667\n",
      "Epoch: 134 loss: 0.6550 Train_acc: 0.7667\n",
      "Epoch: 135 loss: 0.6550 Train_acc: 0.7667\n",
      "Epoch: 136 loss: 0.6549 Train_acc: 0.7667\n",
      "Epoch: 137 loss: 0.6548 Train_acc: 0.7667\n",
      "Epoch: 138 loss: 0.6547 Train_acc: 0.7667\n",
      "Epoch: 139 loss: 0.6547 Train_acc: 0.7667\n",
      "Epoch: 140 loss: 0.6546 Train_acc: 0.7667\n",
      "Test_acc: 0.6765\n",
      "Epoch: 141 loss: 0.6545 Train_acc: 0.7667\n",
      "Epoch: 142 loss: 0.6545 Train_acc: 0.7667\n",
      "Epoch: 143 loss: 0.6544 Train_acc: 0.7667\n",
      "Epoch: 144 loss: 0.6543 Train_acc: 0.7667\n",
      "Epoch: 145 loss: 0.6543 Train_acc: 0.7667\n",
      "Epoch: 146 loss: 0.6542 Train_acc: 0.7667\n",
      "Epoch: 147 loss: 0.6542 Train_acc: 0.7667\n",
      "Epoch: 148 loss: 0.6541 Train_acc: 0.7667\n",
      "Epoch: 149 loss: 0.6540 Train_acc: 0.7667\n",
      "Epoch: 150 loss: 0.6540 Train_acc: 0.7667\n",
      "Test_acc: 0.7059\n",
      "Epoch: 151 loss: 0.6539 Train_acc: 0.8000\n",
      "Epoch: 152 loss: 0.6539 Train_acc: 0.8000\n",
      "Epoch: 153 loss: 0.6538 Train_acc: 0.8000\n",
      "Epoch: 154 loss: 0.6538 Train_acc: 0.8000\n",
      "Epoch: 155 loss: 0.6537 Train_acc: 0.8000\n",
      "Epoch: 156 loss: 0.6537 Train_acc: 0.8000\n",
      "Epoch: 157 loss: 0.6536 Train_acc: 0.8000\n",
      "Epoch: 158 loss: 0.6536 Train_acc: 0.8000\n",
      "Epoch: 159 loss: 0.6535 Train_acc: 0.8000\n",
      "Epoch: 160 loss: 0.6535 Train_acc: 0.8000\n",
      "Test_acc: 0.7059\n",
      "Epoch: 161 loss: 0.6535 Train_acc: 0.8000\n",
      "Epoch: 162 loss: 0.6534 Train_acc: 0.8000\n",
      "Epoch: 163 loss: 0.6534 Train_acc: 0.8000\n",
      "Epoch: 164 loss: 0.6533 Train_acc: 0.8000\n",
      "Epoch: 165 loss: 0.6533 Train_acc: 0.8000\n",
      "Epoch: 166 loss: 0.6533 Train_acc: 0.8000\n",
      "Epoch: 167 loss: 0.6532 Train_acc: 0.8000\n",
      "Epoch: 168 loss: 0.6532 Train_acc: 0.8000\n",
      "Epoch: 169 loss: 0.6531 Train_acc: 0.8000\n",
      "Epoch: 170 loss: 0.6531 Train_acc: 0.8000\n",
      "Test_acc: 0.7059\n",
      "Epoch: 171 loss: 0.6531 Train_acc: 0.8000\n",
      "Epoch: 172 loss: 0.6530 Train_acc: 0.8000\n",
      "Epoch: 173 loss: 0.6530 Train_acc: 0.8000\n",
      "Epoch: 174 loss: 0.6530 Train_acc: 0.8000\n",
      "Epoch: 175 loss: 0.6529 Train_acc: 0.8000\n",
      "Epoch: 176 loss: 0.6529 Train_acc: 0.8000\n",
      "Epoch: 177 loss: 0.6529 Train_acc: 0.8000\n",
      "Epoch: 178 loss: 0.6529 Train_acc: 0.8000\n",
      "Epoch: 179 loss: 0.6528 Train_acc: 0.8000\n",
      "Epoch: 180 loss: 0.6528 Train_acc: 0.8000\n",
      "Test_acc: 0.7059\n",
      "Epoch: 181 loss: 0.6528 Train_acc: 0.8000\n",
      "Epoch: 182 loss: 0.6527 Train_acc: 0.8000\n",
      "Epoch: 183 loss: 0.6527 Train_acc: 0.8000\n",
      "Epoch: 184 loss: 0.6527 Train_acc: 0.8000\n",
      "Epoch: 185 loss: 0.6527 Train_acc: 0.8000\n",
      "Epoch: 186 loss: 0.6526 Train_acc: 0.8000\n",
      "Epoch: 187 loss: 0.6526 Train_acc: 0.8000\n",
      "Epoch: 188 loss: 0.6526 Train_acc: 0.8000\n",
      "Epoch: 189 loss: 0.6526 Train_acc: 0.8000\n",
      "Epoch: 190 loss: 0.6525 Train_acc: 0.8000\n",
      "Test_acc: 0.7059\n",
      "Epoch: 191 loss: 0.6525 Train_acc: 0.8000\n",
      "Epoch: 192 loss: 0.6525 Train_acc: 0.8000\n",
      "Epoch: 193 loss: 0.6525 Train_acc: 0.8000\n",
      "Epoch: 194 loss: 0.6524 Train_acc: 0.8000\n",
      "Epoch: 195 loss: 0.6524 Train_acc: 0.8000\n",
      "Epoch: 196 loss: 0.6524 Train_acc: 0.8000\n",
      "Epoch: 197 loss: 0.6524 Train_acc: 0.8000\n",
      "Epoch: 198 loss: 0.6523 Train_acc: 0.8000\n",
      "Epoch: 199 loss: 0.6523 Train_acc: 0.8000\n",
      "Epoch: 200 loss: 0.6523 Train_acc: 0.8000\n",
      "Test_acc: 0.7059\n"
     ]
    }
   ],
   "source": [
    "model = SAN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=1,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "test_interval = 10\n",
    "num_epochs = 200\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat_edge = model(x, laplacian_up=laplacian_up, laplacian_down=laplacian_down)\n",
    "    # We project the edge-level output of the model to the node-level\n",
    "    # and apply softmax fn to get the final node-level classification output\n",
    "    y_hat = torch.softmax(torch.sparse.mm(incidence_0_1, y_hat_edge), dim=1)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[-len(y_train) :].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_edge_test = model(\n",
    "                x, laplacian_up=laplacian_up, laplacian_down=laplacian_down\n",
    "            )\n",
    "            # Projection to node-level\n",
    "            y_hat_test = torch.softmax(\n",
    "                torch.sparse.mm(incidence_0_1, y_hat_edge_test), dim=1\n",
    "            )\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            # _pred_test = torch.softmax(y_hat_test,dim=1).ge(0.5).float()\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[: len(y_test)], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('topox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b696409e3d9b84bb97012e0a2d03075417bfa260eb8ad887be094cb925d5d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
