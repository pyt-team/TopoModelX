{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BucHHYh0pEc"
   },
   "source": [
    "# Train a Simplicial Attention Network (SAN)\n",
    "\n",
    "We create and train a Simplicial Attention Neural Networks (SAN) originally proposed in [Giusti*, Battiloro* et. al : Simplicial Attention Neural Networks (2022)](https://arxiv.org/abs/2203.07485). The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.\n",
    "\n",
    "### Abstract\n",
    "The aim of this work is to introduce simplicial attention networks (SANs), i.e., novel neural architectures that operate on data defined on simplicial complexes leveraging masked self-attentional layers. Hinging on formal arguments from topological signal processing, we introduce a proper self-attention mechanism able to process data components at different layers (e.g., nodes, edges, triangles, and so on), while learning how to weight both upper and lower neighborhoods of the given topological domain in a totally task-oriented fashion. The proposed SANs generalize most of the current architectures available for processing data defined on simplicial complexes.\n",
    "\n",
    "<center><a href=\"https://ibb.co/jVggJzK\"><img src=\"https://i.ibb.co/PTwwDMp/SAN-architecture.jpg\" alt=\"SAN-architecture\" border=\"0\"></a></center>\n",
    "\n",
    "**Remark.** The notation we use is defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)and [Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)](https://arxiv.org/pdf/2206.00606.pdf). Custom symbols are introduced along the notebook, when necessary.\n",
    "\n",
    "### The Neural Network\n",
    "The SAN layer takes rank-$r$ signals as input  and gives rank-$r$ signals as output. The involved neighborhoods are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal N = \\{\\mathcal N_1, \\mathcal N_2,...,\\mathcal N_{2p+1}\\} =  \\{A_{\\uparrow, r}, A_{\\downarrow, r}, A_{\\uparrow, r}^2, A_{\\downarrow, r}^2,...,A_{\\uparrow, r}^p, A_{\\downarrow, r}^p, Q_r\\},\n",
    "\\end{equation}\n",
    "where $Q_r$ is a sparse projection operator (weighted matrix) over the kernel of the $r$-th Hodge Laplacian $L_r$, computed as in the original paper. $Q_r$ has the same topology of $L_r$.\n",
    "\n",
    "The equation of the SAN layer of this neural network is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{h}_x^{t+1} =  \\phi^l \\Bigg ( \\textbf{h}_x^{t}, \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}\\bigoplus_{y \\in \\mathcal{N}_k(x)}  \\widetilde{\\alpha}_k(h_x^t,hy^t)\\Bigg ),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\widetilde{\\alpha}_k$ being either an attention function $\\alpha_k$ if $\\mathcal{N}_k \\neq Q_r$ or a standard convolution term(affine transformation + weights) with weights given by the entries of $Q_r$ if $\\mathcal{N}_k = Q_r$.\n",
    "\n",
    "Therefore, the SAN layer is made by an attentional convolution from rank-$r$ cells to rank-$r$ cells using an adjacency message passing scheme up to $p$-hops neighborhoods:\n",
    "\n",
    "\\begin{align*}\n",
    "&ðŸŸ¥\\textrm{ Message.} &\\quad m_{(y \\rightarrow x),k} =&\n",
    "\\alpha_k(h_x^t,h_y^t) =\n",
    "a_k(h_x^{t}, h_y^{t}) \\cdot \\psi_k^t(h_x^{t})\\quad \\forall \\mathcal N_k \\in \\mathcal{N}\\\\\n",
    "\\\\\n",
    "&ðŸŸ§ \\textrm{ Within-Neighborhood Aggregation.} &\\quad m_{x,k}               =& \\bigoplus_{y \\in \\mathcal{N}_k(x)}  m_{(y \\rightarrow x),k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ© \\textrm{ Between-Neighborhood Aggregation.} &\\quad m_{x} =& \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}m_{x,k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ¦ \\textrm{ Update.}&\\quad h_x^{t+1}                =& \\phi^{t}(h_x^t, m_{x})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform a binary node classification task using KarateClub dataset. We use a [\"GAT-like\" attention function](https://arxiv.org/abs/1710.10903), in which two different sets of attention weights $a_\\uparrow$ and $a_\\downarrow$ are learned for the upper neighborhoods $A_{\\uparrow,1}^p$ and for the lower neighborhoods $A_{\\downarrow,1}^p$ ($p=1,...,P$), respectively,   i.e.:\n",
    "\n",
    "- If $\\mathcal{N}_k \\neq Q_r$  and suppose, as an example, $\\mathcal{N}_k = A_{\\downarrow,1}^g$, the $g$-hops lower neighborhood:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = (\\textrm{softmax}_j(\\textrm{LeakyReLU}(a_{\\downarrow}^T[\\underset{p=1}{\\overset{P}{||}}h_x^{t}W_{\\downarrow,p}|| \\underset{p=1}{\\overset{P}{||}}h_y^{t}W_{\\downarrow,p}]))^g\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W_{\\downarrow,g}.\n",
    "\\end{align}\n",
    "\n",
    "- If $\\mathcal{N}_k = Q_r$:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = Q_{x,y}\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W.\n",
    "\\end{align}\n",
    "\n",
    "$W$, $a_\\downarrow$, $a_\\uparrow$, \\{$W_{\\downarrow,p}\\}_{p=1}^P$ and $\\{W_{\\uparrow,p}\\}_{p=1}^P$ are learnable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNrtWfL10pEe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import toponetx.datasets.graph as graph\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "from topomodelx.nn.simplicial.san_layer import SANLayer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z05cyYcw0pEh",
    "outputId": "0ba2482b-dc68-451b-95bd-d2ea97e04378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPJIacee0pEh"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BreEb4B00pEi",
    "outputId": "fed8fb52-16b7-418e-812c-7f29bf32de1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape [34, 78, 45, 11, 2] and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdT4Zjsp0pEi",
    "outputId": "054e2a94-3503-421d-d10c-8a9ceb21cce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 78, 45, 11, 2]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALHwd2Q40pEi"
   },
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "We now retrieve the neighborhoods (i.e. their representative matrices) that we will use to send messages on the domain. In this case, we decide w.l.o.g. to work at the edge level (thus considering a simplicial complex of order 2). We therefore need the lower and upper laplacians of rank 1, $L_{\\downarrow,1}=B_1^TB_1$ and $L_{\\uparrow,1}=B_2B_2^T$, both with dimensions $n_\\text{edges} \\times n_\\text{edges}$, where $B_1$ and $B_2$ are the incidence matrices of rank 1 and 2. Please notice that the binary adjacencies $A_{\\downarrow,1}^p$ and  $A_{\\uparrow,1}^p$ encoding the $p$-hops neighborhoods are given by the support (the non-zeros pattern) of $L_{\\downarrow,1}^p$ and $L_{\\uparrow,1}^p$, respectively. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5-BgSXz0pEj"
   },
   "source": [
    "**Remark.** In the case of rank-0 simplices (nodes), there is no lower Laplacian; in this case, we just initialize the down laplacian as a 0-matrix, and SAN automatically becomes a GAT-like architecture.\n",
    "In the case of simplices of maxium rank (the order of the complex), there is no upper Laplacian. In this case we can also initialize it as a 0 matrix and SAN will only consider the lower adjacencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nukmcsOJ0pEj"
   },
   "outputs": [],
   "source": [
    "simplex_order_k = 1\n",
    "# Down laplacian\n",
    "try:\n",
    "    Ldown = torch.from_numpy(\n",
    "        dataset.down_laplacian_matrix(rank=simplex_order_k).todense()\n",
    "    ).to_sparse()\n",
    "except ValueError:\n",
    "    Ldown = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()\n",
    "# Up laplacian\n",
    "try:\n",
    "    Lup = torch.from_numpy(\n",
    "        dataset.up_laplacian_matrix(rank=simplex_order_k).todense()\n",
    "    ).to_sparse()\n",
    "except ValueError:\n",
    "    Lup = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHCGA1gY0pEj"
   },
   "source": [
    "## Import signal ##\n",
    "\n",
    "We define edge features to be the gradient of the nodes features, i.e. given the node feature matrix $X_0$, we compute the edge features matrix as $X_1 = B_1^TX_0$. We will finally obtain the estimated node labels from the updated edge features by multiplying them again with $B_1$, i.e. the final nodes features are computed as the divergence of the final edge features.\n",
    "\n",
    "**Remark.** Please notice that also this way of deriving edges/nodes features from nodes/edges features could be seen as a (non-learnable) message passing between rank-0/1 cells (nodes/edges) and rank-1/0 cells (nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBf26K5N0pEj",
    "outputId": "22acb245-384f-4d62-9e59-b55bdea3353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = np.stack(x_2)\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omDMQ60D0pEk"
   },
   "source": [
    "We use the incidence matrix between nodes-edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmP56FAH0pEk"
   },
   "outputs": [],
   "source": [
    "incidence_0_1 = torch.from_numpy(dataset.incidence_matrix(1).todense()).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTdYu3WX0pEk"
   },
   "source": [
    "The final edge features are obtained summing the original features of those edges plus the projection of the node features onto edges (using the incidence matrix accordingly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2zU2_jL0pEk"
   },
   "outputs": [],
   "source": [
    "x = x_1 + torch.sparse.mm(incidence_0_1.T, x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWQ_NRiU0pEl"
   },
   "source": [
    "Hence, the final input features are defined by this sum, and we also pre-define the number of hidden and output channels of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS7o2U620pEl"
   },
   "outputs": [],
   "source": [
    "in_channels = x.shape[-1]\n",
    "hidden_channels = 16\n",
    "out_channels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDnM5N2G0pEl"
   },
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert one-hot encode the binary labels, and keep the first four nodes for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ub-GWaMm0pEl"
   },
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[:4]\n",
    "y_train = y_true[-30:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTHtR74f0pEl"
   },
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the SANLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ8KUFWq0pEl"
   },
   "outputs": [],
   "source": [
    "class SAN(torch.nn.Module):\n",
    "    r\"\"\"Simplicial Attention Network (SAN) implementation for binary edge classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Dimension of input features.\n",
    "    hidden_channels : int\n",
    "        Dimension of hidden features.\n",
    "    out_channels : int\n",
    "        Dimension of output features.\n",
    "    simplex_order_k : int\n",
    "        Order r of the considered simplices. Default to 1 (edges).\n",
    "    num_filters_J : int, optional\n",
    "        Approximation order for simplicial filters. Defaults to 2.\n",
    "    J_har : int, optional\n",
    "        Approximation order for harmonic convolution. Defaults to 5.\n",
    "    epsilon_har : float, optional\n",
    "        Epsilon value for harmonic convolution. Defaults to 1e-1.\n",
    "    n_layers : int, optional\n",
    "        Number of message passing layers. Defaults to 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_filters_J=2,\n",
    "        J_har=5,\n",
    "        epsilon_har=1e-1,\n",
    "        n_layers=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_filters_J = num_filters_J\n",
    "        self.J_har = J_har\n",
    "        self.epsilon_har = epsilon_har\n",
    "        if n_layers == 1:\n",
    "            self.layers = [\n",
    "                SANLayer(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            self.layers = [\n",
    "                SANLayer(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.hidden_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            ]\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.layers.append(\n",
    "                    SANLayer(\n",
    "                        in_channels=self.hidden_channels,\n",
    "                        out_channels=self.hidden_channels,\n",
    "                        num_filters_J=self.num_filters_J,\n",
    "                    )\n",
    "                )\n",
    "            self.layers.append(\n",
    "                SANLayer(\n",
    "                    in_channels=self.hidden_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            )\n",
    "        self.linear = torch.nn.Linear(out_channels, 2)\n",
    "\n",
    "    def compute_projection_matrix(self, L):\n",
    "        r\"\"\"Computation of the projection matrix which is then used\n",
    "        to calculate the harmonic component in SAN layers.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "\n",
    "        L : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Hodge laplacian of rank 1.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Projection matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        P = torch.eye(L.shape[0]) - self.epsilon_har * L\n",
    "        P = torch.linalg.matrix_power(P, self.J_har)\n",
    "        return P\n",
    "\n",
    "    def forward(self, x, Lup, Ldown):\n",
    "        r\"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x : tensor\n",
    "            shape = [n_nodes, channels_in]\n",
    "            Node features.\n",
    "\n",
    "        Lup : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Upper laplacian of rank 1.\n",
    "\n",
    "        Ld : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Down laplacian of rank 1.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_nodes, 2]\n",
    "            One-hot labels assigned to edges.\n",
    "\n",
    "        \"\"\"\n",
    "        # Compute the projection matrix for the harmonic component\n",
    "        L = Lup + Ldown\n",
    "        P = self.compute_projection_matrix(L)\n",
    "\n",
    "        # Forward computation\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, Lup, Ldown, P)\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2n-lFfkJ0pEm",
    "outputId": "3c633dee-f904-49ba-c169-f21499488cb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([78, 78]), torch.Size([78, 78]), torch.Size([78, 2]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lup.shape, Ldown.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UEoWB2G0pEm"
   },
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNOcultR0pEm"
   },
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWopn2U60pEn",
    "outputId": "a2153bef-91eb-4f99-8aad-3166b4966c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7240 Train_acc: 0.4333\n",
      "Epoch: 2 loss: 0.7114 Train_acc: 0.7333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 3 loss: 0.6948 Train_acc: 0.7333\n",
      "Epoch: 4 loss: 0.6829 Train_acc: 0.7333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 5 loss: 0.6760 Train_acc: 0.7333\n",
      "Epoch: 6 loss: 0.6721 Train_acc: 0.7333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 7 loss: 0.6698 Train_acc: 0.7333\n",
      "Epoch: 8 loss: 0.6685 Train_acc: 0.7333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 9 loss: 0.6677 Train_acc: 0.7333\n",
      "Epoch: 10 loss: 0.6672 Train_acc: 0.7333\n",
      "Test_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "model = SAN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=1,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.4)\n",
    "test_interval = 2\n",
    "num_epochs = 10\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat_edge = model(x, Lup=Lup, Ldown=Ldown)\n",
    "    # We project the edge-level output of the model to the node-level\n",
    "    # and apply softmax fn to get the final node-level classification output\n",
    "    y_hat = torch.softmax(torch.sparse.mm(incidence_0_1, y_hat_edge), dim=1)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[-len(y_train) :].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_edge_test = model(x, Lup=Lup, Ldown=Ldown)\n",
    "            # Projection to node-level\n",
    "            y_hat_test = torch.softmax(\n",
    "                torch.sparse.mm(incidence_0_1, y_hat_edge_test), dim=1\n",
    "            )\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            # _pred_test = torch.softmax(y_hat_test,dim=1).ge(0.5).float()\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[: len(y_test)], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b696409e3d9b84bb97012e0a2d03075417bfa260eb8ad887be094cb925d5d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
