{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BucHHYh0pEc"
   },
   "source": [
    "# Train a Simplicial Attention Network (SAN)\n",
    "\n",
    "We create and train a Simplicial Attention Neural Networks (SAN) originally proposed in [Giusti*, Battiloro* et. al : Simplicial Attention Neural Networks (2022)](https://arxiv.org/abs/2203.07485). The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.\n",
    "\n",
    "### Abstract\n",
    "The aim of this work is to introduce simplicial attention networks (SANs), i.e., novel neural architectures that operate on data defined on simplicial complexes leveraging masked self-attentional layers. Hinging on formal arguments from topological signal processing, we introduce a proper self-attention mechanism able to process data components at different layers (e.g., nodes, edges, triangles, and so on), while learning how to weight both upper and lower neighborhoods of the given topological domain in a totally task-oriented fashion. The proposed SANs generalize most of the current architectures available for processing data defined on simplicial complexes.\n",
    "\n",
    "<center><a href=\"https://ibb.co/jVggJzK\"><img src=\"https://i.ibb.co/PTwwDMp/SAN-architecture.jpg\" alt=\"SAN-architecture\" border=\"0\"></a></center>\n",
    "\n",
    "**Remark.** The notation we use is defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)and [Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)](https://arxiv.org/pdf/2206.00606.pdf). Custom symbols are introduced along the notebook, when necessary.\n",
    "\n",
    "### The Neural Network\n",
    "The SAN layer takes rank-$r$ signals as input  and gives rank-$r$ signals as output. The involved neighborhoods are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal N = \\{\\mathcal N_1, \\mathcal N_2,...,\\mathcal N_{2p+1}\\} =  \\{A_{\\uparrow, r}, A_{\\downarrow, r}, A_{\\uparrow, r}^2, A_{\\downarrow, r}^2,...,A_{\\uparrow, r}^p, A_{\\downarrow, r}^p, Q_r\\},\n",
    "\\end{equation}\n",
    "where $Q_r$ is a sparse projection operator (weighted matrix) over the kernel of the $r$-th Hodge Laplacian $L_r$, computed as in the original paper. $Q_r$ has the same topology of $L_r$.\n",
    "\n",
    "The equation of the SAN layer of this neural network is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{h}_x^{t+1} =  \\phi^l \\Bigg ( \\textbf{h}_x^{t}, \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}\\bigoplus_{y \\in \\mathcal{N}_k(x)}  \\widetilde{\\alpha}_k(h_x^t,hy^t)\\Bigg ),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\widetilde{\\alpha}_k$ being either an attention function $\\alpha_k$ if $\\mathcal{N}_k \\neq Q_r$ or a standard convolution term(affine transformation + weights) with weights given by the entries of $Q_r$ if $\\mathcal{N}_k = Q_r$.\n",
    "\n",
    "Therefore, the SAN layer is made by an attentional convolution from rank-$r$ cells to rank-$r$ cells using an adjacency message passing scheme up to $p$-hops neighborhoods:\n",
    "\n",
    "\\begin{align*}\n",
    "&ðŸŸ¥\\textrm{ Message.} &\\quad m_{(y \\rightarrow x),k} =&\n",
    "\\alpha_k(h_x^t,h_y^t) =\n",
    "a_k(h_x^{t}, h_y^{t}) \\cdot \\psi_k^t(h_x^{t})\\quad \\forall \\mathcal N_k \\in \\mathcal{N}\\\\\n",
    "\\\\\n",
    "&ðŸŸ§ \\textrm{ Within-Neighborhood Aggregation.} &\\quad m_{x,k}               =& \\bigoplus_{y \\in \\mathcal{N}_k(x)}  m_{(y \\rightarrow x),k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ© \\textrm{ Between-Neighborhood Aggregation.} &\\quad m_{x} =& \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}m_{x,k}\\\\\n",
    "\\\\\n",
    "&ðŸŸ¦ \\textrm{ Update.}&\\quad h_x^{t+1}                =& \\phi^{t}(h_x^t, m_{x})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform a binary node classification task using KarateClub dataset. We use a [\"GAT-like\" attention function](https://arxiv.org/abs/1710.10903), in which two different sets of attention weights $a_\\uparrow$ and $a_\\downarrow$ are learned for the upper neighborhoods $A_{\\uparrow,1}^p$ and for the lower neighborhoods $A_{\\downarrow,1}^p$ ($p=1,...,P$), respectively,   i.e.:\n",
    "\n",
    "- If $\\mathcal{N}_k \\neq Q_r$  and suppose, as an example, $\\mathcal{N}_k = A_{\\downarrow,1}^g$, the $g$-hops lower neighborhood:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = (\\textrm{softmax}_j(\\textrm{LeakyReLU}(a_{\\downarrow}^T[\\underset{p=1}{\\overset{P}{||}}h_x^{t}W_{\\downarrow,p}|| \\underset{p=1}{\\overset{P}{||}}h_y^{t}W_{\\downarrow,p}]))^g\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W_{\\downarrow,g}.\n",
    "\\end{align}\n",
    "\n",
    "- If $\\mathcal{N}_k = Q_r$:\n",
    "\\begin{align}\n",
    "&a_k(h_x^{t}, h_y^{t}) = Q_{x,y}\\\\\n",
    "& \\psi_k^t(h_x^{t}) = h_x^{t}W.\n",
    "\\end{align}\n",
    "\n",
    "$W$, $a_\\downarrow$, $a_\\uparrow$, \\{$W_{\\downarrow,p}\\}_{p=1}^P$ and $\\{W_{\\uparrow,p}\\}_{p=1}^P$ are learnable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZNrtWfL10pEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No module named 'igraph'. If you need to use hypernetx.algorithms.hypergraph_modularity, please install additional packages by running the following command: pip install .['all']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import toponetx.datasets.graph as graph\n",
    "import torch\n",
    "\n",
    "from topomodelx.nn.simplicial.san import SAN\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z05cyYcw0pEh",
    "outputId": "0ba2482b-dc68-451b-95bd-d2ea97e04378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPJIacee0pEh"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BreEb4B00pEi",
    "outputId": "fed8fb52-16b7-418e-812c-7f29bf32de1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fdT4Zjsp0pEi",
    "outputId": "054e2a94-3503-421d-d10c-8a9ceb21cce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78, 45, 11, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALHwd2Q40pEi"
   },
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "We now retrieve the neighborhoods (i.e. their representative matrices) that we will use to send messages on the domain. In this case, we decide w.l.o.g. to work at the edge level (thus considering a simplicial complex of order 2). We therefore need the lower and upper laplacians of rank 1, $L_{\\downarrow,1}=B_1^TB_1$ and $L_{\\uparrow,1}=B_2B_2^T$, both with dimensions $n_\\text{edges} \\times n_\\text{edges}$, where $B_1$ and $B_2$ are the incidence matrices of rank 1 and 2. Please notice that the binary adjacencies $A_{\\downarrow,1}^p$ and  $A_{\\uparrow,1}^p$ encoding the $p$-hops neighborhoods are given by the support (the non-zeros pattern) of $L_{\\downarrow,1}^p$ and $L_{\\uparrow,1}^p$, respectively. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5-BgSXz0pEj"
   },
   "source": [
    "**Remark.** In the case of rank-0 simplices (nodes), there is no lower Laplacian; in this case, we just initialize the down laplacian as a 0-matrix, and SAN automatically becomes a GAT-like architecture.\n",
    "In the case of simplices of maxium rank (the order of the complex), there is no upper Laplacian. In this case we can also initialize it as a 0 matrix and SAN will only consider the lower adjacencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nukmcsOJ0pEj"
   },
   "outputs": [],
   "source": [
    "simplex_order_k = 1\n",
    "# Down laplacian\n",
    "try:\n",
    "    laplacian_down = from_sparse(dataset.down_laplacian_matrix(rank=simplex_order_k))\n",
    "except ValueError:\n",
    "    laplacian_down = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()\n",
    "# Up laplacian\n",
    "try:\n",
    "    laplacian_up = from_sparse(dataset.up_laplacian_matrix(rank=simplex_order_k))\n",
    "except ValueError:\n",
    "    laplacian_up = torch.zeros(\n",
    "        (dataset.shape[simplex_order_k], dataset.shape[simplex_order_k])\n",
    "    ).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHCGA1gY0pEj"
   },
   "source": [
    "## Import signal ##\n",
    "\n",
    "We define edge features to be the gradient of the nodes features, i.e. given the node feature matrix $X_0$, we compute the edge features matrix as $X_1 = B_1^TX_0$. We will finally obtain the estimated node labels from the updated edge features by multiplying them again with $B_1$, i.e. the final nodes features are computed as the divergence of the final edge features.\n",
    "\n",
    "**Remark.** Please notice that also this way of deriving edges/nodes features from nodes/edges features could be seen as a (non-learnable) message passing between rank-0/1 cells (nodes/edges) and rank-1/0 cells (nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EBf26K5N0pEj",
    "outputId": "22acb245-384f-4d62-9e59-b55bdea3353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = list(dataset.get_simplex_attributes(\"node_feat\").values())\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "x_1 = list(dataset.get_simplex_attributes(\"edge_feat\").values())\n",
    "x_1 = torch.tensor(np.stack(x_1))\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "x_2 = list(dataset.get_simplex_attributes(\"face_feat\").values())\n",
    "x_2 = torch.tensor(np.stack(x_2))\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omDMQ60D0pEk"
   },
   "source": [
    "We use the incidence matrix between nodes-edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MmP56FAH0pEk"
   },
   "outputs": [],
   "source": [
    "incidence_0_1 = from_sparse(dataset.incidence_matrix(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTdYu3WX0pEk"
   },
   "source": [
    "The final edge features are obtained summing the original features of those edges plus the projection of the node features onto edges (using the incidence matrix accordingly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K2zU2_jL0pEk"
   },
   "outputs": [],
   "source": [
    "x = x_1 + torch.sparse.mm(incidence_0_1.T, x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWQ_NRiU0pEl"
   },
   "source": [
    "Hence, the final input features are defined by this sum, and we also pre-define the number of hidden and output channels of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cS7o2U620pEl"
   },
   "outputs": [],
   "source": [
    "in_channels = x.shape[-1]\n",
    "hidden_channels = 16\n",
    "out_channels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDnM5N2G0pEl"
   },
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert one-hot encode the binary labels, and keep the first four nodes for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ub-GWaMm0pEl"
   },
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_train = y_true[:30]\n",
    "y_test = y_true[-4:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTHtR74f0pEl"
   },
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the SAN class, we create our neural network with stacked layers. Given the considered dataset and task (Karate Club, node classification), a linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.base_model = SAN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, laplacian_up, laplacian_down):\n",
    "        x = self.base_model(x, laplacian_up, laplacian_down)\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2n-lFfkJ0pEm",
    "outputId": "3c633dee-f904-49ba-c169-f21499488cb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([78, 78]), torch.Size([78, 78]), torch.Size([78, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_up.shape, laplacian_down.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (base_model): SAN(\n",
       "    (layers): ModuleList(\n",
       "      (0): SANLayer(\n",
       "        (conv_down): SANConv()\n",
       "        (conv_up): SANConv()\n",
       "        (conv_harmonic): Conv()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = 1\n",
    "model = Network(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=n_layers,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UEoWB2G0pEm"
   },
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNOcultR0pEm"
   },
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sWopn2U60pEn",
    "outputId": "a2153bef-91eb-4f99-8aad-3166b4966c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7247 Train_acc: 0.3000\n",
      "Epoch: 2 loss: 0.7226 Train_acc: 0.6667\n",
      "Epoch: 3 loss: 0.7203 Train_acc: 0.6667\n",
      "Epoch: 4 loss: 0.7167 Train_acc: 0.6667\n",
      "Epoch: 5 loss: 0.7115 Train_acc: 0.6667\n",
      "Epoch: 6 loss: 0.7057 Train_acc: 0.7000\n",
      "Epoch: 7 loss: 0.6999 Train_acc: 0.7000\n",
      "Epoch: 8 loss: 0.6931 Train_acc: 0.7000\n",
      "Epoch: 9 loss: 0.6874 Train_acc: 0.7000\n",
      "Epoch: 10 loss: 0.6814 Train_acc: 0.7000\n",
      "Test_acc: 0.2500\n",
      "Epoch: 11 loss: 0.6760 Train_acc: 0.7333\n",
      "Epoch: 12 loss: 0.6716 Train_acc: 0.7333\n",
      "Epoch: 13 loss: 0.6669 Train_acc: 0.7333\n",
      "Epoch: 14 loss: 0.6624 Train_acc: 0.7000\n",
      "Epoch: 15 loss: 0.6586 Train_acc: 0.7000\n",
      "Epoch: 16 loss: 0.6551 Train_acc: 0.7667\n",
      "Epoch: 17 loss: 0.6528 Train_acc: 0.7667\n",
      "Epoch: 18 loss: 0.6508 Train_acc: 0.7667\n",
      "Epoch: 19 loss: 0.6491 Train_acc: 0.7667\n",
      "Epoch: 20 loss: 0.6479 Train_acc: 0.7667\n",
      "Test_acc: 0.2500\n",
      "Epoch: 21 loss: 0.6463 Train_acc: 0.7667\n",
      "Epoch: 22 loss: 0.6452 Train_acc: 0.7667\n",
      "Epoch: 23 loss: 0.6438 Train_acc: 0.7333\n",
      "Epoch: 24 loss: 0.6433 Train_acc: 0.7333\n",
      "Epoch: 25 loss: 0.6419 Train_acc: 0.7667\n",
      "Epoch: 26 loss: 0.6412 Train_acc: 0.7667\n",
      "Epoch: 27 loss: 0.6397 Train_acc: 0.7667\n",
      "Epoch: 28 loss: 0.6391 Train_acc: 0.7667\n",
      "Epoch: 29 loss: 0.6379 Train_acc: 0.7667\n",
      "Epoch: 30 loss: 0.6369 Train_acc: 0.7667\n",
      "Test_acc: 0.2500\n",
      "Epoch: 31 loss: 0.6360 Train_acc: 0.7667\n",
      "Epoch: 32 loss: 0.6347 Train_acc: 0.7667\n",
      "Epoch: 33 loss: 0.6333 Train_acc: 0.7667\n",
      "Epoch: 34 loss: 0.6317 Train_acc: 0.7667\n",
      "Epoch: 35 loss: 0.6298 Train_acc: 0.7667\n",
      "Epoch: 36 loss: 0.6282 Train_acc: 0.7667\n",
      "Epoch: 37 loss: 0.6272 Train_acc: 0.7667\n",
      "Epoch: 38 loss: 0.6267 Train_acc: 0.8000\n",
      "Epoch: 39 loss: 0.6265 Train_acc: 0.8000\n",
      "Epoch: 40 loss: 0.6262 Train_acc: 0.8000\n",
      "Test_acc: 0.2500\n",
      "Epoch: 41 loss: 0.6260 Train_acc: 0.8000\n",
      "Epoch: 42 loss: 0.6259 Train_acc: 0.8000\n",
      "Epoch: 43 loss: 0.6260 Train_acc: 0.8000\n",
      "Epoch: 44 loss: 0.6261 Train_acc: 0.7667\n",
      "Epoch: 45 loss: 0.6260 Train_acc: 0.7667\n",
      "Epoch: 46 loss: 0.6258 Train_acc: 0.8000\n",
      "Epoch: 47 loss: 0.6255 Train_acc: 0.8000\n",
      "Epoch: 48 loss: 0.6252 Train_acc: 0.8000\n",
      "Epoch: 49 loss: 0.6249 Train_acc: 0.8000\n",
      "Epoch: 50 loss: 0.6245 Train_acc: 0.8000\n",
      "Test_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 50\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat_edge = model(x, laplacian_up=laplacian_up, laplacian_down=laplacian_down)\n",
    "    # We project the edge-level output of the model to the node-level\n",
    "    # and apply softmax fn to get the final node-level classification output\n",
    "    y_hat = torch.softmax(torch.sparse.mm(incidence_0_1, y_hat_edge), dim=1)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_edge_test = model(\n",
    "                x, laplacian_up=laplacian_up, laplacian_down=laplacian_down\n",
    "            )\n",
    "            # Projection to node-level\n",
    "            y_hat_test = torch.softmax(\n",
    "                torch.sparse.mm(incidence_0_1, y_hat_edge_test), dim=1\n",
    "            )\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('topox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b696409e3d9b84bb97012e0a2d03075417bfa260eb8ad887be094cb925d5d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
