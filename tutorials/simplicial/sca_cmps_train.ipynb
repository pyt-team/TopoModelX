{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)\n",
    "\n",
    "\n",
    "ðŸŸ¥ $\\quad m_{y \\rightarrow x}^{(r \\rightarrow r'' \\rightarrow r)} = M(h_{x}^{t, (r)}, h_{y}^{t, (r)},att(h_{x}^{t, (r)}, h_{y}^{t, (r)}),x,y,{\\Theta^t}) \\qquad \\text{where } r'' < r < r'$\n",
    "\n",
    "ðŸŸ¥ $\\quad m_{y \\rightarrow x}^{(r'' \\rightarrow r)} = M(h_{x}^{t, (r)}, h_{y}^{t, (r'')},att(h_{x}^{t, (r)}, h_{y}^{t, (r'')}),x,y,{\\Theta^t})$\n",
    "\n",
    "ðŸŸ§ $\\quad m_x^{(r \\rightarrow r)}  = AGG_{y \\in \\mathcal{L}\\_\\downarrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "ðŸŸ§ $\\quad m_x^{(r'' \\rightarrow r)} = AGG_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r'' \\rightarrow r)}$\n",
    "\n",
    "ðŸŸ© $\\quad m_x^{(r)}  = \\text{AGG}\\_{\\mathcal{N}\\_k \\in \\mathcal{N}}(m_x^{(k)})$\n",
    "\n",
    "ðŸŸ¦ $\\quad h_{x}^{t+1, (r)} = U(h_x^{t, (r)}, m_{x}^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import toponetx.datasets.graph as graph\n",
    "import torch\n",
    "\n",
    "from topomodelx.nn.simplicial.sca_cmps import SCACMPS\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPUs are available we will make use of them. Otherwise, we will use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78, 45, 11, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "#### Coadjacency Message Passing Scheme (CMPS):\n",
    "This will require features from faces, and edges again, but outputs features on the edges. The first neighborhood matrix will be the level 2 lower Laplacian, $L_{\\downarrow, 2}$, and the second neighborhood matrix will be the transpose of the incidence matrix of the faces, $B_{2}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_down_1 = dataset.down_laplacian_matrix(rank=1)\n",
    "laplacian_down_2 = dataset.down_laplacian_matrix(rank=2)\n",
    "incidence_1_t = dataset.incidence_matrix(rank=1).T\n",
    "incidence_2_t = dataset.incidence_matrix(rank=2).T\n",
    "\n",
    "laplacian_down_1 = from_sparse(laplacian_down_1)\n",
    "laplacian_down_2 = from_sparse(laplacian_down_2)\n",
    "incidence_1_t = from_sparse(incidence_1_t)\n",
    "incidence_2_t = from_sparse(incidence_2_t)\n",
    "\n",
    "laplacian_down_list = [laplacian_down_1, laplacian_down_2]\n",
    "incidence_t_list = [incidence_1_t, incidence_2_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "We retrieve an input signal on the nodes, edges and faces. The signal will have shape $n_\\text{simplicial} \\times$ in_channels, where in_channels is the dimension of each simplicial's feature. Here, we have in_channels = channels_nodes $ = 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = torch.tensor(np.stack(x_2))\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pre-define the number output channels of the model, in this case the number of node classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x_0.shape[-1]\n",
    "out_channels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert one-hot encode the binary labels, and keep the first four nodes for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_train = y_true[:30]\n",
    "y_test = y_true[-4:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Networks\n",
    "\n",
    "Using the SCACMPSLayer class, we create a neural network with a modifiable number of layers each following the CMPS at each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, in_channels_all, out_channels, complex_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.base_model = SCACMPS(\n",
    "            in_channels_all=in_channels_all,\n",
    "            complex_dim=complex_dim,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.lin0 = torch.nn.Linear(in_channels_all[0], out_channels)\n",
    "\n",
    "    def forward(self, x, laplacian_down_list, incidence_t_list):\n",
    "        x = self.base_model(x, laplacian_down_list, incidence_t_list)\n",
    "        x_0 = self.lin0(x[0])\n",
    "        return torch.softmax(x_0, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x_0, x_1, x_2]\n",
    "in_channels_all = [x_0[0].shape[-1], x_1[0].shape[-1], x_2[0].shape[-1]]\n",
    "out_channels = 2\n",
    "complex_dim = 3\n",
    "n_layers = 1\n",
    "\n",
    "model = Network(\n",
    "    in_channels_all=in_channels_all,\n",
    "    out_channels=out_channels,\n",
    "    complex_dim=complex_dim,\n",
    "    n_layers=n_layers,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7272 Train_acc: 0.4333\n",
      "Epoch: 2 loss: 0.7215 Train_acc: 0.5667\n",
      "Epoch: 3 loss: 0.7166 Train_acc: 0.5667\n",
      "Epoch: 4 loss: 0.7126 Train_acc: 0.5667\n",
      "Epoch: 5 loss: 0.7097 Train_acc: 0.5667\n",
      "Epoch: 6 loss: 0.7076 Train_acc: 0.5667\n",
      "Epoch: 7 loss: 0.7062 Train_acc: 0.5667\n",
      "Epoch: 8 loss: 0.7052 Train_acc: 0.5667\n",
      "Epoch: 9 loss: 0.7044 Train_acc: 0.5667\n",
      "Epoch: 10 loss: 0.7037 Train_acc: 0.5667\n",
      "Test_acc: 0.0000\n",
      "Epoch: 11 loss: 0.7028 Train_acc: 0.5667\n",
      "Epoch: 12 loss: 0.7018 Train_acc: 0.5667\n",
      "Epoch: 13 loss: 0.7006 Train_acc: 0.5667\n",
      "Epoch: 14 loss: 0.6991 Train_acc: 0.5667\n",
      "Epoch: 15 loss: 0.6974 Train_acc: 0.5667\n",
      "Epoch: 16 loss: 0.6955 Train_acc: 0.5667\n",
      "Epoch: 17 loss: 0.6934 Train_acc: 0.5667\n",
      "Epoch: 18 loss: 0.6913 Train_acc: 0.5667\n",
      "Epoch: 19 loss: 0.6891 Train_acc: 0.5667\n",
      "Epoch: 20 loss: 0.6870 Train_acc: 0.5667\n",
      "Test_acc: 0.0000\n",
      "Epoch: 21 loss: 0.6852 Train_acc: 0.5667\n",
      "Epoch: 22 loss: 0.6836 Train_acc: 0.5667\n",
      "Epoch: 23 loss: 0.6822 Train_acc: 0.7000\n",
      "Epoch: 24 loss: 0.6810 Train_acc: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 loss: 0.6798 Train_acc: 0.9667\n",
      "Epoch: 26 loss: 0.6784 Train_acc: 1.0000\n",
      "Epoch: 27 loss: 0.6768 Train_acc: 1.0000\n",
      "Epoch: 28 loss: 0.6750 Train_acc: 1.0000\n",
      "Epoch: 29 loss: 0.6731 Train_acc: 0.9667\n",
      "Epoch: 30 loss: 0.6712 Train_acc: 0.9333\n",
      "Test_acc: 0.5000\n",
      "Epoch: 31 loss: 0.6695 Train_acc: 0.9333\n",
      "Epoch: 32 loss: 0.6679 Train_acc: 0.9333\n",
      "Epoch: 33 loss: 0.6664 Train_acc: 0.9000\n",
      "Epoch: 34 loss: 0.6651 Train_acc: 0.9000\n",
      "Epoch: 35 loss: 0.6637 Train_acc: 0.9000\n",
      "Epoch: 36 loss: 0.6623 Train_acc: 0.9000\n",
      "Epoch: 37 loss: 0.6608 Train_acc: 0.9000\n",
      "Epoch: 38 loss: 0.6593 Train_acc: 0.9333\n",
      "Epoch: 39 loss: 0.6577 Train_acc: 0.9333\n",
      "Epoch: 40 loss: 0.6562 Train_acc: 0.9333\n",
      "Test_acc: 0.7500\n",
      "Epoch: 41 loss: 0.6547 Train_acc: 0.9667\n",
      "Epoch: 42 loss: 0.6533 Train_acc: 1.0000\n",
      "Epoch: 43 loss: 0.6520 Train_acc: 1.0000\n",
      "Epoch: 44 loss: 0.6507 Train_acc: 1.0000\n",
      "Epoch: 45 loss: 0.6494 Train_acc: 0.9667\n",
      "Epoch: 46 loss: 0.6481 Train_acc: 0.9667\n",
      "Epoch: 47 loss: 0.6468 Train_acc: 0.9667\n",
      "Epoch: 48 loss: 0.6455 Train_acc: 1.0000\n",
      "Epoch: 49 loss: 0.6441 Train_acc: 1.0000\n",
      "Epoch: 50 loss: 0.6429 Train_acc: 1.0000\n",
      "Test_acc: 1.0000\n",
      "Epoch: 51 loss: 0.6417 Train_acc: 1.0000\n",
      "Epoch: 52 loss: 0.6405 Train_acc: 1.0000\n",
      "Epoch: 53 loss: 0.6393 Train_acc: 1.0000\n",
      "Epoch: 54 loss: 0.6382 Train_acc: 1.0000\n",
      "Epoch: 55 loss: 0.6370 Train_acc: 1.0000\n",
      "Epoch: 56 loss: 0.6358 Train_acc: 1.0000\n",
      "Epoch: 57 loss: 0.6347 Train_acc: 1.0000\n",
      "Epoch: 58 loss: 0.6336 Train_acc: 1.0000\n",
      "Epoch: 59 loss: 0.6325 Train_acc: 0.9667\n",
      "Epoch: 60 loss: 0.6314 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 61 loss: 0.6304 Train_acc: 0.9667\n",
      "Epoch: 62 loss: 0.6294 Train_acc: 0.9667\n",
      "Epoch: 63 loss: 0.6283 Train_acc: 0.9667\n",
      "Epoch: 64 loss: 0.6273 Train_acc: 0.9667\n",
      "Epoch: 65 loss: 0.6263 Train_acc: 0.9667\n",
      "Epoch: 66 loss: 0.6253 Train_acc: 0.9667\n",
      "Epoch: 67 loss: 0.6244 Train_acc: 0.9667\n",
      "Epoch: 68 loss: 0.6234 Train_acc: 0.9667\n",
      "Epoch: 69 loss: 0.6225 Train_acc: 0.9667\n",
      "Epoch: 70 loss: 0.6216 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 71 loss: 0.6207 Train_acc: 0.9667\n",
      "Epoch: 72 loss: 0.6198 Train_acc: 0.9667\n",
      "Epoch: 73 loss: 0.6189 Train_acc: 0.9667\n",
      "Epoch: 74 loss: 0.6180 Train_acc: 0.9667\n",
      "Epoch: 75 loss: 0.6172 Train_acc: 0.9667\n",
      "Epoch: 76 loss: 0.6164 Train_acc: 0.9667\n",
      "Epoch: 77 loss: 0.6155 Train_acc: 0.9667\n",
      "Epoch: 78 loss: 0.6147 Train_acc: 0.9667\n",
      "Epoch: 79 loss: 0.6139 Train_acc: 0.9667\n",
      "Epoch: 80 loss: 0.6131 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 81 loss: 0.6124 Train_acc: 0.9667\n",
      "Epoch: 82 loss: 0.6116 Train_acc: 0.9667\n",
      "Epoch: 83 loss: 0.6108 Train_acc: 0.9667\n",
      "Epoch: 84 loss: 0.6101 Train_acc: 0.9667\n",
      "Epoch: 85 loss: 0.6094 Train_acc: 0.9667\n",
      "Epoch: 86 loss: 0.6086 Train_acc: 0.9667\n",
      "Epoch: 87 loss: 0.6079 Train_acc: 0.9667\n",
      "Epoch: 88 loss: 0.6072 Train_acc: 0.9667\n",
      "Epoch: 89 loss: 0.6065 Train_acc: 0.9667\n",
      "Epoch: 90 loss: 0.6058 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 91 loss: 0.6052 Train_acc: 0.9667\n",
      "Epoch: 92 loss: 0.6045 Train_acc: 0.9667\n",
      "Epoch: 93 loss: 0.6038 Train_acc: 0.9667\n",
      "Epoch: 94 loss: 0.6032 Train_acc: 0.9667\n",
      "Epoch: 95 loss: 0.6026 Train_acc: 0.9667\n",
      "Epoch: 96 loss: 0.6019 Train_acc: 0.9667\n",
      "Epoch: 97 loss: 0.6013 Train_acc: 0.9667\n",
      "Epoch: 98 loss: 0.6007 Train_acc: 0.9667\n",
      "Epoch: 99 loss: 0.6001 Train_acc: 0.9667\n",
      "Epoch: 100 loss: 0.5995 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 101 loss: 0.5989 Train_acc: 0.9667\n",
      "Epoch: 102 loss: 0.5983 Train_acc: 0.9667\n",
      "Epoch: 103 loss: 0.5977 Train_acc: 0.9667\n",
      "Epoch: 104 loss: 0.5972 Train_acc: 0.9667\n",
      "Epoch: 105 loss: 0.5966 Train_acc: 0.9667\n",
      "Epoch: 106 loss: 0.5961 Train_acc: 0.9667\n",
      "Epoch: 107 loss: 0.5955 Train_acc: 0.9667\n",
      "Epoch: 108 loss: 0.5950 Train_acc: 0.9667\n",
      "Epoch: 109 loss: 0.5944 Train_acc: 0.9667\n",
      "Epoch: 110 loss: 0.5939 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 111 loss: 0.5934 Train_acc: 0.9667\n",
      "Epoch: 112 loss: 0.5929 Train_acc: 0.9667\n",
      "Epoch: 113 loss: 0.5924 Train_acc: 0.9667\n",
      "Epoch: 114 loss: 0.5919 Train_acc: 0.9667\n",
      "Epoch: 115 loss: 0.5914 Train_acc: 0.9667\n",
      "Epoch: 116 loss: 0.5909 Train_acc: 0.9667\n",
      "Epoch: 117 loss: 0.5904 Train_acc: 0.9667\n",
      "Epoch: 118 loss: 0.5899 Train_acc: 0.9667\n",
      "Epoch: 119 loss: 0.5894 Train_acc: 0.9667\n",
      "Epoch: 120 loss: 0.5890 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 121 loss: 0.5885 Train_acc: 0.9667\n",
      "Epoch: 122 loss: 0.5881 Train_acc: 0.9667\n",
      "Epoch: 123 loss: 0.5876 Train_acc: 0.9667\n",
      "Epoch: 124 loss: 0.5872 Train_acc: 0.9667\n",
      "Epoch: 125 loss: 0.5867 Train_acc: 0.9667\n",
      "Epoch: 126 loss: 0.5863 Train_acc: 0.9667\n",
      "Epoch: 127 loss: 0.5859 Train_acc: 0.9667\n",
      "Epoch: 128 loss: 0.5854 Train_acc: 0.9667\n",
      "Epoch: 129 loss: 0.5850 Train_acc: 0.9667\n",
      "Epoch: 130 loss: 0.5846 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 131 loss: 0.5842 Train_acc: 0.9667\n",
      "Epoch: 132 loss: 0.5838 Train_acc: 0.9667\n",
      "Epoch: 133 loss: 0.5834 Train_acc: 0.9667\n",
      "Epoch: 134 loss: 0.5830 Train_acc: 0.9667\n",
      "Epoch: 135 loss: 0.5826 Train_acc: 0.9667\n",
      "Epoch: 136 loss: 0.5822 Train_acc: 0.9667\n",
      "Epoch: 137 loss: 0.5818 Train_acc: 0.9667\n",
      "Epoch: 138 loss: 0.5814 Train_acc: 0.9667\n",
      "Epoch: 139 loss: 0.5810 Train_acc: 0.9667\n",
      "Epoch: 140 loss: 0.5806 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 141 loss: 0.5803 Train_acc: 0.9667\n",
      "Epoch: 142 loss: 0.5799 Train_acc: 0.9667\n",
      "Epoch: 143 loss: 0.5795 Train_acc: 0.9667\n",
      "Epoch: 144 loss: 0.5792 Train_acc: 0.9667\n",
      "Epoch: 145 loss: 0.5788 Train_acc: 0.9667\n",
      "Epoch: 146 loss: 0.5785 Train_acc: 0.9667\n",
      "Epoch: 147 loss: 0.5781 Train_acc: 0.9667\n",
      "Epoch: 148 loss: 0.5778 Train_acc: 0.9667\n",
      "Epoch: 149 loss: 0.5774 Train_acc: 0.9667\n",
      "Epoch: 150 loss: 0.5771 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 151 loss: 0.5768 Train_acc: 0.9667\n",
      "Epoch: 152 loss: 0.5764 Train_acc: 0.9667\n",
      "Epoch: 153 loss: 0.5761 Train_acc: 0.9667\n",
      "Epoch: 154 loss: 0.5758 Train_acc: 0.9667\n",
      "Epoch: 155 loss: 0.5755 Train_acc: 0.9667\n",
      "Epoch: 156 loss: 0.5751 Train_acc: 0.9667\n",
      "Epoch: 157 loss: 0.5748 Train_acc: 0.9667\n",
      "Epoch: 158 loss: 0.5745 Train_acc: 0.9667\n",
      "Epoch: 159 loss: 0.5742 Train_acc: 0.9667\n",
      "Epoch: 160 loss: 0.5739 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 161 loss: 0.5736 Train_acc: 0.9667\n",
      "Epoch: 162 loss: 0.5733 Train_acc: 0.9667\n",
      "Epoch: 163 loss: 0.5730 Train_acc: 0.9667\n",
      "Epoch: 164 loss: 0.5727 Train_acc: 0.9667\n",
      "Epoch: 165 loss: 0.5724 Train_acc: 0.9667\n",
      "Epoch: 166 loss: 0.5721 Train_acc: 0.9667\n",
      "Epoch: 167 loss: 0.5718 Train_acc: 0.9667\n",
      "Epoch: 168 loss: 0.5715 Train_acc: 0.9667\n",
      "Epoch: 169 loss: 0.5712 Train_acc: 0.9667\n",
      "Epoch: 170 loss: 0.5709 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 171 loss: 0.5707 Train_acc: 0.9667\n",
      "Epoch: 172 loss: 0.5704 Train_acc: 0.9667\n",
      "Epoch: 173 loss: 0.5701 Train_acc: 0.9667\n",
      "Epoch: 174 loss: 0.5698 Train_acc: 0.9667\n",
      "Epoch: 175 loss: 0.5696 Train_acc: 0.9667\n",
      "Epoch: 176 loss: 0.5693 Train_acc: 0.9667\n",
      "Epoch: 177 loss: 0.5690 Train_acc: 0.9667\n",
      "Epoch: 178 loss: 0.5688 Train_acc: 0.9667\n",
      "Epoch: 179 loss: 0.5685 Train_acc: 0.9667\n",
      "Epoch: 180 loss: 0.5683 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 181 loss: 0.5680 Train_acc: 0.9667\n",
      "Epoch: 182 loss: 0.5678 Train_acc: 0.9667\n",
      "Epoch: 183 loss: 0.5675 Train_acc: 0.9667\n",
      "Epoch: 184 loss: 0.5673 Train_acc: 0.9667\n",
      "Epoch: 185 loss: 0.5670 Train_acc: 0.9667\n",
      "Epoch: 186 loss: 0.5668 Train_acc: 0.9667\n",
      "Epoch: 187 loss: 0.5665 Train_acc: 0.9667\n",
      "Epoch: 188 loss: 0.5663 Train_acc: 0.9667\n",
      "Epoch: 189 loss: 0.5660 Train_acc: 0.9667\n",
      "Epoch: 190 loss: 0.5658 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n",
      "Epoch: 191 loss: 0.5656 Train_acc: 0.9667\n",
      "Epoch: 192 loss: 0.5653 Train_acc: 0.9667\n",
      "Epoch: 193 loss: 0.5651 Train_acc: 0.9667\n",
      "Epoch: 194 loss: 0.5649 Train_acc: 0.9667\n",
      "Epoch: 195 loss: 0.5647 Train_acc: 0.9667\n",
      "Epoch: 196 loss: 0.5644 Train_acc: 0.9667\n",
      "Epoch: 197 loss: 0.5642 Train_acc: 0.9667\n",
      "Epoch: 198 loss: 0.5640 Train_acc: 0.9667\n",
      "Epoch: 199 loss: 0.5638 Train_acc: 0.9667\n",
      "Epoch: 200 loss: 0.5635 Train_acc: 0.9667\n",
      "Test_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 200\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(x, laplacian_down_list, incidence_t_list)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x, laplacian_down_list, incidence_t_list)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
