{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Combinatorial Complex Attention Neural Network for Mesh Classification.\n",
    "\n",
    "We create and train a combinatorial complex attention neural network for mesh classification, introduced in [Figure 35(b), Hajij et. al : Topological Deep Learning: Going Beyond Graph Data (2023)](https://arxiv.org/pdf/2206.00606.pdf).\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The neural network is composed of a sequence of identical attention layers for a dimension two combinatorial complex. Each layer is composed of two levels. In both levels, messages computed for the cells of identical dimension are aggregated using a sum operation. All the messages are computed using the attention mechanisms for squared and non-squared neighborhoods presented in [Definitions 31, 32, and 33, Hajij et. al : Topological Deep Learning: Going Beyond Graph Data (2023)](https://arxiv.org/pdf/2206.00606.pdf). The following message passing scheme is followed in each of the levels for each layer:\n",
    "\n",
    "1. First level:\n",
    "\n",
    "游린 $\\quad m^{0\\rightarrow 0}_{y\\rightarrow x} = \\phi\\left(\\left((A_{\\uparrow, 0})_{xy} \\cdot \\text{att}_{xy}^{0\\rightarrow 0}\\right) h_y^{t,(0)} \\Theta^t_{0\\rightarrow 0}\\right)$\n",
    "\n",
    "游린 $\\quad m^{0\\rightarrow 1}_{y\\rightarrow x} = \\phi\\left(\\left((B_{1}^T)_{xy} \\cdot \\text{att}_{xy}^{0\\rightarrow 1}\\right) h_y^{t,(0)} \\Theta^t_{0\\rightarrow 1}\\right)$\n",
    "\n",
    "游린 $\\quad  m^{1\\rightarrow 0}_{y\\rightarrow x} = \\phi\\left(\\left((B_{1})_{xy} \\cdot \\text{att}_{xy}^{1\\rightarrow 0}\\right) h_y^{t,(1)} \\Theta^t_{1\\rightarrow 0}\\right)$\n",
    "\n",
    "游린 $\\quad  m^{1\\rightarrow 2}_{y\\rightarrow x} = \\phi\\left(\\left((B_{2}^T)_{xy} \\cdot \\text{att}_{xy}^{1\\rightarrow 2}\\right) h_y^{t,(1)} \\Theta^t_{1\\rightarrow 2}\\right)$\n",
    "\n",
    "游린 $\\quad m^{2\\rightarrow 1}_{y\\rightarrow x} = \\phi\\left(\\left((B_{2})_{xy} \\cdot \\text{att}_{xy}^{2\\rightarrow 1}\\right) h_y^{t,(2)} \\Theta^t_{2\\rightarrow 1}\\right)$\n",
    "\n",
    "游릲 $\\quad m^{0\\rightarrow 0}_{x}=\\sum_{y\\in A_{\\uparrow, 0}(x)} m^{0\\rightarrow 0}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{0\\rightarrow 1}_{x}=\\sum_{y\\in B_{1}^T(x)} m^{0\\rightarrow 1}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{1\\rightarrow 0}_{x}=\\sum_{y\\in B_{1}(x)} m^{1\\rightarrow 0}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{1\\rightarrow 2}_{x}=\\sum_{y\\in B_{2}^T(x)} m^{1\\rightarrow 2}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{2\\rightarrow 1}_{x}=\\sum_{y\\in B_{2}(x)} m^{2\\rightarrow 1}_{y\\rightarrow x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}=m^{0\\rightarrow 0}_{x}+m^{1\\rightarrow 0}_{x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}=m^{0\\rightarrow 1}_{x}+m^{2\\rightarrow 1}_{x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(2)}=m^{1\\rightarrow 2}_{x}$\n",
    "\n",
    "游릱 $\\quad i_x^{t,(0)} = m_x^{(0)}$\n",
    "\n",
    "游릱 $\\quad i_x^{t,(1)} = m_x^{(1)}$\n",
    "\n",
    "游릱 $\\quad i_x^{t,(2)} = m_x^{(2)}$\n",
    "\n",
    "where $i_x^{t,(\\cdot)}$ represents intermediate feature vectors.\n",
    "\n",
    "\n",
    "2. Second level:\n",
    "\n",
    "\n",
    "游린 $\\quad m^{0\\rightarrow 0}_{y\\rightarrow x} = \\phi\\left(\\left((A_{\\uparrow, 0})_{xy} \\cdot \\text{att}_{xy}^{0\\rightarrow 0}\\right) i_y^{t,(0)} \\Theta^t_{0\\rightarrow 0}\\right)$\n",
    "\n",
    "游린 $\\quad m^{1\\rightarrow 1}_{y\\rightarrow x} = \\phi\\left(\\left((A_{\\uparrow, 1})_{xy} \\cdot \\text{att}_{xy}^{1\\rightarrow 1}\\right) i_y^{t,(1)} \\Theta^t_{1\\rightarrow 1}\\right)$\n",
    "\n",
    "游린 $\\quad m^{2\\rightarrow 2}_{y\\rightarrow x} = \\phi\\left(\\left((A_{\\downarrow, 2})_{xy} \\cdot \\text{att}_{xy}^{2\\rightarrow 2}\\right) i_y^{t,(2)} \\Theta^t_{2\\rightarrow 2}\\right)$\n",
    "\n",
    "游린 $\\quad m^{0\\rightarrow 1}_{y\\rightarrow x} = \\phi\\left(\\left((B_{1}^T)_{xy} \\cdot \\text{att}_{xy}^{0\\rightarrow 1}\\right) i_y^{t,(0)} \\Theta^t_{0\\rightarrow 1}\\right)$\n",
    "\n",
    "游린 $\\quad m^{1\\rightarrow 2}_{y\\rightarrow x} = \\phi\\left(\\left((B_{2}^T)_{xy} \\cdot \\text{att}_{xy}^{1\\rightarrow 2}\\right) i_y^{t,(1)} \\Theta^t_{1\\rightarrow 2}\\right)$\n",
    "\n",
    "游릲 $\\quad m^{0\\rightarrow 0}_{x}=\\sum_{y\\in A_{\\uparrow, 0}(x)} m^{0\\rightarrow 0}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{1\\rightarrow 1}_{x}=\\sum_{y\\in A_{\\uparrow, 1}(x)} m^{1\\rightarrow 1}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{2\\rightarrow 2}_{x}=\\sum_{y\\in A_{\\downarrow, 2}(x)} m^{2\\rightarrow 2}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{0\\rightarrow 1}_{x}=\\sum_{y\\in B_{1}^T(x)} m^{0\\rightarrow 1}_{y\\rightarrow x}$\n",
    "\n",
    "游릲 $\\quad m^{1\\rightarrow 2}_{x}=\\sum_{y\\in B_{2}^T(x)} m^{1\\rightarrow 2}_{y\\rightarrow x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}=m^{0\\rightarrow 0}_{x}+m^{1\\rightarrow 0}_{x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}=m^{1\\rightarrow 1}_{x} + m^{0\\rightarrow 1}_{x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(2)}=m^{1\\rightarrow 2}_{x} + m^{2\\rightarrow 2}_{x}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(0)} = m_x^{(0)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(1)} = m_x^{(1)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(2)} = m_x^{(2)}$\n",
    "\n",
    "In both message passing levels, $\\phi$ represents a common activation function. Also, $\\Theta$ and $\\text{att}$ represent learnable weights and attention matrices, respectively, that are different in each level. Attention matrices are introduced in [Figure 35(b), Hajij et. al : Topological Deep Learning: Going Beyond Graph Data (2023)](https://arxiv.org/pdf/2206.00606.pdf). In this case, attention matrices are computed using the LeakyReLU activation function, as in previous versions of the paper.\n",
    "\n",
    "Notations, adjacency, coadjacency, and incidence matrices are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031). The tensor diagram for the layer can be found in the first column and last row of Figure 11, from the same paper.\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on [`MUTAG` from the TUDataset](https://paperswithcode.com/dataset/mutag). This dataset contains:\n",
    "- 188 samples of chemical compounds represented as graphs,\n",
    "- with 7 discrete node features.\n",
    "\n",
    "The task is to predict the mutagenicity of each compound on Salmonella typhimurium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:37:14.219487278Z",
     "start_time": "2023-07-02T06:37:09.970974275Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from toponetx import CombinatorialComplex\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from toponetx.datasets.mesh import shrec_16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from topomodelx.nn.combinatorial.hmc_layer import HMCLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:37:15.491268482Z",
     "start_time": "2023-07-02T06:37:14.221183612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "We import a subset of MUTAG, a benchmark dataset for graph classification.\n",
    "\n",
    "We then lift each graph into our topological domain of choice, here: a cell complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0` and `x_1` on the nodes (0-cells) and edges (1-cells) for each complex: these will be the model's inputs,\n",
    "- a binary classification label `y` associated to the cell complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node features:\n",
    "    - Position\n",
    "    - Normal\n",
    "\n",
    "Edge features:\n",
    "    - Dihedral angle\n",
    "    - Edge span\n",
    "    - 2 edge angle in the triangle\n",
    "    - 6 edge ratios\n",
    "\n",
    "Face features:\n",
    "    - Face area\n",
    "    - Face normal\n",
    "    - Face angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:37:16.591113383Z",
     "start_time": "2023-07-02T06:37:15.477204458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipping the files...\n",
      "\n",
      "done!\n",
      "Loading dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec_training, shrec_testing = shrec_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:37:47.512826900Z",
     "start_time": "2023-07-02T06:37:47.452390153Z"
    }
   },
   "outputs": [],
   "source": [
    "class SHRECDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.complexes = [cc.to_combinatorial_complex() for cc in data[\"complexes\"]]\n",
    "        self.x_0 = data[\"node_feat\"]\n",
    "        self.x_1 = data[\"edge_feat\"]\n",
    "        self.x_2 = data[\"face_feat\"]\n",
    "        self.y = data[\"label\"]\n",
    "        self.a0, self.a1, self.coa2, self.b1, self.b2 = self._get_neighborhood_matrix()\n",
    "\n",
    "    def _get_neighborhood_matrix(self):\n",
    "\n",
    "        a0 = []\n",
    "        a1 = []\n",
    "        coa2 = []\n",
    "        b1 = []\n",
    "        b2 = []\n",
    "\n",
    "        for cc in self.complexes:\n",
    "\n",
    "            a0.append(torch.from_numpy(cc.adjacency_matrix(0, 1).todense()).to_sparse())\n",
    "            a1.append(torch.from_numpy(cc.adjacency_matrix(1, 2).todense()).to_sparse())\n",
    "\n",
    "            B = cc.incidence_matrix(rank=2, to_rank=1)\n",
    "            A = B.T @ B\n",
    "            A.setdiag(0)\n",
    "            coa2.append(torch.from_numpy(A.todense()).to_sparse())\n",
    "\n",
    "            b1.append(torch.from_numpy(cc.incidence_matrix(1, 0).todense()).to_sparse())\n",
    "            b2.append(torch.from_numpy(cc.incidence_matrix(2, 1).todense()).to_sparse())\n",
    "\n",
    "        return a0, a1, coa2, b1, b2\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(np.unique(self.y))\n",
    "\n",
    "    def channels_dim(self):\n",
    "        return self.x_0[0].shape[1], self.x_1[0].shape[1], self.x_2[0].shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.complexes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.x_0[idx],\n",
    "            self.x_1[idx],\n",
    "            self.x_2[idx],\n",
    "            self.a0[idx],\n",
    "            self.a1[idx],\n",
    "            self.coa2[idx],\n",
    "            self.b1[idx],\n",
    "            self.b2[idx],\n",
    "            self.y[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:38:53.397404792Z",
     "start_time": "2023-07-02T06:37:49.462752702Z"
    }
   },
   "outputs": [],
   "source": [
    "training_dataset = SHRECDataset(shrec_training)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:39:59.609357898Z",
     "start_time": "2023-07-02T06:38:53.437838127Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_dataset = SHRECDataset(shrec_training)\n",
    "testing_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T09:12:40.935701Z",
     "start_time": "2023-06-29T09:12:40.933077Z"
    }
   },
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the CCXNLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T06:39:59.651376734Z",
     "start_time": "2023-07-02T06:39:59.650355941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of input features on nodes, edges and faces are: 6, 10 and 7.\n"
     ]
    }
   ],
   "source": [
    "d_0, d_1, d_2 = training_dataset.channels_dim()\n",
    "in_channels = [d_0, d_1, d_2]\n",
    "print(\n",
    "    f\"The dimension of input features on nodes, edges and faces are: {d_0}, {d_1} and {d_2}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T07:11:39.886151084Z",
     "start_time": "2023-07-02T07:11:39.843721535Z"
    }
   },
   "outputs": [],
   "source": [
    "class HoanMeshClassifier(torch.nn.Module):\n",
    "    \"\"\"HoanMeshClassifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : List[int]\n",
    "        Dimension of input features on nodes, edges and faces respectively.\n",
    "    intermediate_channels : List[int]\n",
    "        Dimension of intermediate features on nodes, edges and faces respectively.\n",
    "    out_channels : List[int]\n",
    "        Dimension of output features on nodes, edges and faces respectively.\n",
    "    num_classes : int\n",
    "        Number of classes.\n",
    "    n_layers : int\n",
    "        Number of CCXN layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        intermediate_channels,\n",
    "        out_channels,\n",
    "        num_classes,\n",
    "        negative_slope=0.2,\n",
    "        n_layers=1,\n",
    "        final_layer_embedding_dimension=20,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            HMCLayer(\n",
    "                in_channels=in_channels,\n",
    "                intermediate_channels=intermediate_channels,\n",
    "                out_channels=out_channels,\n",
    "                negative_slope=negative_slope,\n",
    "                softmax_attention=True,\n",
    "                update_func_attention='relu',\n",
    "                update_func_aggregation='relu',\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        )\n",
    "        self.l0 = torch.nn.Linear(out_channels[0], final_layer_embedding_dimension)\n",
    "        self.l1 = torch.nn.Linear(out_channels[1], final_layer_embedding_dimension)\n",
    "        self.l2 = torch.nn.Linear(out_channels[2], final_layer_embedding_dimension)\n",
    "        self.final_layer = torch.nn.Linear(\n",
    "            3 * final_layer_embedding_dimension, num_classes\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        neighborhood_0_to_0,\n",
    "        neighborhood_1_to_1,\n",
    "        neighborhood_2_to_2,\n",
    "        neighborhood_0_to_1,\n",
    "        neighborhood_1_to_2,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_0, x_1, x_2 = layer(\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                neighborhood_0_to_0,\n",
    "                neighborhood_1_to_1,\n",
    "                neighborhood_2_to_2,\n",
    "                neighborhood_0_to_1,\n",
    "                neighborhood_1_to_2,\n",
    "            )\n",
    "        \"\"\"\n",
    "        # Take the sum of the 2D, 1D, and 0D cell features. If they are NaN, convert them to 0.\n",
    "        two_dimensional_cells_mean = torch.nansum(x_2, dim=0)\n",
    "        two_dimensional_cells_mean[torch.isnan(two_dimensional_cells_mean)] = 0\n",
    "        one_dimensional_cells_mean = torch.nanmean(x_1, dim=0)\n",
    "        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] = 0\n",
    "        zero_dimensional_cells_mean = torch.nanmean(x_0, dim=0)\n",
    "        zero_dimensional_cells_mean[torch.isnan(zero_dimensional_cells_mean)] = 0\n",
    "\n",
    "        x_0 = self.l0(zero_dimensional_cells_mean)\n",
    "        x_1 = self.l1(one_dimensional_cells_mean)\n",
    "        x_2 = self.l2(two_dimensional_cells_mean)\n",
    "\n",
    "\n",
    "        #return F.softmax(x_0 + x_1 + x_2, dim=-1)\n",
    "        return x_0 + x_1 + x_2\n",
    "        \"\"\"\n",
    "        x_0 = self.l0(x_0)\n",
    "        x_1 = self.l1(x_1)\n",
    "        x_2 = self.l2(x_2)\n",
    "        # Sum all the elements in the dimension zero\n",
    "        x_0 = torch.nanmean(x_0, dim=0)\n",
    "        x_1 = torch.nanmean(x_1, dim=0)\n",
    "        x_2 = torch.nanmean(x_2, dim=0)\n",
    "        x = torch.cat((x_0, x_1, x_2), dim=0)\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T10:21:11.467775Z",
     "start_time": "2023-06-29T10:21:11.463344Z"
    }
   },
   "source": [
    "칂# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer. We first try it without any attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T07:24:30.378980322Z",
     "start_time": "2023-07-02T07:24:30.368343602Z"
    }
   },
   "outputs": [],
   "source": [
    "intermediate_channels = [100, 100, 100]\n",
    "final_channels = [100, 100, 100]\n",
    "model = HoanMeshClassifier(\n",
    "    in_channels,\n",
    "    intermediate_channels,\n",
    "    in_channels,\n",
    "    negative_slope=0.2,\n",
    "    num_classes=training_dataset.num_classes(),\n",
    "    n_layers=1,\n",
    "    final_layer_embedding_dimension=50,\n",
    ")\n",
    "model = model.to(device)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T15:39:23.697609Z",
     "start_time": "2023-06-28T15:38:23.505866Z"
    }
   },
   "source": [
    "We train the HoanMeshClassifier using low amount of epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T07:53:43.928746382Z",
     "start_time": "2023-07-02T07:24:32.418912723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.4717 Train_acc: 0.0292\n",
      "Epoch: 2 loss: 3.3213 Train_acc: 0.0583\n",
      "Epoch: 3 loss: 2.9902 Train_acc: 0.1083\n",
      "Epoch: 4 loss: 2.5439 Train_acc: 0.2042\n",
      "Epoch: 5 loss: 2.2157 Train_acc: 0.2896\n",
      "Epoch: 6 loss: 1.9642 Train_acc: 0.3521\n",
      "Epoch: 7 loss: 1.7900 Train_acc: 0.3979\n",
      "Epoch: 8 loss: 1.6392 Train_acc: 0.4729\n",
      "Epoch: 9 loss: 1.5216 Train_acc: 0.4854\n",
      "Epoch: 10 loss: 1.4898 Train_acc: 0.4750\n",
      "Epoch: 11 loss: 1.3756 Train_acc: 0.5229\n",
      "Epoch: 12 loss: 1.2955 Train_acc: 0.5625\n",
      "Epoch: 13 loss: 1.2584 Train_acc: 0.5729\n",
      "Epoch: 14 loss: 1.1688 Train_acc: 0.6208\n",
      "Epoch: 15 loss: 1.1594 Train_acc: 0.6021\n",
      "Epoch: 16 loss: 1.1125 Train_acc: 0.6250\n",
      "Epoch: 17 loss: 1.0131 Train_acc: 0.6667\n",
      "Epoch: 18 loss: 0.9931 Train_acc: 0.6583\n",
      "Epoch: 19 loss: 0.9581 Train_acc: 0.6729\n",
      "Epoch: 20 loss: 0.8682 Train_acc: 0.7021\n",
      "Epoch: 21 loss: 0.7745 Train_acc: 0.7312\n",
      "Epoch: 22 loss: 0.7960 Train_acc: 0.7333\n",
      "Epoch: 23 loss: 0.7576 Train_acc: 0.7375\n",
      "Epoch: 24 loss: 0.6472 Train_acc: 0.7604\n",
      "Epoch: 25 loss: 0.6109 Train_acc: 0.7917\n",
      "Test_acc: 0.8250\n",
      "Epoch: 26 loss: 0.5637 Train_acc: 0.7958\n",
      "Epoch: 27 loss: 0.5381 Train_acc: 0.8104\n",
      "Epoch: 28 loss: 0.5538 Train_acc: 0.8187\n",
      "Epoch: 29 loss: 0.4858 Train_acc: 0.8292\n",
      "Epoch: 30 loss: 0.4713 Train_acc: 0.8354\n",
      "Epoch: 31 loss: 0.4263 Train_acc: 0.8667\n",
      "Epoch: 32 loss: 0.3797 Train_acc: 0.8667\n",
      "Epoch: 33 loss: 0.3843 Train_acc: 0.8938\n",
      "Epoch: 34 loss: 0.3815 Train_acc: 0.8562\n",
      "Epoch: 35 loss: 0.3680 Train_acc: 0.8854\n",
      "Epoch: 36 loss: 0.3093 Train_acc: 0.8917\n",
      "Epoch: 37 loss: 0.2977 Train_acc: 0.8854\n",
      "Epoch: 38 loss: 0.2386 Train_acc: 0.9292\n",
      "Epoch: 39 loss: 0.3991 Train_acc: 0.8604\n",
      "Epoch: 40 loss: 0.2730 Train_acc: 0.9042\n",
      "Epoch: 41 loss: 0.2595 Train_acc: 0.9042\n",
      "Epoch: 42 loss: 0.2130 Train_acc: 0.9375\n",
      "Epoch: 43 loss: 0.2228 Train_acc: 0.9229\n",
      "Epoch: 44 loss: 0.2029 Train_acc: 0.9354\n",
      "Epoch: 45 loss: 0.2745 Train_acc: 0.8896\n",
      "Epoch: 46 loss: 0.2322 Train_acc: 0.9083\n",
      "Epoch: 47 loss: 0.1794 Train_acc: 0.9375\n",
      "Epoch: 48 loss: 0.1601 Train_acc: 0.9458\n",
      "Epoch: 49 loss: 0.1845 Train_acc: 0.9354\n",
      "Epoch: 50 loss: 0.2201 Train_acc: 0.9271\n",
      "Test_acc: 0.9812\n",
      "Epoch: 51 loss: 0.1231 Train_acc: 0.9667\n",
      "Epoch: 52 loss: 0.1434 Train_acc: 0.9500\n",
      "Epoch: 53 loss: 0.1317 Train_acc: 0.9604\n",
      "Epoch: 54 loss: 0.1879 Train_acc: 0.9437\n",
      "Epoch: 55 loss: 0.1066 Train_acc: 0.9688\n",
      "Epoch: 56 loss: 0.1650 Train_acc: 0.9417\n",
      "Epoch: 57 loss: 0.1511 Train_acc: 0.9396\n",
      "Epoch: 58 loss: 0.2789 Train_acc: 0.9021\n",
      "Epoch: 59 loss: 0.0502 Train_acc: 0.9917\n",
      "Epoch: 60 loss: 0.0742 Train_acc: 0.9875\n",
      "Epoch: 61 loss: 0.1065 Train_acc: 0.9563\n",
      "Epoch: 62 loss: 0.0901 Train_acc: 0.9708\n",
      "Epoch: 63 loss: 0.1360 Train_acc: 0.9604\n",
      "Epoch: 64 loss: 0.0657 Train_acc: 0.9833\n",
      "Epoch: 65 loss: 0.0813 Train_acc: 0.9812\n",
      "Epoch: 66 loss: 0.1897 Train_acc: 0.9250\n",
      "Epoch: 67 loss: 0.0521 Train_acc: 0.9854\n",
      "Epoch: 68 loss: 0.0351 Train_acc: 0.9958\n",
      "Epoch: 69 loss: 0.2522 Train_acc: 0.9250\n",
      "Epoch: 70 loss: 0.1215 Train_acc: 0.9604\n",
      "Epoch: 71 loss: 0.0408 Train_acc: 0.9896\n",
      "Epoch: 72 loss: 0.0165 Train_acc: 0.9979\n",
      "Epoch: 73 loss: 0.0405 Train_acc: 0.9917\n",
      "Epoch: 74 loss: 0.1642 Train_acc: 0.9479\n",
      "Epoch: 75 loss: 0.2474 Train_acc: 0.9250\n",
      "Test_acc: 0.9854\n",
      "Epoch: 76 loss: 0.0274 Train_acc: 0.9938\n",
      "Epoch: 77 loss: 0.0132 Train_acc: 1.0000\n",
      "Epoch: 78 loss: 0.0102 Train_acc: 1.0000\n",
      "Epoch: 79 loss: 0.1051 Train_acc: 0.9667\n",
      "Epoch: 80 loss: 0.2938 Train_acc: 0.9062\n",
      "Epoch: 81 loss: 0.0503 Train_acc: 0.9833\n",
      "Epoch: 82 loss: 0.0229 Train_acc: 0.9938\n",
      "Epoch: 83 loss: 0.0094 Train_acc: 1.0000\n",
      "Epoch: 84 loss: 0.1186 Train_acc: 0.9646\n",
      "Epoch: 85 loss: 0.2842 Train_acc: 0.9208\n",
      "Epoch: 86 loss: 0.0280 Train_acc: 0.9958\n",
      "Epoch: 87 loss: 0.0129 Train_acc: 0.9979\n",
      "Epoch: 88 loss: 0.0084 Train_acc: 1.0000\n",
      "Epoch: 89 loss: 0.0061 Train_acc: 1.0000\n",
      "Epoch: 90 loss: 0.0684 Train_acc: 0.9729\n",
      "Epoch: 91 loss: 0.1980 Train_acc: 0.9417\n",
      "Epoch: 92 loss: 0.2139 Train_acc: 0.9313\n",
      "Epoch: 93 loss: 0.0136 Train_acc: 0.9979\n",
      "Epoch: 94 loss: 0.0202 Train_acc: 0.9979\n",
      "Epoch: 95 loss: 0.0077 Train_acc: 1.0000\n",
      "Epoch: 96 loss: 0.0062 Train_acc: 1.0000\n",
      "Epoch: 97 loss: 0.0058 Train_acc: 1.0000\n",
      "Epoch: 98 loss: 0.0043 Train_acc: 1.0000\n",
      "Epoch: 99 loss: 0.0049 Train_acc: 1.0000\n",
      "Epoch: 100 loss: 0.3155 Train_acc: 0.9208\n",
      "Test_acc: 0.9958\n",
      "Epoch: 101 loss: 0.0534 Train_acc: 0.9771\n",
      "Epoch: 102 loss: 0.1042 Train_acc: 0.9583\n",
      "Epoch: 103 loss: 0.0795 Train_acc: 0.9688\n",
      "Epoch: 104 loss: 0.0064 Train_acc: 1.0000\n",
      "Epoch: 105 loss: 0.0053 Train_acc: 1.0000\n",
      "Epoch: 106 loss: 0.0038 Train_acc: 1.0000\n",
      "Epoch: 107 loss: 0.0031 Train_acc: 1.0000\n",
      "Epoch: 108 loss: 0.0028 Train_acc: 1.0000\n",
      "Epoch: 109 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 110 loss: 0.0023 Train_acc: 1.0000\n",
      "Epoch: 111 loss: 0.0025 Train_acc: 1.0000\n",
      "Epoch: 112 loss: 0.4732 Train_acc: 0.8958\n",
      "Epoch: 113 loss: 0.0481 Train_acc: 0.9854\n",
      "Epoch: 114 loss: 0.0623 Train_acc: 0.9812\n",
      "Epoch: 115 loss: 0.0078 Train_acc: 1.0000\n",
      "Epoch: 116 loss: 0.0042 Train_acc: 1.0000\n",
      "Epoch: 117 loss: 0.0068 Train_acc: 1.0000\n",
      "Epoch: 118 loss: 0.0043 Train_acc: 1.0000\n",
      "Epoch: 119 loss: 0.0055 Train_acc: 1.0000\n",
      "Epoch: 120 loss: 0.4457 Train_acc: 0.9333\n",
      "Epoch: 121 loss: 0.1518 Train_acc: 0.9479\n",
      "Epoch: 122 loss: 0.0080 Train_acc: 1.0000\n",
      "Epoch: 123 loss: 0.0072 Train_acc: 1.0000\n",
      "Epoch: 124 loss: 0.0050 Train_acc: 1.0000\n",
      "Epoch: 125 loss: 0.0042 Train_acc: 1.0000\n",
      "Test_acc: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 50\u001B[0m\n\u001B[1;32m     48\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (y_hat\u001B[38;5;241m.\u001B[39margmax() \u001B[38;5;241m==\u001B[39m y)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     49\u001B[0m num_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 50\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     52\u001B[0m epoch_loss\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/Documents/PhD/Research/Topological Data Analysis/venvs/TDL-basics/lib/python3.11/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PhD/Research/Topological Data Analysis/venvs/TDL-basics/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_interval = 25\n",
    "num_epochs = 500\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for (\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        adjacency_0,\n",
    "        adjacency_1,\n",
    "        coadjacency_2,\n",
    "        incidence_1,\n",
    "        incidence_2,\n",
    "        y,\n",
    "    ) in training_dataloader:\n",
    "        x_0, x_1, x_2 = (\n",
    "            x_0[0].float().to(device),\n",
    "            x_1[0].float().to(device),\n",
    "            x_2[0].float().to(device),\n",
    "        )\n",
    "\n",
    "        y = y[0].long().to(device)\n",
    "\n",
    "        adjacency_0, adjacency_1, coadjacency_2 = (\n",
    "            adjacency_0[0].float().to(device),\n",
    "            adjacency_1[0].float().to(device),\n",
    "            coadjacency_2[0].float().to(device),\n",
    "        )\n",
    "\n",
    "        incidence_1 = incidence_1[0].float().to(device)\n",
    "        incidence_2 = incidence_2[0].float().to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        y_hat = model.forward(\n",
    "            x_0,\n",
    "            x_1,\n",
    "            x_2,\n",
    "            adjacency_0,\n",
    "            adjacency_1,\n",
    "            coadjacency_2,\n",
    "            incidence_1,\n",
    "            incidence_2,\n",
    "        )\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for (\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                adjacency_0,\n",
    "                adjacency_1,\n",
    "                coadjacency_2,\n",
    "                incidence_1,\n",
    "                incidence_2,\n",
    "                y,\n",
    "            ) in testing_dataloader:\n",
    "\n",
    "                x_0, x_1, x_2 = (\n",
    "                    x_0[0].float().to(device),\n",
    "                    x_1[0].float().to(device),\n",
    "                    x_2[0].float().to(device),\n",
    "                )\n",
    "\n",
    "                y = y[0].long().to(device)\n",
    "\n",
    "                adjacency_0, adjacency_1, coadjacency_2 = (\n",
    "                    adjacency_0[0].float().to(device),\n",
    "                    adjacency_1[0].float().to(device),\n",
    "                    coadjacency_2[0].float().to(device),\n",
    "                )\n",
    "\n",
    "                incidence_1 = incidence_1[0].float().to(device)\n",
    "                incidence_2 = incidence_2[0].float().to(device)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                y_hat = model.forward(\n",
    "                    x_0,\n",
    "                    x_1,\n",
    "                    x_2,\n",
    "                    adjacency_0,\n",
    "                    adjacency_1,\n",
    "                    coadjacency_2,\n",
    "                    incidence_1,\n",
    "                    incidence_2,\n",
    "                )\n",
    "                loss = crit(y_hat, y)\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
