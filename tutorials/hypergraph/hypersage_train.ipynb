{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypersage TNN\n",
    "\n",
    "In this notebook, we will create and train HyperSAGE layer (Arya et al., [2020](https://arxiv.org/abs/2010.04558)) - two-levels message passing strategy for hypergraphs learning. We will use a benchmark dataset, shrec16, a collection of 3D meshes, to train the model to perform classification at the level of the hypergraph.\n",
    "\n",
    "Following the \"awesome-tnns\" [github repo.](https://github.com/awesome-tnns/awesome-tnns/blob/main/Hypergraphs.md)\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow z}^{(0 \\rightarrow 1)} = (B_1)^T_{zy} \\cdot w_y \\cdot (h_y^{(0)})^p$ \n",
    "\n",
    "游린 $\\quad m_z^{(0 \\rightarrow 1)}  = \\left(\\frac{1}{\\vert \\mathcal{B}(z)\\vert}\\sum_{y \\in \\mathcal{B}(z)} m_{y \\rightarrow z}^{(0 \\rightarrow 1)}\\right)^{\\frac{1}{p}}$\n",
    "\n",
    "游린 $\\quad m_{z \\rightarrow x}^{(1 \\rightarrow 0)} =  (B_1)_{xz} \\cdot w_z  \\cdot (m_z^{(0 \\rightarrow 1)})^p$\n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 0)}  = \\left(\\frac{1}{\\vert \\mathcal{C}(x) \\vert}\\sum_{z \\in \\mathcal{C}(x)} m_{z \\rightarrow x}^{(1 \\rightarrow 0)}\\right)^{\\frac{1}{p}}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}  = m_x^{(1 \\rightarrow 0)}$ \n",
    "\n",
    "游릱 $\\quad h_x^{t+1, (0)} = \\sigma \\left(\\frac{m_x^{(0)} + h_x^{t,(0)}}{\\lvert m_x^{(0)} + h_x^{t,(0)}\\rvert} \\cdot \\Theta^t\\right)$ \n",
    "\n",
    "### Additional theoretical clarifications\n",
    "\n",
    "Arya et al propose to interpret the propagation of information in a given hypergraph as a two-level aggregation problem, where the neighborhood of any node is divided into intra-edge neighbors and inter-edge neighbors. Given a hypergraph $H=(\\mathcal{V}, \\mathcal{E})$, let $\\textbf{X}$ denote the feature matrix, such that $\\textbf{x}_{i} \\in \\textbf{X}$ is the feature set for node $\\textbf{v}_{i} \\in \\textbf{V}$ . For two-level aggregation, \n",
    "let $\\mathcal{F}_{1}(췅)$ and $\\mathcal{F}_{2}(췅)$ denote the intra-edge and inter-edge aggregation functions, respectively. Message passing at node vi for aggregation of information at the $\\mathcal{l}^{th}$ layer can then be stated as\n",
    "\n",
    "$ \\mathcal{x}_{i,l}^{(e)} \\leftarrow \\mathcal{F}_{1}(\\{ \\mathcal{x}_{j,l-1} | \\mathcal{v}_{j} \\in \\mathcal{N}( \\mathcal{v}_{i},\n",
    "\\textbf{e},\\alpha) \\}), $\n",
    "\n",
    "$ \\mathcal{x}_{i,l} \\leftarrow \\mathcal{x}_{i,l-1} + \\mathcal{F}_{2}(\\{ \\mathcal{x}_{i,l}^{(e)} | \\mathcal{v}_{i} \\in {E}( \\mathcal{v}_{i}) \\}), $\n",
    "\n",
    "where, $ \\mathcal{x}_{i,l}^{(e)}$  refers to the aggregated feature set at $\\mathcal{v}_{i}$ obtained with intra-edge aggregation for edge $\\textbf{e}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import toponetx.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from topomodelx.nn.hypergraph.hypersage import HyperSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th simplicial complex has 252 nodes with features of dimension 6.\n",
      "The 6th simplicial complex has 750 edges with features of dimension 10.\n",
      "The 6th simplicial complex has 500 faces with features of dimension 7.\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_0s[i_complex].shape[0]} nodes with features of dimension {x_0s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_1s[i_complex].shape[0]} edges with features of dimension {x_1s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_2s[i_complex].shape[0]} faces with features of dimension {x_2s[i_complex].shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "Once we have recorded the incidence matrix (note that all incidence amtrices in the hypergraph domain must be unsigned), we lift each simplicial complex into a hypergraph. The pairwise edges will become pairwise hyperedges, and faces in the simplciial complex will become 3-wise hyperedges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_list = []\n",
    "incidence_1_list = []\n",
    "for simplex in simplexes:\n",
    "    incidence_1 = simplex.incidence_matrix(rank=1, signed=False)\n",
    "    hg = simplex.to_hypergraph()\n",
    "    hg_list.append(hg)\n",
    "\n",
    "# Extract hypergraphs incident matrices from collected hypergraphs\n",
    "for hg in hg_list:\n",
    "    incidence_1 = hg.incidence_matrix()\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_1_list.append(incidence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th hypergraph has an incidence matrix of shape torch.Size([252, 1250]).\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th hypergraph has an incidence matrix of shape {incidence_1_list[i_complex].shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x_0s[0].shape[1]\n",
    "out_channels = 10\n",
    "p = 2\n",
    "initialization = \"xavier_uniform\"\n",
    "n_layers = 2\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = HyperSAGE(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=n_layers,\n",
    "    device=\"cpu\",\n",
    "    initialization=initialization,\n",
    ")\n",
    "\n",
    "# Optimizer and loss\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 275.3827\n",
      "Test_loss: 529.0000\n",
      "Epoch: 2 loss: 274.6125\n",
      "Test_loss: 529.0000\n",
      "Epoch: 3 loss: 274.6125\n",
      "Test_loss: 529.0000\n",
      "Epoch: 4 loss: 274.6125\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, incidence_1, y in zip(x_0_train, incidence_1_train, y_train):\n",
    "        x_0 = torch.tensor(x_0)\n",
    "        x_0, incidence_1, y = (\n",
    "            x_0.float().to(device),\n",
    "            incidence_1.float().to(device),\n",
    "            torch.tensor(y, dtype=torch.float).to(device),\n",
    "        )\n",
    "        opt.zero_grad()\n",
    "        # Extract edge_index from sparse incidence matrix\n",
    "        # edge_index, _ = to_edge_index(incidence_1)\n",
    "        y_hat = model(x_0, incidence_1)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x_0, incidence_1, y in zip(x_0_test, incidence_1_test, y_test):\n",
    "                x_0 = torch.tensor(x_0)\n",
    "                x_0, incidence_1, y = (\n",
    "                    x_0.float().to(device),\n",
    "                    incidence_1.float().to(device),\n",
    "                    torch.tensor(y, dtype=torch.float).to(device),\n",
    "                )\n",
    "                y_hat = model(x_0, incidence_1)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
