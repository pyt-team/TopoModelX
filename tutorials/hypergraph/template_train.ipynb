{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Neural Network\n",
    "\n",
    "In this notebook, we will create and train a two-step message passing network in the hypergraph domain. We will use a benchmark dataset, MUTAG (from the TUDataset), to train the model to perform binary classification at the level of the h. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from toponetx import SimplicialComplex\n",
    "from topomodelx.nn.hypergraph.template_layer import TemplateLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, MUTAG, a benchmark dataset for graph classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the binary label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\", use_edge_attr=True)\n",
    "dataset = dataset[:20]\n",
    "hg_list = []\n",
    "x_1_list = []\n",
    "y_list = []\n",
    "for graph in dataset:\n",
    "    hg = SimplicialComplex(to_networkx(graph)).to_hypergraph()\n",
    "    hg_list.append(hg)\n",
    "    x_1 = torch.chunk(graph.edge_attr, 2, dim=0)[1]\n",
    "    x_1_list.append(x_1)\n",
    "    y_list.append(int(graph.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:54.320559691Z",
     "start_time": "2023-06-01T16:14:54.310717495Z"
    }
   },
   "outputs": [],
   "source": [
    "incidence_1_list = []\n",
    "for hg in hg_list:\n",
    "    incidence_1 = hg.incidence_matrix()\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_1_list.append(incidence_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the TemplateLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:55.343005145Z",
     "start_time": "2023-06-01T16:14:55.339481459Z"
    }
   },
   "outputs": [],
   "source": [
    "channels_edge = x_1_list[0].shape[1]\n",
    "channels_node = dataset[0].x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemplateNN(torch.nn.Module):\n",
    "    \"\"\"Neural network implementation of Template for hypergraph classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels_edge : int\n",
    "        Dimension of edge features\n",
    "    channels_node : int\n",
    "        Dimension of node features\n",
    "    n_layer : 2\n",
    "        Amount of message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels_edge, channels_node, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                TemplateLayer(\n",
    "                    in_channels=channels_edge,\n",
    "                    intermediate_channels=channels_node,\n",
    "                    out_channels=channels_edge,\n",
    "                )\n",
    "            )\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.linear = torch.nn.Linear(channels_edge, 1)\n",
    "\n",
    "    def forward(self, x_1, incidence_1):\n",
    "        \"\"\"Forward computation through layers, then linear layer, then global max pooling.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x_1 : tensor\n",
    "            shape = [n_edges, channels_edge]\n",
    "            Edge features.\n",
    "\n",
    "        incidence_1 : tensor\n",
    "            shape = [n_nodes, n_edges]\n",
    "            Boundary matrix of rank 1.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [1]\n",
    "            Label assigned to whole complex.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_1 = layer(x_1, incidence_1)\n",
    "        pooled_x = torch.max(x_1, dim=0)[0]\n",
    "        return torch.sigmoid(self.linear(pooled_x))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TemplateNN(channels_edge, channels_node, n_layers=2)\n",
    "model = model.to(device)\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:59.046068930Z",
     "start_time": "2023-06-01T16:14:59.037648626Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7279 Train_acc: 0.3125\n",
      "Epoch: 2 loss: 0.6796 Train_acc: 0.6250\n",
      "Test_acc: 0.5000\n",
      "Epoch: 3 loss: 0.6917 Train_acc: 0.6250\n",
      "Epoch: 4 loss: 0.6793 Train_acc: 0.6250\n",
      "Test_acc: 0.5000\n",
      "Epoch: 5 loss: 0.6780 Train_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "threshold_probability_positive_class = 0.5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_1, incidence_1, y in zip(x_1_train, incidence_1_train, y_train):\n",
    "        x_1, incidence_1, y = x_1.float().to(device), incidence_1.float().to(device), torch.tensor(y, dtype=torch.float).to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_1, incidence_1)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += ((y_hat > threshold_probability_positive_class) == y.bool()).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_1, incidence_1, y in zip(x_1_test, incidence_1_test, y_test):\n",
    "                x_1, incidence_1, y = x_1.float().to(device), incidence_1.float().to(device), torch.tensor(y, dtype=torch.float).to(device)\n",
    "                y_hat = model(x_1, incidence_1)\n",
    "                correct += ((y_hat > threshold_probability_positive_class) == y.bool()).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
