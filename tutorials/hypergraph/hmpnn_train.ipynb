{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(\".\", \"cora\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the incidence matrix ($B_1$) which is of shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"incidence_1\"] = torch.sparse_coo_tensor(\n",
    "    dataset[\"edge_index\"], torch.ones(dataset[\"edge_index\"].shape[1]), dtype=torch.long\n",
    ")\n",
    "dataset = dataset.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(41)\n",
    "\n",
    "in_features = 1433\n",
    "hidden_features = 8\n",
    "num_classes = 7\n",
    "n_layers = 2\n",
    "\n",
    "model = HMPNN(in_features, (256, hidden_features), num_classes, n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train loss: 2.1079 train acc: 0.14 val loss: 2.1436 val acc: 0.16\n",
      "Epoch: 2 train loss: 2.0234 train acc: 0.15 val loss: 2.1016 val acc: 0.16\n",
      "Epoch: 3 train loss: 1.9800 train acc: 0.15 val loss: 2.0681 val acc: 0.16\n",
      "Epoch: 4 train loss: 1.9504 train acc: 0.18 val loss: 2.0389 val acc: 0.16\n",
      "Epoch: 5 train loss: 1.9194 train acc: 0.21 val loss: 2.0137 val acc: 0.16\n",
      "Epoch: 6 train loss: 1.9241 train acc: 0.19 val loss: 1.9917 val acc: 0.16\n",
      "Epoch: 7 train loss: 1.8917 train acc: 0.19 val loss: 1.9729 val acc: 0.16\n",
      "Epoch: 8 train loss: 1.8710 train acc: 0.23 val loss: 1.9556 val acc: 0.16\n",
      "Epoch: 9 train loss: 1.8574 train acc: 0.29 val loss: 1.9402 val acc: 0.17\n",
      "Epoch: 10 train loss: 1.8646 train acc: 0.29 val loss: 1.9265 val acc: 0.17\n",
      "Epoch: 11 train loss: 1.8540 train acc: 0.33 val loss: 1.9137 val acc: 0.19\n",
      "Epoch: 12 train loss: 1.8430 train acc: 0.36 val loss: 1.9012 val acc: 0.22\n",
      "Epoch: 13 train loss: 1.8336 train acc: 0.38 val loss: 1.8886 val acc: 0.25\n",
      "Epoch: 14 train loss: 1.8405 train acc: 0.34 val loss: 1.8775 val acc: 0.30\n",
      "Epoch: 15 train loss: 1.8264 train acc: 0.34 val loss: 1.8668 val acc: 0.34\n",
      "Epoch: 16 train loss: 1.8065 train acc: 0.38 val loss: 1.8562 val acc: 0.37\n",
      "Epoch: 17 train loss: 1.8158 train acc: 0.37 val loss: 1.8456 val acc: 0.38\n",
      "Epoch: 18 train loss: 1.7957 train acc: 0.44 val loss: 1.8346 val acc: 0.39\n",
      "Epoch: 19 train loss: 1.8028 train acc: 0.40 val loss: 1.8249 val acc: 0.41\n",
      "Epoch: 20 train loss: 1.7882 train acc: 0.37 val loss: 1.8156 val acc: 0.42\n",
      "Epoch: 21 train loss: 1.7912 train acc: 0.36 val loss: 1.8070 val acc: 0.42\n",
      "Epoch: 22 train loss: 1.7610 train acc: 0.46 val loss: 1.7985 val acc: 0.41\n",
      "Epoch: 23 train loss: 1.7617 train acc: 0.47 val loss: 1.7903 val acc: 0.42\n",
      "Epoch: 24 train loss: 1.7596 train acc: 0.41 val loss: 1.7827 val acc: 0.41\n",
      "Epoch: 25 train loss: 1.7391 train acc: 0.38 val loss: 1.7737 val acc: 0.42\n",
      "Epoch: 26 train loss: 1.7316 train acc: 0.44 val loss: 1.7650 val acc: 0.42\n",
      "Epoch: 27 train loss: 1.7364 train acc: 0.42 val loss: 1.7559 val acc: 0.43\n",
      "Epoch: 28 train loss: 1.7184 train acc: 0.48 val loss: 1.7454 val acc: 0.44\n",
      "Epoch: 29 train loss: 1.7086 train acc: 0.44 val loss: 1.7362 val acc: 0.45\n",
      "Epoch: 30 train loss: 1.6815 train acc: 0.46 val loss: 1.7276 val acc: 0.46\n",
      "Epoch: 31 train loss: 1.6673 train acc: 0.50 val loss: 1.7178 val acc: 0.46\n",
      "Epoch: 32 train loss: 1.6846 train acc: 0.51 val loss: 1.7080 val acc: 0.45\n",
      "Epoch: 33 train loss: 1.6483 train acc: 0.51 val loss: 1.7007 val acc: 0.47\n",
      "Epoch: 34 train loss: 1.6435 train acc: 0.54 val loss: 1.6982 val acc: 0.44\n",
      "Epoch: 35 train loss: 1.6354 train acc: 0.54 val loss: 1.6998 val acc: 0.43\n",
      "Epoch: 36 train loss: 1.6336 train acc: 0.54 val loss: 1.6982 val acc: 0.43\n",
      "Epoch: 37 train loss: 1.5937 train acc: 0.60 val loss: 1.6972 val acc: 0.43\n",
      "Epoch: 38 train loss: 1.5885 train acc: 0.56 val loss: 1.6962 val acc: 0.42\n",
      "Epoch: 39 train loss: 1.5970 train acc: 0.55 val loss: 1.6861 val acc: 0.42\n",
      "Epoch: 40 train loss: 1.5597 train acc: 0.52 val loss: 1.6673 val acc: 0.44\n",
      "Epoch: 41 train loss: 1.5444 train acc: 0.64 val loss: 1.6510 val acc: 0.45\n",
      "Epoch: 42 train loss: 1.5500 train acc: 0.60 val loss: 1.6320 val acc: 0.46\n",
      "Epoch: 43 train loss: 1.5396 train acc: 0.54 val loss: 1.6160 val acc: 0.48\n",
      "Epoch: 44 train loss: 1.5096 train acc: 0.59 val loss: 1.6032 val acc: 0.48\n",
      "Epoch: 45 train loss: 1.4991 train acc: 0.59 val loss: 1.5924 val acc: 0.47\n",
      "Epoch: 46 train loss: 1.5020 train acc: 0.58 val loss: 1.5830 val acc: 0.49\n",
      "Epoch: 47 train loss: 1.4710 train acc: 0.64 val loss: 1.5741 val acc: 0.49\n",
      "Epoch: 48 train loss: 1.4607 train acc: 0.67 val loss: 1.5691 val acc: 0.48\n",
      "Epoch: 49 train loss: 1.4339 train acc: 0.62 val loss: 1.5616 val acc: 0.48\n",
      "Epoch: 50 train loss: 1.4425 train acc: 0.66 val loss: 1.5615 val acc: 0.49\n",
      "Epoch: 51 train loss: 1.4206 train acc: 0.66 val loss: 1.5477 val acc: 0.49\n",
      "Epoch: 52 train loss: 1.4142 train acc: 0.64 val loss: 1.5281 val acc: 0.50\n",
      "Epoch: 53 train loss: 1.4085 train acc: 0.66 val loss: 1.5042 val acc: 0.50\n",
      "Epoch: 54 train loss: 1.4017 train acc: 0.62 val loss: 1.4884 val acc: 0.50\n",
      "Epoch: 55 train loss: 1.3852 train acc: 0.65 val loss: 1.4838 val acc: 0.50\n",
      "Epoch: 56 train loss: 1.3908 train acc: 0.61 val loss: 1.4847 val acc: 0.50\n",
      "Epoch: 57 train loss: 1.3434 train acc: 0.71 val loss: 1.4878 val acc: 0.49\n",
      "Epoch: 58 train loss: 1.3259 train acc: 0.69 val loss: 1.4851 val acc: 0.50\n",
      "Epoch: 59 train loss: 1.3378 train acc: 0.66 val loss: 1.4907 val acc: 0.50\n",
      "Epoch: 60 train loss: 1.2930 train acc: 0.69 val loss: 1.4966 val acc: 0.50\n",
      "Epoch: 61 train loss: 1.3125 train acc: 0.69 val loss: 1.5017 val acc: 0.50\n",
      "Epoch: 62 train loss: 1.3093 train acc: 0.65 val loss: 1.4984 val acc: 0.50\n",
      "Epoch: 63 train loss: 1.2770 train acc: 0.67 val loss: 1.4915 val acc: 0.51\n",
      "Epoch: 64 train loss: 1.2753 train acc: 0.65 val loss: 1.4854 val acc: 0.50\n",
      "Epoch: 65 train loss: 1.2741 train acc: 0.69 val loss: 1.4831 val acc: 0.50\n",
      "Epoch: 66 train loss: 1.2784 train acc: 0.67 val loss: 1.4728 val acc: 0.50\n",
      "Epoch: 67 train loss: 1.2445 train acc: 0.67 val loss: 1.4712 val acc: 0.50\n",
      "Epoch: 68 train loss: 1.2020 train acc: 0.69 val loss: 1.4743 val acc: 0.48\n",
      "Epoch: 69 train loss: 1.1976 train acc: 0.69 val loss: 1.4705 val acc: 0.48\n",
      "Epoch: 70 train loss: 1.1895 train acc: 0.71 val loss: 1.4638 val acc: 0.48\n",
      "Epoch: 71 train loss: 1.2016 train acc: 0.71 val loss: 1.4672 val acc: 0.48\n",
      "Epoch: 72 train loss: 1.1565 train acc: 0.74 val loss: 1.4450 val acc: 0.50\n",
      "Epoch: 73 train loss: 1.1744 train acc: 0.69 val loss: 1.4279 val acc: 0.50\n",
      "Epoch: 74 train loss: 1.1863 train acc: 0.72 val loss: 1.4286 val acc: 0.49\n",
      "Epoch: 75 train loss: 1.1765 train acc: 0.72 val loss: 1.4333 val acc: 0.49\n",
      "Epoch: 76 train loss: 1.1773 train acc: 0.66 val loss: 1.4499 val acc: 0.49\n",
      "Epoch: 77 train loss: 1.1232 train acc: 0.74 val loss: 1.4701 val acc: 0.50\n",
      "Epoch: 78 train loss: 1.0667 train acc: 0.76 val loss: 1.4692 val acc: 0.49\n",
      "Epoch: 79 train loss: 1.0716 train acc: 0.76 val loss: 1.4593 val acc: 0.50\n",
      "Epoch: 80 train loss: 1.0567 train acc: 0.77 val loss: 1.4450 val acc: 0.50\n",
      "Epoch: 81 train loss: 1.0626 train acc: 0.72 val loss: 1.4309 val acc: 0.50\n",
      "Epoch: 82 train loss: 1.0453 train acc: 0.73 val loss: 1.4244 val acc: 0.50\n",
      "Epoch: 83 train loss: 1.0300 train acc: 0.78 val loss: 1.4135 val acc: 0.50\n",
      "Epoch: 84 train loss: 1.0505 train acc: 0.74 val loss: 1.4152 val acc: 0.51\n",
      "Epoch: 85 train loss: 1.0212 train acc: 0.74 val loss: 1.4340 val acc: 0.52\n",
      "Epoch: 86 train loss: 1.0401 train acc: 0.76 val loss: 1.4580 val acc: 0.50\n",
      "Epoch: 87 train loss: 0.9888 train acc: 0.81 val loss: 1.4668 val acc: 0.50\n",
      "Epoch: 88 train loss: 1.0185 train acc: 0.74 val loss: 1.4771 val acc: 0.50\n",
      "Epoch: 89 train loss: 0.9739 train acc: 0.78 val loss: 1.4710 val acc: 0.49\n",
      "Epoch: 90 train loss: 0.9517 train acc: 0.83 val loss: 1.4661 val acc: 0.50\n",
      "Epoch: 91 train loss: 0.9341 train acc: 0.79 val loss: 1.4519 val acc: 0.50\n",
      "Epoch: 92 train loss: 0.9642 train acc: 0.80 val loss: 1.4457 val acc: 0.50\n",
      "Epoch: 93 train loss: 0.9191 train acc: 0.77 val loss: 1.4126 val acc: 0.51\n",
      "Epoch: 94 train loss: 0.8985 train acc: 0.79 val loss: 1.4001 val acc: 0.51\n",
      "Epoch: 95 train loss: 0.8884 train acc: 0.84 val loss: 1.3864 val acc: 0.51\n",
      "Epoch: 96 train loss: 0.9020 train acc: 0.82 val loss: 1.3875 val acc: 0.52\n",
      "Epoch: 97 train loss: 0.8673 train acc: 0.84 val loss: 1.3893 val acc: 0.53\n",
      "Epoch: 98 train loss: 0.9479 train acc: 0.78 val loss: 1.3814 val acc: 0.52\n",
      "Epoch: 99 train loss: 0.8873 train acc: 0.81 val loss: 1.3855 val acc: 0.51\n",
      "Epoch: 100 train loss: 0.8565 train acc: 0.86 val loss: 1.3936 val acc: 0.52\n"
     ]
    }
   ],
   "source": [
    "train_y_true = dataset[\"y\"][dataset[\"train_mask\"]]\n",
    "val_y_true = dataset[\"y\"][dataset[\"val_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    loss = loss_fn(y_pred_logits[dataset[\"train_mask\"]], train_y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(train_y_true, y_pred[dataset[\"train_mask\"]])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    val_loss = loss_fn(y_pred_logits[dataset[\"val_mask\"]], val_y_true).item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    val_acc = accuracy_score(val_y_true, y_pred[dataset[\"val_mask\"]])\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "        f\"val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3152 test acc: 0.55 \n"
     ]
    }
   ],
   "source": [
    "test_y_true = dataset[\"y\"][dataset[\"test_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "test_loss = loss_fn(y_pred_logits[dataset[\"test_mask\"]], test_y_true).item()\n",
    "y_pred = y_pred_logits.argmax(dim=-1)\n",
    "test_acc = accuracy_score(test_y_true, y_pred[dataset[\"test_mask\"]])\n",
    "print(f\"Test loss: {test_loss:.4f} test acc: {test_acc:.2f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
