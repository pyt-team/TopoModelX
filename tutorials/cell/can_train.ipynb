{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Cell Attention Network (CAN)\n",
    "\n",
    "We create and train a  Cell Attention Network (CAN) originally proposed in [Giusti et al. Cell Attention Networks (2022)](https://arxiv.org/abs/2209.08179). The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.\n",
    "\n",
    "### Abstract:\n",
    "\n",
    "Since their introduction, graph attention networks achieved outstanding results in graph representation learning tasks. However, these networks consider only pairwise relationships among nodes and then they are not able to fully exploit higher-order interactions present in many real world data-sets. In this paper, we introduce Cell Attention Networks (CANs), a neural architecture operating on data defined over the vertices of a graph, representing the graph as the 1-skeleton of a cell complex introduced to capture higher order interactions. In particular, we exploit the lower and upper neighborhoods, as encoded in the cell complex, to design two independent masked self-attention mechanisms, thus generalizing the conventional graph attention strategy. The approach used in CANs is hierarchical and it incorporates the following steps: i) a lifting algorithm that learns edge features from node features; ii) a cell attention mechanism to find the optimal combination of edge features over both lower and upper neighbors; iii) a hierarchical edge pooling mechanism to extract a compact meaningful set of features. \n",
    "\n",
    "<center>\n",
    "        <a href=\"https://ibb.co/1JHND1j\"><img src=\"https://i.ibb.co/YTvSzmw/98d25e90-4216-4d4d-975c-2baa3e388f1c.jpg\" alt=\"98d25e90-4216-4d4d-975c-2baa3e388f1c\"></a>\n",
    "        <figcaption></figcaption>\n",
    "</center>\n",
    "\n",
    "**Remark.** The notation we use is defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)and [Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)](https://arxiv.org/pdf/2206.00606.pdf). Custom symbols are introduced along the notebook, when necessary.\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The CAN layer, in the original paper, takes rank-$0$ signals as input  and gives rank-$0$ signals as output (in general, it could take rank-$r$ signals as input  and give rank-$r$ signals as output). The involved neighborhoods are: $N = \\{\\mathcal N_1, \\mathcal N_2\\} = \\{A_{\\uparrow,r+1}, A_{\\downarrow, r+1}\\}$.\n",
    "\n",
    "A CAN layer is made by the following 3 message passing stages:\n",
    "\n",
    "1) Attentional Lift (to compute $r+1$-signals from $r$-signals):\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} \\quad m_{(y,z) \\rightarrow x} &=& \\alpha(h_y^0,h_z^0) = \\\\\n",
    "        &&=&\\Theta \\cdot (h_y^0||h_z^0)\\\\\n",
    "&游릱\\textrm{ Update.} \\quad h_x^1 &=& \\phi(h_x^0,  m_{(y,z) \\rightarrow x})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha$ is a learnable function parameterized by $\\Theta$ $\\in$ $\\mathbb R^{2F_0 \\times H}$. In the case of node signals as input, $F_0$ is the number of nodes' features and $H$ is the number of heads as defined in the original paper.\n",
    "- $||$ is the concatenation operator.\n",
    "- $\\phi$ is a learnable function that updates the features of a cell.\n",
    "\n",
    "2) ($\\times L$) Attentional message passing at level $r+1$. The general equation is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{h}_x^{t+1} =  \\phi^t \\Bigg ( \\textbf{h}_x^{t}, \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}\\bigoplus_{y \\in \\mathcal{N}_k(x)}  \\alpha_k(h_x^t,h_y^t)\\Bigg ) \n",
    "\\end{align*}\n",
    "\n",
    "In detail:\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} &\\quad m_{(y \\rightarrow x),k} =&\n",
    "\\alpha_k(h_x^t,h_y^t) =\n",
    "a_k(h_x^{t}, h_y^{t}) \\cdot \\psi_k^t(h_x^{t})\\quad \\forall \\mathcal N_k \\in \\mathcal{N}\\\\\n",
    "\\\\\n",
    "&游릲 \\textrm{ Within-Neighborhood Aggregation.} &\\quad m_{x,k}               =& \\bigoplus_{y \\in \\mathcal{N}_k(x)}  m_{(y \\rightarrow x),k}\\\\\n",
    "\\\\\n",
    "&游릴 \\textrm{ Between-Neighborhood Aggregation.} &\\quad m_{x} =& \\bigotimes_{\\mathcal{N}_k\\in\\mathcal N}m_{x,k}\\\\\n",
    "\\\\\n",
    "&游릱 \\textrm{ Update.}&\\quad h_x^{t+1}                =& \\phi^{t}(h_x^t, m_{x})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\psi_k^t$ is a learnable function that computes the importance of a $r+1$-cell.\n",
    "- $a_k^t: \\mathbb R^{F^l}\\times \\mathbb R^{F^l} \\to \\mathbb R$  are learnable functions responsible for evaluating the reciprocal importance of two $r+1$-cells that share a common $(r)$-cell or are parts of the same $(r+2)$-cell.\n",
    "- $\\phi^t$ is a learnable function that updates the features of a cell.\n",
    "\n",
    "3) Attentional Pooling (performed after each message passing round of 2)):\n",
    "\n",
    "\\begin{align*}\n",
    "&游린\\textrm{ Message.} \\quad m_{x} &=& \\gamma^t(h_x^t) =\\\\\n",
    "                &&=& \\tau^t (a^t\\cdot h_x^t)\\\\\n",
    "&游릱\\textrm{ Update.} \\quad h_x^{t+1} &=&  m_{x}h_x^t, \\forall x\\in \\mathcal C_r^{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\gamma^t$ is a learnable function that computes the attention coefficients (self-scores) as defined in the original paper.\n",
    "- $\\tau^t$ is a non-linear function, $a$ are learnable parameters.\n",
    "- $C^{t+1}_r$ is the set of rank-$r$ cells of the coarse cell complex, defined keeping the rank-$r$ cells corresponding to the top-K self-scores $\\gamma^t(h_x^t)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on [`MUTAG` from the TUDataset](https://paperswithcode.com/dataset/mutag). This dataset contains:\n",
    "- 188 samples of chemical compounds represented as graphs,\n",
    "- with 7 discrete node features.\n",
    "\n",
    "The task is to predict the mutagenicity of each compound on Salmonella Typhimurium. We use a [\"GAT-like\" attention function](https://arxiv.org/abs/1710.10903) following the approach from [SAN](https://arxiv.org/abs/2203.07485). We implemented also a  [\"GATv2-like\" attention function](https://arxiv.org/abs/2105.14491).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:06:36.009880829Z",
     "start_time": "2023-05-31T09:06:34.285257706Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toponetx.classes.cell_complex import CellComplex\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from topomodelx.nn.cell.can import CAN\n",
    "from topomodelx.utils.sparse import from_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:53.006542411Z",
     "start_time": "2023-05-31T09:13:52.963074076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "We import a subset of MUTAG, a benchmark dataset for graph classification. \n",
    "\n",
    "We then lift each graph into our topological domain of choice, here: a cell complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0` and `x_1` on the nodes (0-cells) and edges (1-cells) for each complex: these will be the model's inputs,\n",
    "- a binary classification label `y` associated to the cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.279147916Z",
     "start_time": "2023-05-31T09:13:55.269057585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 36], x=[16, 7], edge_attr=[36, 4], y=[1])\n",
      "Features on nodes for the 0th cell complex: torch.Size([17, 7]).\n",
      "Features on edges for the 0th cell complex: torch.Size([38, 4]).\n",
      "Label of 0th cell complex: 1.\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(\n",
    "    root=\"/tmp/MUTAG\", name=\"MUTAG\", use_edge_attr=True, use_node_attr=True\n",
    ")\n",
    "dataset = dataset\n",
    "cc_list = []\n",
    "x_0_list = []\n",
    "x_1_list = []\n",
    "y_list = []\n",
    "for graph in dataset:\n",
    "    cell_complex = CellComplex(to_networkx(graph))\n",
    "    cc_list.append(cell_complex)\n",
    "    x_0_list.append(graph.x)\n",
    "    x_1_list.append(graph.edge_attr)\n",
    "    y_list.append(int(graph.y))\n",
    "else:\n",
    "    print(graph)\n",
    "\n",
    "i_cc = 0\n",
    "print(f\"Features on nodes for the {i_cc}th cell complex: {x_0_list[i_cc].shape}.\")\n",
    "print(f\"Features on edges for the {i_cc}th cell complex: {x_1_list[i_cc].shape}.\")\n",
    "print(f\"Label of {i_cc}th cell complex: {y_list[i_cc]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Implementing CAN will require to perform message passing along neighborhood structures of the cell complexes.\n",
    "\n",
    "Thus, now we retrieve these neighborhood structures (i.e. their representative matrices) that we will use to send messages. \n",
    "\n",
    "We need the matrices $A_{\\downarrow, 1}$ and $A_{\\uparrow, 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.832585216Z",
     "start_time": "2023-05-31T09:13:55.815448708Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_neighborhood_list = []\n",
    "upper_neighborhood_list = []\n",
    "adjacency_0_list = []\n",
    "\n",
    "for cell_complex in cc_list:\n",
    "    adjacency_0 = cell_complex.adjacency_matrix(rank=0)\n",
    "    adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "    adjacency_0_list.append(adjacency_0)\n",
    "\n",
    "    lower_neighborhood_t = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    lower_neighborhood_t = from_sparse(lower_neighborhood_t)\n",
    "    lower_neighborhood_list.append(lower_neighborhood_t)\n",
    "\n",
    "    try:\n",
    "        upper_neighborhood_t = cell_complex.up_laplacian_matrix(rank=1)\n",
    "        upper_neighborhood_t = from_sparse(upper_neighborhood_t)\n",
    "    except:\n",
    "        upper_neighborhood_t = np.zeros(\n",
    "            (lower_neighborhood_t.shape[0], lower_neighborhood_t.shape[0])\n",
    "        )\n",
    "        upper_neighborhood_t = torch.from_numpy(upper_neighborhood_t).to_sparse()\n",
    "\n",
    "    upper_neighborhood_list.append(upper_neighborhood_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the CANLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:56.672913561Z",
     "start_time": "2023-05-31T09:13:56.667986426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of input features on nodes, edges and faces are: 7, 4 and 5.\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0_list[0].shape[-1]\n",
    "in_channels_1 = x_1_list[0].shape[-1]\n",
    "in_channels_2 = 5\n",
    "print(\n",
    "    f\"The dimension of input features on nodes, edges and faces are: {in_channels_0}, {in_channels_1} and {in_channels_2}.\"\n",
    ")\n",
    "model = CAN(\n",
    "    in_channels_0,\n",
    "    in_channels_1,\n",
    "    32,\n",
    "    dropout=0.5,\n",
    "    heads=2,\n",
    "    num_classes=2,\n",
    "    n_layers=2,\n",
    "    att_lift=True,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer. We first try it without any attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:40.411845803Z",
     "start_time": "2023-05-31T09:19:40.408861921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAN(\n",
       "  (lift_layer): MultiHeadLiftLayer(\n",
       "    (lifts): LiftLayer()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=11, out_features=64, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (1): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (2): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lin_0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (lin_1): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:41.150933630Z",
     "start_time": "2023-05-31T09:19:41.146986990Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "x_0_train, x_0_test = train_test_split(x_0_list, test_size=test_size, shuffle=False)\n",
    "lower_neighborhood_train, lower_neighborhood_test = train_test_split(\n",
    "    lower_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "upper_neighborhood_train, upper_neighborhood_test = train_test_split(\n",
    "    upper_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "adjacency_0_train, adjacency_0_test = train_test_split(\n",
    "    adjacency_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:42.918836083Z",
     "start_time": "2023-05-31T09:19:42.114801039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.6251 Train_acc: 0.6794\n",
      "Test_acc: 0.5965\n",
      "Epoch: 2 loss: 0.6007 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 3 loss: 0.6094 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 4 loss: 0.5881 Train_acc: 0.6947\n",
      "Test_acc: 0.5965\n",
      "Epoch: 5 loss: 0.5885 Train_acc: 0.7099\n",
      "Test_acc: 0.6316\n",
      "Epoch: 6 loss: 0.5685 Train_acc: 0.7252\n",
      "Test_acc: 0.6316\n",
      "Epoch: 7 loss: 0.5792 Train_acc: 0.7176\n",
      "Test_acc: 0.6491\n",
      "Epoch: 8 loss: 0.5614 Train_acc: 0.7405\n",
      "Test_acc: 0.7368\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 8\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        adjacency_0_train,\n",
    "        lower_neighborhood_train,\n",
    "        upper_neighborhood_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0 = x_0.float().to(device)\n",
    "        x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "        adjacency = adjacency.float().to(device)\n",
    "        lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "            device\n",
    "        ), upper_neighborhood.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                adjacency_0_test,\n",
    "                lower_neighborhood_test,\n",
    "                upper_neighborhood_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = x_0.float().to(device)\n",
    "                x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(\n",
    "                    device\n",
    "                )\n",
    "                adjacency = adjacency.float().to(device)\n",
    "                lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "                    device\n",
    "                ), upper_neighborhood.float().to(device)\n",
    "                y_hat = model(\n",
    "                    x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood\n",
    "                )\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b696409e3d9b84bb97012e0a2d03075417bfa260eb8ad887be094cb925d5d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
